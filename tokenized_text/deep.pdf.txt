{"words": ["doi", "hammingmesh", "view", "accompanying", "technical", "perspective", "tp", "visit", "doiacmorg", "network", "topology", "largescale", "deep", "learning", "torsten", "hoefler", "tommaso", "bonoto", "daniele", "de", "sensi", "salvatore", "di", "girolamo", "shigang", "li", "marco", "heddes", "deepak", "goel", "miguel", "castro", "steve", "scott", "abstract", "matrix", "multiply", "unit", "tensor", "core", "specialized", "numerous", "microarchitectural", "optimization", "unlocked", "vector", "core", "specific", "lowprecision", "datatypes", "tremendous", "processing", "power", "deep", "neural", "network", "optimization", "lead", "order", "magnitude", "efficiency", "turn", "fueled", "ongoing", "ai", "revolution", "improvement", "yet", "approach", "limit", "mi", "exhaustion", "optimization", "growth", "modern", "croarchitectural", "improvement", "need", "direct", "fo", "ai", "gated", "performance", "training", "system", "cu", "system", "level", "especially", "data", "movement", "instead", "focusing", "today", "training", "job", "already", "limited", "data", "move", "single", "accelerator", "investigate", "datamovement", "charac", "ment", "addition", "trend", "deep", "neural", "network", "teristics", "largescale", "training", "full", "system", "scale", "based", "dnns", "sparsity", "increase", "band", "workload", "analysis", "design", "hammingmesh", "width", "demand", "near", "future", "memory", "network", "novel", "network", "topology", "provides", "high", "bandwidth", "bandwidth", "expensivein", "fact", "form", "larg", "low", "cost", "high", "jobscheduling", "flexibility", "specifically", "est", "cost", "component", "today", "system", "standard", "high", "hammingmesh", "support", "full", "bandwidth", "isolation", "performance", "computing", "hpc", "system", "newest", "deep", "learning", "training", "job", "two", "dimension", "par", "infiniband", "adapter", "offer", "gb", "modern", "deep", "allelism", "furthermore", "also", "support", "high", "global", "band", "learning", "training", "system", "offer", "much", "higher", "bandwidth", "width", "generic", "traffic", "thus", "hammingmesh", "power", "google", "tpuv", "designed", "seven", "year", "ago", "tbps", "future", "largescale", "deeplearning", "system", "extreme", "chip", "bandwidth", "aws", "trainium", "tbps", "per", "bandwidth", "requirement", "tmn", "instance", "nvidia", "h", "chip", "tbps", "tbps", "local", "nvlink", "connectivity", "respec", "tively", "chip", "tesla", "dojo", "deeplearning", "supercom", "motivation", "puter", "even", "tbps", "offchip", "bandwidthmore", "artificial", "intelligence", "ai", "experiencing", "unprecedented", "network", "switch", "connecting", "extremebandwidth", "growth", "providing", "seemingly", "openended", "opportunity", "chip", "reasonable", "cost", "daunting", "task", "today", "deep", "learning", "model", "combine", "many", "layer", "operator", "solution", "nvlink", "provide", "local", "island", "complex", "function", "trained", "optimizing", "param", "high", "bandwidth", "eters", "large", "datasets", "given", "abundance", "sensor", "argue", "generalpurpose", "hpc", "datacenter", "simulation", "human", "artifact", "data", "new", "model", "topology", "costeffective", "endpoint", "injec", "designing", "computer", "program", "also", "known", "datadriven", "tion", "bandwidth", "yet", "workload", "specialization", "similar", "programming", "software", "mainly", "limited", "existing", "microarchitectural", "optimization", "lead", "capability", "machine", "perform", "compute", "data", "efficient", "design", "provides", "needed", "highbandwidth", "intensive", "training", "job", "fact", "predictive", "quality", "networking", "begin", "developing", "generic", "model", "model", "improves", "size", "training", "data", "grow", "accurately", "represents", "fundamental", "data", "move", "unprecedented", "scale", "building", "deep", "learning", "supercom", "ment", "characteristic", "deeplearning", "workload", "puters", "explore", "limit", "ai", "commoditize", "model", "show", "inadequacy", "simplistic", "view", "becoming", "interesting", "big", "industry", "main", "communication", "deep", "learning", "allreduce", "also", "humanity", "whole", "fact", "show", "communication", "expressed", "plethora", "different", "model", "type", "exist", "deep", "learn", "concurrent", "mixture", "pipeline", "orthogonal", "ing", "new", "major", "model", "developed", "every", "two", "three", "ductions", "forming", "toroidal", "data", "movement", "pattern", "year", "yet", "computational", "structure", "similarthey", "formulation", "show", "today", "hpc", "network", "optimized", "consist", "layer", "operator", "fundamentally", "full", "global", "bisection", "bandwidth", "inefficient", "dataintensive", "many", "domainspecific", "accelerator", "take", "deeplearning", "workload", "specifically", "global", "band", "advantage", "peculiarity", "deeplearning", "workload", "width", "overprovisioned", "local", "bandwidth", "un", "derprovisioned", "use", "insight", "develop", "hammingmesh", "flex", "original", "version", "paper", "published", "ible", "topology", "adjust", "ratio", "local", "global", "proceeding", "intern", "conf", "high", "performance", "com", "bandwidth", "deeplearning", "workload", "hammingmesh", "puting", "networking", "storage", "analysis", "nov", "combine", "idea", "torus", "globalbandwidth", "topolo", "december", "vol", "communication", "acm", "research", "highlight", "ence", "table", "offer", "overview", "symbol", "used", "figure", "hammingmeshs", "bandwidthcostflexibility", "tradeoff", "paper", "global", "topology", "hammingmesh", "local", "topology", "eg", "fat", "tree", "many", "configuration", "eg", "torus", "communication", "distributed", "deep", "learning", "one", "iteration", "deeplearning", "training", "stochastic", "gradient", "descent", "sgd", "consists", "two", "phase", "reduce", "bandwidth", "ward", "pas", "back", "global", "bandwidth", "e", "placement", "flexibility", "ward", "pas", "forward", "injection", "bandwidth", "pas", "evaluates", "net", "l", "work", "function", "fx", "set", "gy", "example", "fat", "tree", "enable", "flexibilitycost", "trad", "example", "also", "called", "eoff", "shown", "schematically", "figure", "inspired", "machine", "minibatch", "back", "fx", "learning", "ml", "traffic", "pattern", "hammingmesh", "connects", "ward", "pas", "sgd", "com", "local", "highbandwidth", "mesh", "using", "row", "column", "putes", "average", "loss", "l", "propagates", "error", "e", "back", "blue", "red", "switch", "global", "networksa", "ward", "network", "adapt", "parameter", "p", "summary", "show", "deeplearning", "communica", "training", "process", "proceeds", "multiple", "computa", "tion", "modeled", "set", "orthogonal", "parallel", "tionally", "identical", "iteration", "model", "achieves", "hamiltonian", "cycle", "simplify", "mapping", "reasoning", "desired", "accuracy", "based", "observation", "define", "principle", "net", "parallelism", "data", "distribution", "fundamentally", "work", "design", "deeplearning", "workload", "specifically", "arranged", "along", "three", "ax", "data", "parallelism", "pipeline", "hammingmesh", "topology", "parallelism", "operator", "parallelism", "latter", "two", "us", "technologyoptimized", "local", "example", "pcb", "often", "summarized", "model", "parallelism", "operator", "par", "board", "global", "optical", "switched", "connectivity", "allelism", "sometimes", "called", "tensor", "parallelism", "us", "limited", "packetforwarding", "capability", "briefly", "discus", "main", "characteristic", "network", "endpoint", "reduce", "cost", "improve", "flexibility", "enables", "fullbandwidth", "embedding", "virtual", "topolo", "data", "parallelism", "gy", "deeplearning", "traffic", "characteristic", "parallelizing", "training", "data", "train", "sep", "support", "flexible", "job", "allocation", "even", "failed", "node", "arate", "copy", "model", "different", "example", "enables", "flexible", "configuration", "oversubscription", "fac", "achieve", "exactly", "result", "serial", "training", "tor", "adjust", "global", "bandwidth", "sum", "distributed", "gradient", "applying", "principle", "hammingmesh", "enables", "extreme", "weight", "end", "iteration", "network", "offchip", "bandwidth", "nearest", "neighbor", "x", "n", "parameter", "communication", "volume", "p", "cheaper", "allreduce", "bandwidth", "compared", "standard", "hpc", "step", "wn", "p", "topology", "fat", "tree", "hammingmesh", "reduces", "modern", "deep", "neural", "network", "million", "billion", "number", "external", "switch", "cable", "thus", "reduces", "parameter", "making", "communication", "step", "expen", "overall", "system", "cost", "furthermore", "provides", "significantly", "sive", "thus", "many", "optimization", "target", "gradient", "summa", "higher", "flexibility", "torus", "network", "hammingmesh", "tionsome", "even", "change", "convergence", "property", "also", "enables", "seamless", "scaling", "larger", "domain", "without", "training", "process", "maintain", "final", "result", "quality", "separation", "offchassis", "programming", "dozen", "different", "technique", "developed", "op", "model", "like", "nvlink", "v", "infiniband", "believe", "timize", "communicationhowever", "perform", "hammingmesh", "topology", "extend", "ml", "multilin", "form", "distributed", "summation", "operation", "like", "mpiallre", "ear", "algebra", "parallel", "solver", "many", "workload", "duce", "dataparallelism", "differs", "thus", "mostly", "detail", "similar", "traffic", "characteristic", "invocation", "frequency", "consistency", "sparsity", "start", "characterization", "parallel", "deep", "learn", "ing", "related", "data", "movement", "pattern", "refer", "pipeline", "parallelism", "deep", "neural", "network", "evaluated", "layer", "layer", "name", "hammingmesh", "inspired", "structural", "similarity", "output", "layer", "feeding", "input", "layer", "hamming", "graph", "mesh", "vertex", "backpropagation", "performed", "along", "reverse", "direc", "tion", "starting", "loss", "function", "l", "last", "layer", "table", "symbol", "used", "paper", "proceeding", "layer", "layer", "model", "symbol", "description", "network", "pipeline", "p", "stage", "one", "lay", "er", "per", "stage", "forward", "backward", "pass", "inter", "number", "example", "per", "minibatch", "leaved", "processing", "element", "form", "bidirectional", "np", "number", "network", "parameter", "training", "pipeline", "pipeline", "suffer", "characteristic", "w", "size", "word", "startup", "teardown", "overhead", "reduced", "p", "degree", "data", "pipeline", "operator", "parallelism", "running", "two", "pipeline", "direction", "using", "b", "x", "hammingmesh", "board", "global", "size", "asynchronous", "scheme", "impact", "convergence", "overall", "pipelining", "scheme", "use", "p", "processor", "communication", "acm", "december", "vol", "figure", "hammingmesh", "structure", "left", "x", "hxmesh", "right", "hxmesh", "board", "four", "plane", "x", "x", "x", "yx", "nearestneighbor", "communication", "volume", "proportional", "allel", "dimension", "use", "nine", "simultaneous", "allreductions", "number", "output", "activation", "cut", "layer", "size", "three", "pipeline", "parallel", "dimension", "us", "nine", "threedeep", "pipeline", "three", "different", "model", "replica", "operator", "parallelism", "split", "three", "piece", "large", "layer", "computation", "operator", "distrib", "map", "logical", "torus", "fullband", "uted", "processor", "deeplearning", "layer", "operator", "width", "network", "topology", "seems", "wasteful", "provide", "full", "follow", "computational", "schedule", "multilinear", "algebra", "bandwidth", "sparse", "communication", "example", "tensor", "contraction", "require", "either", "tightly", "cou", "gb", "nonblocking", "fat", "tree", "endpoint", "pro", "pled", "distributed", "reduction", "nearestneighbor", "commu", "vides", "full", "bisection", "bandwidth", "gb", "nications", "tb", "bidirectional", "xx", "torus", "communication", "pattern", "requires", "gb", "tb", "bisec", "overall", "communication", "pattern", "tions", "cutting", "one", "dimension", "size", "mere", "form", "parallelism", "used", "resulting", "job", "offered", "bandwidth", "word", "available", "comprises", "p", "accelerator", "accelerator", "job", "bandwidth", "remain", "unused", "wasted", "furthermore", "logical", "address", "p", "data", "pipeline", "always", "simple", "map", "torus", "communication", "operatorparallel", "communication", "arranged", "pattern", "efficiently", "fullbandwidth", "lowdiameter", "topol", "onedimensional", "slice", "ring", "varying", "one", "coor", "ogies", "practice", "dinate", "cartesian", "structure", "pipeline", "would", "leave", "one", "connection", "ring", "unused", "example", "data", "hammingmesh", "parallel", "dimension", "consists", "p", "ring", "length", "based", "communication", "workload", "analysis", "ring", "represents", "single", "allreduce", "show", "design", "flexible", "efficient", "network", "topology", "basic", "efficient", "ringbased", "reduction", "broadcast", "algorithm", "requirement", "support", "highest", "injection", "bandwidth", "large", "data", "volume", "section", "set", "job", "following", "virtual", "toroidal", "com", "overall", "composition", "communication", "pattern", "munication", "topology", "note", "mediumsize", "model", "form", "torus", "illustrated", "right", "part", "figure", "often", "decomposed", "two", "dimension", "prac", "example", "operator", "data", "par", "tice", "usually", "data", "pipeline", "data", "operator", "december", "vol", "communication", "acm", "plane", "fullyconnected", "x", "n", "n", "n", "n", "four", "direction", "per", "plane", "nsew", "axb", "accelerator", "inexpensive", "per", "board", "short", "pcb", "four", "plane", "w", "e", "connection", "per", "accelerator", "board", "w", "e", "w", "e", "w", "e", "x", "board", "plane", "fullyconnected", "figure", "distribution", "strategy", "parallel", "deep", "neural", "network", "training", "data", "parallelism", "pipeline", "parallelism", "operator", "parallelism", "ddata", "pipeline", "operator", "parallelism", "allreduce", "ring", "communication", "allreduce", "ring", "pipeline", "communication", "neighborcommunication", "pd", "p", "p", "p", "pd", "p", "p", "opd", "op", "op", "accelerator", "packet", "package", "switch", "x", "boardresearch", "highlight", "extremescale", "workload", "require", "three", "dimension", "square", "board", "topology", "skip", "first", "number", "ex", "even", "communication", "along", "data", "parallel", "dimen", "ample", "hxmesh", "connects", "x", "board", "called", "sion", "happens", "one", "complete", "iteration", "thus", "x", "hxmesh", "use", "twodimensional", "physical", "topology", "hxmesh", "large", "design", "space", "combine", "dif", "case", "study", "assume", "modern", "deeplearning", "ac", "ferent", "board", "global", "topology", "example", "mesh", "celerator", "package", "gb", "offchip", "network", "link", "board", "global", "slim", "fly", "topology", "work", "total", "network", "injection", "bandwidth", "gb", "top", "left", "consider", "board", "practical", "pcb", "trace", "figure", "topology", "design", "also", "take", "technology", "board", "arrangement", "could", "reduced", "hxmesh", "cost", "account", "similar", "dragonfly", "combine", "nk", "link", "connected", "corre", "local", "short", "copper", "cable", "global", "long", "fiber", "cable", "sponding", "sk", "link", "wrapped", "around", "global", "design", "costeffective", "overall", "topology", "combine", "pology", "also", "span", "multiple", "row", "column", "exam", "local", "group", "global", "topology", "different", "drag", "ple", "full", "board", "single", "fat", "tree", "ease", "exposition", "onfly", "choose", "two", "quite", "distinct", "topology", "local", "limit", "hxmeshes", "using", "board", "group", "formed", "local", "inexpensive", "highbandwidth", "rowcolumnseparated", "global", "topology", "use", "twolevel", "mesh", "using", "short", "metal", "trace", "pcb", "board", "fat", "tree", "global", "topology", "connect", "board", "col", "opposite", "dragonfly", "design", "combine", "densely", "umn", "rowwise", "board", "connected", "connected", "local", "group", "virtual", "switch", "connect", "single", "port", "switch", "use", "instead", "fat", "tree", "fully", "globally", "hammingmesh", "combine", "sparsely", "connected", "board", "dimensionwise", "globally", "fully", "bisection", "global", "bandwidth", "connected", "topology", "board", "connected", "two", "bisection", "cut", "defined", "minimal", "number", "con", "dimensional", "hamming", "graph", "dimension", "nections", "would", "need", "cut", "order", "bisect", "logically", "fully", "connected", "example", "fat", "tree", "network", "two", "piece", "equal", "number", "ac", "accelerator", "port", "arranged", "plane", "four", "direc", "celerators", "bisection", "bandwidth", "cut", "multiplied", "tions", "example", "accelerator", "four", "plane", "top", "link", "bandwidth", "let", "u", "assume", "singleplane", "left", "figure", "example", "plane", "port", "e", "w", "n", "x", "hxamesh", "square", "board", "x", "even", "wlog", "assume", "accelerator", "forward", "packet", "consider", "xy", "lower", "half", "board", "co", "within", "plane", "like", "network", "switch", "accelerator", "ordinate", "split", "hxmesh", "two", "equal", "forward", "packet", "plane", "example", "piece", "cutting", "link", "direction", "packet", "arriving", "n", "may", "forwarded", "e", "w", "lower", "half", "board", "result", "total", "cut", "width", "none", "port", "thus", "simple", "x", "axy", "accelerator", "four", "network", "link", "per", "plane", "switch", "needed", "accelerator", "figure", "illus", "total", "injection", "bandwidth", "per", "board", "xy", "trates", "structure", "detail", "board", "total", "injection", "bandwidth", "axy", "xya", "hammingmesh", "parameterized", "number", "partition", "thus", "relative", "bisection", "bandwidth", "plane", "four", "additional", "number", "b", "dimension", "axyxya", "board", "x", "dimension", "global", "topol", "bisection", "traffic", "pattern", "traffic", "cross", "net", "ogy", "connects", "total", "abxy", "accelerator", "abbreviate", "work", "bisection", "two", "communicating", "endpoint", "hammingmesh", "hxmesh", "following", "different", "set", "bisection", "worstcase", "pattern", "hxmesh", "b", "accelerator", "board", "called", "rare", "practice", "useful", "pattern", "often", "haxbmesh", "example", "x", "board", "hxmesh", "observed", "practice", "alltoall", "process", "sends", "table", "overview", "example", "network", "small", "large", "cluster", "using", "cost", "model", "described", "full", "version", "paper", "bandwidth", "result", "packetlevel", "simulation", "detailed", "section", "global", "alltoall", "bandwidth", "reported", "share", "jection", "bandwidth", "large", "message", "tb", "allreduce", "bandwidth", "reported", "share", "theoretical", "optimum", "injection", "bandwidth", "large", "message", "cost", "saving", "global", "allreduce", "bandwidth", "relative", "corresponding", "network", "cost", "nonblocking", "fat", "tree", "small", "cluster", "accelerator", "large", "cluster", "accelerator", "topology", "cost", "glob", "bw", "global", "ared", "bw", "ared", "diam", "cost", "glob", "bw", "global", "ared", "bw", "ared", "diam", "inject", "saving", "peak", "saving", "inject", "saving", "peak", "saving", "nonbl", "ft", "x", "x", "x", "x", "tap", "ft", "x", "x", "x", "x", "tap", "ft", "x", "x", "x", "x", "dragonfly", "x", "x", "x", "x", "hyperx", "x", "x", "x", "x", "hxmesh", "x", "x", "x", "x", "hxmesh", "x", "x", "x", "x", "torus", "x", "x", "x", "x", "communication", "acm", "december", "vol", "process", "pattern", "basis", "paral", "figure", "workload", "mapping", "onto", "hxmesh", "example", "lel", "transposition", "fast", "fourier", "transforms", "many", "left", "virtual", "xx", "topology", "right", "mapping", "hxmesh", "graph", "algorithm", "achievable", "theoretical", "bandwidth", "alltoall", "pattern", "often", "called", "global", "band", "width", "topology", "construction", "take", "advantage", "fact", "global", "bandwidth", "higher", "bisection", "bandwidth", "prisacari", "et", "al", "show", "fullglobal", "band", "width", "alltoall", "fat", "tree", "constructed", "less", "switch", "nonblocking", "fat", "tree", "dragonfly", "slim", "fly", "lowdiameter", "topology", "reduce", "traffic", "physical", "allocation", "hxmelsh", "number", "switch", "large", "installation", "virtual", "topogy", "xx", "rd", "dimension", "traffic", "routed", "fat", "tree", "maintaining", "full", "global", "bandwidth", "customary", "lowdiameter", "topology", "assess", "using", "packetlevel", "figure", "subnetworks", "case", "failure", "simulation", "alltoall", "traffic", "example", "topology", "xx", "x", "x", "x", "consider", "small", "cluster", "approximately", "ac", "celerators", "large", "cluster", "approximately", "accelerator", "specific", "design", "point", "compare", "realis", "x", "job", "tic", "network", "compare", "various", "fat", "tree", "nonblocking", "tapered", "full", "bandwidth", "dragonfly", "twodimen", "x", "job", "sional", "torus", "hyperxb", "hxmesh", "hxmesh", "example", "topology", "table", "summarizes", "main", "cost", "bandwidth", "sults", "global", "allreduce", "bandwidth", "determined", "using", "packetlevel", "simulation", "see", "section", "large", "folding", "technique", "embed", "twodimensional", "job", "message", "experiment", "simulated", "single", "plane", "figure", "show", "example", "virtual", "topology", "mapped", "hammingmesh", "four", "plane", "topology", "hxmesh", "physical", "topology", "process", "sliced", "total", "injection", "bandwidth", "gb", "use", "third", "dimension", "mapped", "different", "board", "dustrystandard", "layout", "cable", "configuration", "communication", "different", "slice", "third", "cost", "estimate", "fat", "tree", "tapered", "beginning", "dimension", "routed", "percolumn", "perrow", "fat", "second", "level", "connect", "endpoint", "using", "dac", "tree", "depending", "different", "slice", "mapped", "min", "switch", "using", "aoc", "dragonfly", "topology", "use", "fullband", "imize", "communication", "latency", "slice", "consecutive", "width", "group", "router", "p", "endpoint", "slice", "adjacent", "per", "router", "h", "link", "group", "dac", "link", "easy", "see", "consecutive", "u", "v", "block", "inside", "group", "aoc", "link", "group", "board", "hxmesh", "property", "full", "ru", "us", "board", "topology", "discounted", "local", "pcb", "u", "v", "hxmesh", "call", "subnetworks", "virtual", "subhx", "connectivity", "similar", "hxmesh", "dac", "cable", "mesh", "major", "strength", "hxmesh", "compared", "board", "hxmeshes", "use", "dac", "link", "torus", "network", "term", "fault", "tolerance", "well", "connect", "endpoint", "switch", "along", "one", "dimension", "allocating", "job", "fact", "hxmeshes", "major", "strength", "com", "aoc", "link", "dimension", "interswitch", "link", "pared", "torus", "network", "virtual", "subnetworks", "aoc", "fat", "tree", "formed", "nonconsecutive", "set", "board", "block", "set", "board", "hxmesh", "board", "logical", "job", "topology", "failure", "hxmesh", "row", "sequence", "col", "discussed", "section", "communication", "pattern", "umn", "coordinate", "form", "virtual", "subnetwork", "deep", "learning", "modeled", "set", "cycle", "typi", "show", "example", "together", "motivation", "sub", "cal", "learning", "job", "use", "either", "logical", "cycle", "small", "networksfaults", "model", "data", "parallelism", "torus", "combine", "faulttolerance", "assume", "board", "unit", "data", "pipeline", "parallelism", "mediumscale", "model", "failure", "hxmesh", "accelerator", "link", "combining", "pipeline", "model", "parallelism", "large", "board", "fail", "whole", "board", "considered", "failed", "sim", "model", "specific", "training", "job", "different", "plifies", "system", "design", "service", "partial", "failure", "mode", "optimal", "decomposition", "resulting", "sometimes", "example", "per", "plane", "outside", "scope", "work", "even", "logical", "communication", "topology", "left", "part", "figure", "show", "x", "hxmesh", "use", "logical", "topology", "training", "job", "three", "board", "failure", "show", "two", "different", "subnetworks", "job", "us", "several", "board", "request", "u", "v", "layout", "many", "possible", "x", "subnetwork", "blue", "b", "divide", "u", "v", "respectively", "application", "topol", "physical", "board", "ogy", "follows", "scheme", "user", "use", "standard", "x", "subnetwork", "yellow", "physi", "cal", "board", "b", "note", "hyperx", "identical", "hxmesh", "also", "annotate", "new", "coordinate", "board", "december", "vol", "communication", "acm", "research", "highlight", "virtual", "subnetworks", "remapping", "performed", "tern", "rare", "deeplearning", "traffic", "transparently", "user", "application", "ob", "alltoall", "alltoall", "sends", "message", "process", "serve", "difference", "virtual", "physical", "hxmesh", "process", "implementation", "term", "network", "performance", "right", "part", "p", "process", "performs", "p", "iteration", "iteration", "figure", "show", "output", "automatic", "mapping", "tool", "process", "j", "sends", "process", "j", "mod", "p", "balanced", "shift", "described", "detail", "full", "version", "paper", "pattern", "complex", "configuration", "job", "top", "read", "job", "id", "table", "show", "result", "mib", "message", "fig", "logical", "job", "etc", "ure", "show", "global", "bandwidth", "different", "message", "size", "small", "hx", "hxmeshes", "achieve", "bandwidth", "result", "comparison", "around", "cut", "width", "respectively", "cf", "sec", "evaluate", "hxmesh", "topology", "option", "compared", "tion", "global", "traffic", "cross", "topology", "listed", "table", "use", "structural", "bisection", "cut", "especially", "smaller", "cluster", "large", "simulation", "toolkit", "sst", "packetlevel", "network", "simula", "cluster", "configuration", "performs", "closer", "bound", "tor", "validated", "cray", "slingshot", "loses", "bandwidth", "due", "adaptive", "routing", "interconnect", "sst", "enables", "u", "reproduce", "behavior", "head", "despite", "lower", "bandwidth", "even", "large", "hxmeshes", "full", "mpi", "application", "directly", "simulation", "envi", "remain", "competitive", "term", "costper", "global", "bandwidth", "ronment", "react", "dynamic", "network", "change", "even", "cost", "effective", "global", "band", "example", "congestion", "total", "ran", "simulation", "width", "fat", "tree", "billion", "packet", "using", "million", "core", "random", "permutation", "permutation", "traffic", "hour", "parallel", "simulation", "select", "various", "repre", "accelerator", "selects", "unique", "random", "peer", "send", "sentative", "microbenchmarks", "scenario", "deeplearn", "receive", "achieved", "bandwidth", "also", "depends", "ing", "job", "publish", "full", "simulation", "infrastructure", "location", "peer", "figure", "show", "distribu", "reader", "simulate", "job", "setup", "tions", "receive", "bandwidth", "across", "k", "accelerator", "small", "cluster", "configuration", "microbenchmarks", "result", "indicate", "topology", "significant", "start", "analyzing", "wellknown", "microbenchmark", "traf", "variance", "across", "different", "connection", "different", "fic", "pattern", "assess", "compare", "achievable", "peak", "band", "node", "pair", "make", "job", "placement", "locality", "sig", "width", "nificant", "hxmeshes", "among", "cost", "effective", "pologies", "global", "traffic", "pattern", "first", "investigate", "global", "traffic", "pattern", "alltoall", "reduction", "traffic", "pattern", "random", "permutation", "globaltraffic", "workload", "distinguish", "three", "fundamental", "algorithm", "type", "note", "hammingmesh", "optimized", "pat", "tree", "pipeline", "nearoptimal", "fullglobal", "bandwidth", "algorithm", "figure", "alltoall", "small", "topology", "simple", "tree", "small", "data", "simple", "binary", "binomial", "tree", "reduction", "best", "choice", "perform", "reduc", "tion", "byte", "p", "processor", "time", "log", "p", "log", "psc", "algorithm", "sends", "data", "item", "logarithmic", "number", "time", "thus", "inefficient", "large", "data", "size", "deeplearning", "training", "workload", "consider", "tree", "work", "pipelined", "ring", "single", "network", "interface", "large", "data", "volume", "reduced", "simple", "pipelined", "ring", "data", "process", "split", "p", "segment", "operation", "proceeds", "two", "epoch", "p", "round", "per", "ep", "b", "b", "b", "kb", "kb", "kb", "kb", "kb", "och", "first", "reduction", "epoch", "process", "sends", "seg", "message", "size", "ment", "process", "mod", "p", "receives", "segment", "process", "mod", "p", "received", "segment", "added", "local", "data", "sent", "process", "mod", "p", "next", "round", "p", "round", "process", "full", "sum", "one", "segment", "second", "epoch", "simply", "send", "ing", "summed", "segment", "along", "pipeline", "overall", "time", "tp", "p", "bandwidth", "optimal", "process", "sends", "receives", "segment", "twice", "propose", "bidirectional", "pipelined", "ring", "use", "two", "c", "define", "latency", "inverse", "bandwidth", "omit", "additive", "constant", "minor", "lowerorder", "term", "clarity", "communication", "acm", "december", "vol", "sbg", "tuphguorht", "alltoall", "small", "topology", "node", "nonblocking", "fat", "tree", "hyperx", "dragonfly", "fat", "tree", "gb", "mib", "tapered", "fat", "tree", "tapered", "hxmesh", "hxmesh", "torus", "figure", "bandwidth", "distribution", "per", "accelerator", "random", "permutation", "mibsmall", "topology", "node", "sbg", "htdiwdnab", "nonblocki", "fn", "ag", "f", "tra", "et", "et", "r", "e", "e", "f", "ta", "rep", "ee", "r", "e", "taper", "e", "dd", "r", "ha", "yg", "po", "en", "rfl", "x", "h", "mesh", "h", "mesh", "h", "mesh", "torusproblem", "along", "dimension", "mean", "larg", "figure", "global", "allreduce", "using", "different", "algorithm", "est", "allreduce", "broadcast", "would", "process", "ring", "algorithm", "would", "perform", "efficiently", "full", "system", "allreduce", "job", "experiment", "show", "single", "job", "using", "last", "two", "allreduce", "algorithm", "vari", "ous", "topology", "dragonfly", "fat", "tree", "accelerator", "connects", "single", "nic", "four", "plane", "use", "standard", "ring", "algorithm", "single", "allre", "hyperx", "bucket", "duce", "large", "hxmesh", "cluster", "use", "two", "bi", "directional", "ring", "ring", "well", "bucket", "bucket", "algorithm", "figure", "show", "achieved", "bandwidth", "see", "topology", "deliver", "nearly", "full", "bandwidth", "ring", "algorithm", "large", "message", "hxmesh", "x", "x", "cheaper", "per", "bandwidth", "nonblocking", "network", "interface", "splitting", "data", "size", "half", "fat", "tree", "table", "network", "cartesian", "structure", "sending", "half", "along", "different", "direction", "laten", "hammingmesh", "torus", "hyperx", "bucket", "algo", "cy", "stay", "unchanged", "segment", "travel", "twice", "rithm", "outperforms", "ring", "algorithm", "message", "whole", "ring", "data", "half", "direc", "size", "exception", "job", "one", "two", "tion", "leading", "runtime", "bp", "p", "dimension", "much", "smaller", "following", "time", "per", "byte", "interface", "ring", "algorithm", "outperforms", "bucket", "algorithm", "system", "k", "network", "interface", "inject", "k", "bytess", "shown", "highlighting", "importance", "using", "multialgo", "extend", "idea", "four", "network", "interface", "per", "rithms", "optimize", "performance", "similar", "established", "hxmesh", "plane", "use", "two", "bidirectional", "ring", "practice", "mpi", "ducing", "quarter", "data", "across", "accelerator", "two", "ring", "mapped", "two", "disjoint", "hamiltonian", "cycle", "dnn", "workload", "covering", "accelerator", "hxmesh", "overall", "time", "proceed", "define", "accurate", "communication", "pat", "scheme", "ring", "p", "tern", "including", "computation", "time", "real", "dnn", "mod", "bucket", "pipelined", "ring", "bandwidthoptimal", "el", "choose", "four", "large", "representative", "model", "mapped", "hamiltonian", "cycle", "topology", "resnet", "cosmoflow", "dlrm", "transformer", "gpt", "however", "find", "large", "hxmeshes", "moder", "trained", "fp", "discus", "dlrm", "transform", "ate", "message", "size", "latency", "component", "become", "er", "detailed", "discussion", "covering", "mod", "bottleneck", "thus", "use", "stateoftheart", "bucket", "el", "found", "full", "version", "paper", "use", "algorithmd", "bucket", "algorithm", "arranges", "communi", "nvidias", "gpu", "benchmark", "runtimes", "operator", "cation", "toroidal", "communication", "pattern", "p", "model", "communication", "time", "based", "data", "latency", "good", "bandwidth", "usage", "process", "executes", "volume", "first", "reducescatter", "process", "row", "cost", "p", "process", "run", "allreduce", "communication", "traffic", "characterization", "process", "column", "pre", "example", "model", "constructed", "sequence", "iden", "viously", "reduced", "chunk", "size", "p", "co", "p", "p", "tical", "layer", "containing", "multiple", "operator", "paral", "eventually", "allgather", "process", "lel", "dimension", "carry", "different", "volume", "depending", "row", "cost", "p", "use", "four", "network", "inter", "detail", "model", "training", "hyperparameters", "face", "time", "four", "allreduce", "ex", "dimension", "assume", "general", "case", "ecuted", "parallel", "starting", "different", "port", "network", "use", "three", "form", "parallelism", "working", "quarter", "data", "thus", "overall", "time", "running", "p", "accelerator", "scheme", "p", "p", "p", "data", "dimension", "data", "parallelism", "p", "summary", "pipeline", "ring", "bucket", "algorithm", "process", "need", "reduce", "gradient", "sparse", "communication", "pattern", "process", "distribute", "model", "p", "dimension", "process", "communicates", "two", "four", "direct", "neighbor", "total", "allreduce", "size", "v", "w", "n", "p", "p", "reduction", "hap", "mapped", "perfectly", "hxmesh", "broadcast", "col", "pen", "end", "iteration", "processing", "lectives", "implemented", "similarly", "example", "full", "minibatch", "draining", "pipeline", "second", "part", "allreduce", "follow", "similar", "trade", "lapped", "per", "layer", "using", "nonblocking", "allreduce", "offs", "furthermore", "dimension", "logical", "job", "topol", "pipeline", "dimension", "pipeline", "parallel", "ogy", "typically", "small", "total", "number", "accelerator", "ism", "na", "output", "activation", "cut", "layer", "product", "dimension", "example", "even", "large", "system", "accelerator", "dimension", "could", "size", "decompose", "compared", "original", "version", "paper", "replaced", "torus", "algorithm", "better", "bucket", "algorithm", "december", "vol", "communication", "acm", "sbg", "tuphguorht", "allreducelarge", "topology", "k", "node", "fat", "tree", "ring", "hmesh", "bucket", "hmesh", "ring", "hmesh", "bucket", "hmesh", "ring", "torus", "hyperx", "ring", "torusrings", "dragonfly", "ring", "ring", "mib", "mib", "mib", "mib", "mib", "gib", "allreduce", "size", "figure", "overlap", "pipelinedparallel", "execution", "opi", "opi", "opi", "opi", "time", "recvi", "sendirecvi", "sendirecvi", "sendiresearch", "highlight", "figure", "hxmesh", "cost", "saving", "relative", "topology", "relative", "cost", "saving", "communication", "overheard", "dnn", "workload", "hmesh", "resnet", "moe", "process", "sends", "n", "output", "value", "next", "feedforward", "layer", "multiply", "p", "process", "forward", "pas", "volume", "error", "matrix", "per", "example", "layer", "backward", "pas", "layer", "input", "gpt", "total", "layer", "layer", "activa", "output", "distributed", "pe", "total", "send", "vol", "tions", "size", "n", "mb", "per", "example", "ume", "dimension", "v", "p", "dw", "p", "n", "communication", "input", "output", "choose", "p", "pipe", "hidden", "accelerator", "shown", "figure", "line", "stage", "process", "one", "layer", "data", "parallelism", "overlapping", "nonblocking", "sendreceive", "operation", "bot", "operator", "parallelism", "use", "scheme", "tom", "blue", "operator", "computation", "top", "green", "outlined", "megatronlm", "performs", "one", "allreduce", "operator", "dimension", "operator", "parallelism", "ff", "one", "mha", "forward", "backward", "pass", "process", "send", "volume", "depends", "operator", "par", "operation", "size", "layer", "input", "allelization", "influenced", "either", "p", "output", "thus", "volume", "pipeline", "communica", "operator", "seen", "innermost", "loop", "tion", "operatordimension", "allreduce", "n", "per", "example", "sense", "operator", "distribution", "scheme", "forward", "backward", "pass", "one", "iteration", "gpt", "characteristic", "capture", "v", "wn", "op", "computes", "m", "total", "runtimes", "three", "fattree", "erator", "communication", "volume", "forward", "variant", "m", "m", "m", "respectively", "backward", "pas", "function", "local", "minibatch", "size", "torus", "code", "executes", "m", "per", "iteration", "hyperx", "dp", "per", "process", "m", "hx", "hxmesh", "m", "m", "respectively", "dlrm", "gpt", "mixtureofexperts", "moes", "use", "dlrm", "us", "combination", "model", "parallelism", "data", "expert", "gpt", "ffs", "b", "parameter", "parallelism", "embedding", "mlp", "layer", "respective", "fore", "expert", "b", "parameter", "moes", "ly", "two", "alltoall", "operation", "aggregate", "sparse", "embedding", "perform", "two", "alltoalls", "ff", "forward", "lookup", "forward", "pas", "corresponding", "gra", "backward", "pass", "operation", "size", "dients", "backward", "pas", "allreduce", "required", "synchro", "inputoutput", "computation", "time", "nize", "gradient", "dataparallel", "mlp", "layer", "m", "total", "runtime", "fat", "tree", "varies", "m", "parallelism", "dlrm", "limited", "minibatch", "m", "depending", "tapering", "torus", "code", "exe", "size", "embedding", "dimension", "dlrm", "trained", "cutis", "m", "per", "iteration", "hyperx", "take", "m", "gpu", "node", "total", "runtimes", "fat", "tree", "hx", "hxmesh", "m", "m", "respectively", "variant", "m", "m", "m", "respectively", "figure", "show", "relative", "cost", "saving", "hxmesh", "torus", "code", "executes", "m", "hyperx", "m", "compared", "topology", "calculated", "hxmesh", "hxmesh", "m", "m", "respec", "ratio", "network", "cost", "section", "time", "inverse", "tively", "dlrm", "computes", "around", "u", "u", "ratio", "communication", "overhead", "presented", "u", "embedding", "feature", "interaction", "mlp", "section", "layer", "respectively", "communicates", "mb", "per", "alltoall", "conclude", "hx", "hxmesh", "signifi", "mb", "per", "allreduce", "cantly", "reduce", "network", "cost", "dnn", "workload", "torus", "network", "configuration", "cheaper", "transformer", "hxmesh", "provide", "significantly", "less", "allocation", "transformer", "communication", "inten", "management", "flexibility", "especially", "presence", "fail", "sive", "transformer", "block", "consists", "multihead", "ures", "moreover", "also", "conclude", "even", "presence", "tention", "mha", "two", "feedforward", "ff", "layer", "alltoall", "communication", "pattern", "gpt", "moe", "mha", "ff", "inputoutputs", "size", "embedding", "dlrm", "hxmesh", "topology", "still", "offer", "significant", "cost", "ad", "dimensionbatchsequence", "length", "example", "gpt", "vantage", "compared", "traditional", "topology", "scale", "communication", "acm", "december", "vol", "gnivas", "tsoc", "evitaler", "gnivas", "tsoc", "evitaler", "nonblocking", "fat", "tree", "dragonfly", "fat", "tree", "tapered", "hyperx", "fat", "tree", "tapered", "torus", "hmesh", "gpt", "gpt", "cosmoflow", "dlrm", "resnet", "gpt", "gpt", "cosmoflow", "dlrm", "moe", "network", "increase", "hxmesh", "becomes", "significantly", "computing", "networking", "storage", "efficiently", "training", "largescale", "neural", "analysis", "sc", "ieee", "press", "network", "bidirectional", "pipeline", "cost", "efficient", "hxmesh", "especially", "pres", "hoefler", "heddes", "mc", "proceeding", "intern", "conf", "ence", "alltoall", "traffic", "belk", "jr", "distributed", "processing", "high", "performance", "computing", "architecture", "u", "patent", "networking", "storage", "analysis", "discussion", "cover", "additional", "related", "work", "usb", "jul", "sc", "acm", "nov", "comparison", "topology", "well", "significantly", "hoefler", "heddes", "mc", "goel", "naumov", "et", "al", "deep", "learning", "belk", "jr", "distributed", "recommendation", "model", "detail", "hammingmesh", "configuration", "option", "ta", "processing", "architecture", "u", "patent", "personalization", "recommendation", "pering", "diameter", "cost", "routing", "deadlock", "avoidance", "usa", "jul", "system", "arxiv", "preprint", "hoefler", "lumsdaine", "rehm", "arxiv", "well", "scheduling", "without", "board", "failure", "w", "implementation", "performance", "prisacari", "b", "rodriguez", "g", "full", "version", "paper", "analysis", "nonblocking", "collective", "minkenberg", "c", "hoefler", "operation", "mpi", "proceeding", "bandwidthoptimal", "alltoall", "intern", "conf", "high", "exchange", "fat", "tree", "network", "conclusion", "performance", "computing", "networking", "proceeding", "th", "intern", "storage", "analysis", "sc", "ieee", "acm", "conf", "intern", "conf", "hammingmesh", "optimized", "specifically", "ml", "work", "computer", "societyacm", "nov", "supercomputing", "acm", "jun", "load", "communication", "pattern", "relies", "ivanov", "et", "al", "data", "movement", "need", "case", "study", "optimizing", "renggli", "c", "alistarh", "observation", "deeplearning", "training", "us", "three", "transformer", "proceeding", "aghagolzadeh", "hoefler", "machine", "learning", "system", "sparcml", "highperformance", "sparse", "dimensional", "communication", "pattern", "rarely", "need", "mlsys", "apr", "communication", "machine", "learning", "global", "bandwidth", "support", "extreme", "local", "bandwidth", "kaplan", "j", "et", "al", "scaling", "law", "proceeding", "intern", "conf", "neural", "language", "model", "high", "performance", "computing", "controlling", "cost", "global", "bandwidth", "bank", "kathareios", "g", "et", "al", "costeffective", "networking", "storage", "analysis", "inexpensive", "local", "pcbmesh", "interconnect", "together", "diametertwo", "topology", "analysis", "sc", "nov", "evaluation", "proceeding", "sack", "p", "gropp", "w", "collective", "workloadoptimized", "global", "connectivity", "forming", "intern", "conf", "high", "performance", "algorithm", "multiported", "torus", "virtual", "torus", "network", "adjustable", "global", "bandwidth", "computing", "networking", "storage", "network", "acm", "trans", "parallel", "analysis", "sc", "acm", "nov", "comput", "feb", "due", "lower", "number", "switch", "external", "ca", "kim", "j", "dally", "wj", "scott", "shoeybi", "et", "al", "megatronlm", "bles", "nearly", "always", "cost", "effective", "torus", "abts", "technologydriven", "highly", "training", "multibillion", "parameter", "scalable", "dragony", "topology", "language", "model", "using", "model", "network", "also", "offering", "higher", "global", "bandwidth", "proceeding", "intern", "symp", "parallelism", "significantly", "higher", "flexibility", "job", "allocation", "deal", "computer", "architecture", "thakur", "r", "rabenseifner", "r", "lepikhin", "et", "al", "gshard", "scaling", "gropp", "w", "optimization", "collective", "ing", "failure", "giant", "model", "conditional", "communication", "operation", "mpich", "allinall", "believe", "hammingmesh", "drive", "fu", "computation", "automatic", "int", "j", "high", "perform", "comput", "appl", "sharding", "feb", "ture", "deep", "learning", "system", "also", "support", "adjacent", "li", "hoefler", "chimera", "workload", "multilinear", "algebra", "quantum", "simu", "lation", "parallel", "solver", "cartesian", "communi", "torsten", "hoefler", "torstenhoeflerinfethz", "shigang", "li", "shigangliinfethzch", "eth", "cation", "pattern", "ch", "eth", "zurich", "switzerland", "microsoft", "zurich", "switzerland", "corp", "marco", "heddes", "marcoheddes", "acknowledgment", "tommaso", "bonato", "tommasobonatoinf", "microsoftcom", "microsoft", "redmond", "wa", "ethzch", "eth", "zurich", "switzerland", "usa", "thank", "microsoft", "hosting", "th", "sabbatical", "daniel", "de", "sensi", "danieledesensiinf", "deepak", "goel", "deepakgoelmicrosoft", "much", "idea", "developed", "thank", "whole", "ethzch", "eth", "zurich", "switzerland", "com", "microsoft", "sunnyvale", "ca", "usa", "azure", "hardware", "architecture", "team", "especially", "doug", "salvatore", "di", "girolamo", "salvatore", "miguel", "castro", "miguelcastrmicrosoft", "digirolamoinfethzch", "eth", "zurich", "com", "microsoft", "cambridge", "usa", "burger", "continued", "support", "deep", "technical", "switzerland", "steve", "scott", "stevescottmicrosoftcom", "discussion", "thank", "swiss", "national", "supercomput", "microsoft", "redmond", "wa", "usa", "ing", "center", "cscs", "compute", "resource", "piz", "daint", "slim", "fly", "cluster", "thanks", "hussein", "harake", "run", "simulation", "daniele", "de", "sensi", "supported", "eth", "postdoctoral", "fellowship", "fel", "reference", "adalsteinsson", "h", "et", "al", "simulator", "besta", "hoefler", "slim", "fly", "largescale", "parallel", "computer", "cost", "effective", "lowdiameter", "network", "architecture", "int", "j", "distrib", "syst", "topology", "proceeding", "technol", "apr", "intern", "conf", "high", "performance", "alistarh", "et", "al", "convergence", "computing", "networking", "storage", "sparsified", "gradient", "method", "analysis", "sc", "nov", "advance", "neural", "information", "brown", "tb", "et", "al", "language", "model", "processing", "system", "curran", "fewshot", "learner", "associate", "inc", "dec", "de", "sensi", "et", "al", "indepth", "bae", "mm", "albdaiwi", "bf", "bose", "analysis", "slingshot", "interconnect", "b", "edgedisjoint", "hamiltonian", "cycle", "proceeding", "intern", "conf", "twodimensional", "torus", "int", "j", "high", "performance", "computing", "math", "math", "sci", "networking", "storage", "analysis", "sc", "nov", "barnett", "littlefield", "r", "payne", "hoefler", "et", "al", "sparsity", "deep", "vandegeijn", "r", "global", "combine", "learning", "pruning", "growth", "algorithm", "mesh", "efficient", "inference", "training", "wormhole", "routing", "j", "parallel", "distrib", "neural", "network", "j", "machine", "comput", "feb", "learning", "research", "sep", "bennun", "hoefler", "demystifying", "parallel", "distributed", "hoefler", "et", "al", "hammingmesh", "deep", "learning", "indepth", "network", "topology", "largescale", "concurrency", "analysis", "acm", "comput", "deep", "learning", "proceeding", "work", "licensed", "creative", "common", "surv", "aug", "intern", "conf", "high", "performance", "attribution", "international", "license", "december", "vol", "communication", "acm", "copyrightof", "communicationsof", "theacm", "isthepropertyof", "associationfor", "computing", "machineryanditscontentmaynotbecopiedor", "emailedtomultiplesitesor", "postedtoa", "listservwithoutthecopyrightholders", "express", "writtenpermissionhowever", "user", "mayprint", "downloador", "emailarticlesfor", "individualuse"], "sentences": ["doi hammingmesh view accompanying technical perspective tp visit doiacmorg network topology largescale deep learning torsten hoefler tommaso bonoto daniele de sensi salvatore di girolamo shigang li marco heddes deepak goel miguel castro steve scott abstract matrix multiply unit tensor core specialized numerous microarchitectural optimization unlocked vector core specific lowprecision datatypes tremendous processing power deep neural network optimization lead order magnitude efficiency turn fueled ongoing ai revolution improvement yet approach limit mi exhaustion optimization growth modern croarchitectural improvement need direct fo ai gated performance training system cu system level especially data movement instead focusing today training job already limited data move single accelerator investigate datamovement charac ment addition trend deep neural network teristics largescale training full system scale based dnns sparsity increase band workload analysis design hammingmesh width demand near future memory network novel network topology provides high bandwidth bandwidth expensivein fact form larg low cost high jobscheduling flexibility specifically est cost component today system standard high hammingmesh support full bandwidth isolation performance computing hpc system newest deep learning training job two dimension par infiniband adapter offer gb modern deep allelism furthermore also support high global band learning training system offer much higher bandwidth width generic traffic thus hammingmesh power google tpuv designed seven year ago tbps future largescale deeplearning system extreme chip bandwidth aws trainium tbps per bandwidth requirement tmn instance nvidia h chip tbps tbps local nvlink connectivity respec tively chip tesla dojo deeplearning supercom motivation puter even tbps offchip bandwidthmore artificial intelligence ai experiencing unprecedented network switch connecting extremebandwidth growth providing seemingly openended opportunity chip reasonable cost daunting task today deep learning model combine many layer operator solution nvlink provide local island complex function trained optimizing param high bandwidth eters large datasets given abundance sensor argue generalpurpose hpc datacenter simulation human artifact data new model topology costeffective endpoint injec designing computer program also known datadriven tion bandwidth yet workload specialization similar programming software mainly limited existing microarchitectural optimization lead capability machine perform compute data efficient design provides needed highbandwidth intensive training job fact predictive quality networking begin developing generic model model improves size training data grow accurately represents fundamental data move unprecedented scale building deep learning supercom ment characteristic deeplearning workload puters explore limit ai commoditize model show inadequacy simplistic view becoming interesting big industry main communication deep learning allreduce also humanity whole fact show communication expressed plethora different model type exist deep learn concurrent mixture pipeline orthogonal ing new major model developed every two three ductions forming toroidal data movement pattern year yet computational structure similarthey formulation show today hpc network optimized consist layer operator fundamentally full global bisection bandwidth inefficient dataintensive many domainspecific accelerator take deeplearning workload specifically global band advantage peculiarity deeplearning workload width overprovisioned local bandwidth un derprovisioned use insight develop hammingmesh flex original version paper published ible topology adjust ratio local global proceeding intern conf high performance com bandwidth deeplearning workload hammingmesh puting networking storage analysis nov combine idea torus globalbandwidth topolo december vol communication acm research highlight ence table offer overview symbol used figure hammingmeshs bandwidthcostflexibility tradeoff paper global topology hammingmesh local topology eg fat tree many configuration eg torus communication distributed deep learning one iteration deeplearning training stochastic gradient descent sgd consists two phase reduce bandwidth ward pas back global bandwidth e placement flexibility ward pas forward injection bandwidth pas evaluates net l work function fx set gy example fat tree enable flexibilitycost trad example also called eoff shown schematically figure inspired machine minibatch back fx learning ml traffic pattern hammingmesh connects ward pas sgd com local highbandwidth mesh using row column putes average loss l propagates error e back blue red switch global networksa ward network adapt parameter p summary show deeplearning communica training process proceeds multiple computa tion modeled set orthogonal parallel tionally identical iteration model achieves hamiltonian cycle simplify mapping reasoning desired accuracy based observation define principle net parallelism data distribution fundamentally work design deeplearning workload specifically arranged along three ax data parallelism pipeline hammingmesh topology parallelism operator parallelism latter two us technologyoptimized local example pcb often summarized model parallelism operator par board global optical switched connectivity allelism sometimes called tensor parallelism us limited packetforwarding capability briefly discus main characteristic network endpoint reduce cost improve flexibility enables fullbandwidth embedding virtual topolo data parallelism gy deeplearning traffic characteristic parallelizing training data train sep support flexible job allocation even failed node arate copy model different example enables flexible configuration oversubscription fac achieve exactly result serial training tor adjust global bandwidth sum distributed gradient applying principle hammingmesh enables extreme weight end iteration network offchip bandwidth nearest neighbor x n parameter communication volume p cheaper allreduce bandwidth compared standard hpc step wn p topology fat tree hammingmesh reduces modern deep neural network million billion number external switch cable thus reduces parameter making communication step expen overall system cost furthermore provides significantly sive thus many optimization target gradient summa higher flexibility torus network hammingmesh tionsome even change convergence property also enables seamless scaling larger domain without training process maintain final result quality separation offchassis programming dozen different technique developed op model like nvlink v infiniband believe timize communicationhowever perform hammingmesh topology extend ml multilin form distributed summation operation like mpiallre ear algebra parallel solver many workload duce dataparallelism differs thus mostly detail similar traffic characteristic invocation frequency consistency sparsity start characterization parallel deep learn ing related data movement pattern refer pipeline parallelism deep neural network evaluated layer layer name hammingmesh inspired structural similarity output layer feeding input layer hamming graph mesh vertex backpropagation performed along reverse direc tion starting loss function l last layer table symbol used paper proceeding layer layer model symbol description network pipeline p stage one lay er per stage forward backward pass inter number example per minibatch leaved processing element form bidirectional np number network parameter training pipeline pipeline suffer characteristic w size word startup teardown overhead reduced p degree data pipeline operator parallelism running two pipeline direction using b x hammingmesh board global size asynchronous scheme impact convergence overall pipelining scheme use p processor communication acm december vol figure hammingmesh structure left x hxmesh right hxmesh board four plane x x x yx nearestneighbor communication volume proportional allel dimension use nine simultaneous allreductions number output activation cut layer size three pipeline parallel dimension us nine threedeep pipeline three different model replica operator parallelism split three piece large layer computation operator distrib map logical torus fullband uted processor deeplearning layer operator width network topology seems wasteful provide full follow computational schedule multilinear algebra bandwidth sparse communication example tensor contraction require either tightly cou gb nonblocking fat tree endpoint pro pled distributed reduction nearestneighbor commu vides full bisection bandwidth gb nications tb bidirectional xx torus communication pattern requires gb tb bisec overall communication pattern tions cutting one dimension size mere form parallelism used resulting job offered bandwidth word available comprises p accelerator accelerator job bandwidth remain unused wasted furthermore logical address p data pipeline always simple map torus communication operatorparallel communication arranged pattern efficiently fullbandwidth lowdiameter topol onedimensional slice ring varying one coor ogies practice dinate cartesian structure pipeline would leave one connection ring unused example data hammingmesh parallel dimension consists p ring length based communication workload analysis ring represents single allreduce show design flexible efficient network topology basic efficient ringbased reduction broadcast algorithm requirement support highest injection bandwidth large data volume section set job following virtual toroidal com overall composition communication pattern munication topology note mediumsize model form torus illustrated right part figure often decomposed two dimension prac example operator data par tice usually data pipeline data operator december vol communication acm plane fullyconnected x n n n n four direction per plane nsew axb accelerator inexpensive per board short pcb four plane w e connection per accelerator board w e w e w e x board plane fullyconnected figure distribution strategy parallel deep neural network training data parallelism pipeline parallelism operator parallelism ddata pipeline operator parallelism allreduce ring communication allreduce ring pipeline communication neighborcommunication pd p p p pd p p opd op op accelerator packet package switch x boardresearch highlight extremescale workload require three dimension square board topology skip first number ex even communication along data parallel dimen ample hxmesh connects x board called sion happens one complete iteration thus x hxmesh use twodimensional physical topology hxmesh large design space combine dif case study assume modern deeplearning ac ferent board global topology example mesh celerator package gb offchip network link board global slim fly topology work total network injection bandwidth gb top left consider board practical pcb trace figure topology design also take technology board arrangement could reduced hxmesh cost account similar dragonfly combine nk link connected corre local short copper cable global long fiber cable sponding sk link wrapped around global design costeffective overall topology combine pology also span multiple row column exam local group global topology different drag ple full board single fat tree ease exposition onfly choose two quite distinct topology local limit hxmeshes using board group formed local inexpensive highbandwidth rowcolumnseparated global topology use twolevel mesh using short metal trace pcb board fat tree global topology connect board col opposite dragonfly design combine densely umn rowwise board connected connected local group virtual switch connect single port switch use instead fat tree fully globally hammingmesh combine sparsely connected board dimensionwise globally fully bisection global bandwidth connected topology board connected two bisection cut defined minimal number con dimensional hamming graph dimension nections would need cut order bisect logically fully connected example fat tree network two piece equal number ac accelerator port arranged plane four direc celerators bisection bandwidth cut multiplied tions example accelerator four plane top link bandwidth let u assume singleplane left figure example plane port e w n x hxamesh square board x even wlog assume accelerator forward packet consider xy lower half board co within plane like network switch accelerator ordinate split hxmesh two equal forward packet plane example piece cutting link direction packet arriving n may forwarded e w lower half board result total cut width none port thus simple x axy accelerator four network link per plane switch needed accelerator figure illus total injection bandwidth per board xy trates structure detail board total injection bandwidth axy xya hammingmesh parameterized number partition thus relative bisection bandwidth plane four additional number b dimension axyxya board x dimension global topol bisection traffic pattern traffic cross net ogy connects total abxy accelerator abbreviate work bisection two communicating endpoint hammingmesh hxmesh following different set bisection worstcase pattern hxmesh b accelerator board called rare practice useful pattern often haxbmesh example x board hxmesh observed practice alltoall process sends table overview example network small large cluster using cost model described full version paper bandwidth result packetlevel simulation detailed section global alltoall bandwidth reported share jection bandwidth large message tb allreduce bandwidth reported share theoretical optimum injection bandwidth large message cost saving global allreduce bandwidth relative corresponding network cost nonblocking fat tree small cluster accelerator large cluster accelerator topology cost glob bw global ared bw ared diam cost glob bw global ared bw ared diam inject saving peak saving inject saving peak saving nonbl ft x x x x tap ft x x x x tap ft x x x x dragonfly x x x x hyperx x x x x hxmesh x x x x hxmesh x x x x torus x x x x communication acm december vol process pattern basis paral figure workload mapping onto hxmesh example lel transposition fast fourier transforms many left virtual xx topology right mapping hxmesh graph algorithm achievable theoretical bandwidth alltoall pattern often called global band width topology construction take advantage fact global bandwidth higher bisection bandwidth prisacari et al show fullglobal band width alltoall fat tree constructed less switch nonblocking fat tree dragonfly slim fly lowdiameter topology reduce traffic physical allocation hxmelsh number switch large installation virtual topogy xx rd dimension traffic routed fat tree maintaining full global bandwidth customary lowdiameter topology assess using packetlevel figure subnetworks case failure simulation alltoall traffic example topology xx x x x consider small cluster approximately ac celerators large cluster approximately accelerator specific design point compare realis x job tic network compare various fat tree nonblocking tapered full bandwidth dragonfly twodimen x job sional torus hyperxb hxmesh hxmesh example topology table summarizes main cost bandwidth sults global allreduce bandwidth determined using packetlevel simulation see section large folding technique embed twodimensional job message experiment simulated single plane figure show example virtual topology mapped hammingmesh four plane topology hxmesh physical topology process sliced total injection bandwidth gb use third dimension mapped different board dustrystandard layout cable configuration communication different slice third cost estimate fat tree tapered beginning dimension routed percolumn perrow fat second level connect endpoint using dac tree depending different slice mapped min switch using aoc dragonfly topology use fullband imize communication latency slice consecutive width group router p endpoint slice adjacent per router h link group dac link easy see consecutive u v block inside group aoc link group board hxmesh property full ru us board topology discounted local pcb u v hxmesh call subnetworks virtual subhx connectivity similar hxmesh dac cable mesh major strength hxmesh compared board hxmeshes use dac link torus network term fault tolerance well connect endpoint switch along one dimension allocating job fact hxmeshes major strength com aoc link dimension interswitch link pared torus network virtual subnetworks aoc fat tree formed nonconsecutive set board block set board hxmesh board logical job topology failure hxmesh row sequence col discussed section communication pattern umn coordinate form virtual subnetwork deep learning modeled set cycle typi show example together motivation sub cal learning job use either logical cycle small networksfaults model data parallelism torus combine faulttolerance assume board unit data pipeline parallelism mediumscale model failure hxmesh accelerator link combining pipeline model parallelism large board fail whole board considered failed sim model specific training job different plifies system design service partial failure mode optimal decomposition resulting sometimes example per plane outside scope work even logical communication topology left part figure show x hxmesh use logical topology training job three board failure show two different subnetworks job us several board request u v layout many possible x subnetwork blue b divide u v respectively application topol physical board ogy follows scheme user use standard x subnetwork yellow physi cal board b note hyperx identical hxmesh also annotate new coordinate board december vol communication acm research highlight virtual subnetworks remapping performed tern rare deeplearning traffic transparently user application ob alltoall alltoall sends message process serve difference virtual physical hxmesh process implementation term network performance right part p process performs p iteration iteration figure show output automatic mapping tool process j sends process j mod p balanced shift described detail full version paper pattern complex configuration job top read job id table show result mib message fig logical job etc ure show global bandwidth different message size small hx hxmeshes achieve bandwidth result comparison around cut width respectively cf sec evaluate hxmesh topology option compared tion global traffic cross topology listed table use structural bisection cut especially smaller cluster large simulation toolkit sst packetlevel network simula cluster configuration performs closer bound tor validated cray slingshot loses bandwidth due adaptive routing interconnect sst enables u reproduce behavior head despite lower bandwidth even large hxmeshes full mpi application directly simulation envi remain competitive term costper global bandwidth ronment react dynamic network change even cost effective global band example congestion total ran simulation width fat tree billion packet using million core random permutation permutation traffic hour parallel simulation select various repre accelerator selects unique random peer send sentative microbenchmarks scenario deeplearn receive achieved bandwidth also depends ing job publish full simulation infrastructure location peer figure show distribu reader simulate job setup tions receive bandwidth across k accelerator small cluster configuration microbenchmarks result indicate topology significant start analyzing wellknown microbenchmark traf variance across different connection different fic pattern assess compare achievable peak band node pair make job placement locality sig width nificant hxmeshes among cost effective pologies global traffic pattern first investigate global traffic pattern alltoall reduction traffic pattern random permutation globaltraffic workload distinguish three fundamental algorithm type note hammingmesh optimized pat tree pipeline nearoptimal fullglobal bandwidth algorithm figure alltoall small topology simple tree small data simple binary binomial tree reduction best choice perform reduc tion byte p processor time log p log psc algorithm sends data item logarithmic number time thus inefficient large data size deeplearning training workload consider tree work pipelined ring single network interface large data volume reduced simple pipelined ring data process split p segment operation proceeds two epoch p round per ep b b b kb kb kb kb kb och first reduction epoch process sends seg message size ment process mod p receives segment process mod p received segment added local data sent process mod p next round p round process full sum one segment second epoch simply send ing summed segment along pipeline overall time tp p bandwidth optimal process sends receives segment twice propose bidirectional pipelined ring use two c define latency inverse bandwidth omit additive constant minor lowerorder term clarity communication acm december vol sbg tuphguorht alltoall small topology node nonblocking fat tree hyperx dragonfly fat tree gb mib tapered fat tree tapered hxmesh hxmesh torus figure bandwidth distribution per accelerator random permutation mibsmall topology node sbg htdiwdnab nonblocki fn ag f tra et et r e e f ta rep ee r e taper e dd r ha yg po en rfl x h mesh h mesh h mesh torusproblem along dimension mean larg figure global allreduce using different algorithm est allreduce broadcast would process ring algorithm would perform efficiently full system allreduce job experiment show single job using last two allreduce algorithm vari ous topology dragonfly fat tree accelerator connects single nic four plane use standard ring algorithm single allre hyperx bucket duce large hxmesh cluster use two bi directional ring ring well bucket bucket algorithm figure show achieved bandwidth see topology deliver nearly full bandwidth ring algorithm large message hxmesh x x cheaper per bandwidth nonblocking network interface splitting data size half fat tree table network cartesian structure sending half along different direction laten hammingmesh torus hyperx bucket algo cy stay unchanged segment travel twice rithm outperforms ring algorithm message whole ring data half direc size exception job one two tion leading runtime bp p dimension much smaller following time per byte interface ring algorithm outperforms bucket algorithm system k network interface inject k bytess shown highlighting importance using multialgo extend idea four network interface per rithms optimize performance similar established hxmesh plane use two bidirectional ring practice mpi ducing quarter data across accelerator two ring mapped two disjoint hamiltonian cycle dnn workload covering accelerator hxmesh overall time proceed define accurate communication pat scheme ring p tern including computation time real dnn mod bucket pipelined ring bandwidthoptimal el choose four large representative model mapped hamiltonian cycle topology resnet cosmoflow dlrm transformer gpt however find large hxmeshes moder trained fp discus dlrm transform ate message size latency component become er detailed discussion covering mod bottleneck thus use stateoftheart bucket el found full version paper use algorithmd bucket algorithm arranges communi nvidias gpu benchmark runtimes operator cation toroidal communication pattern p model communication time based data latency good bandwidth usage process executes volume first reducescatter process row cost p process run allreduce communication traffic characterization process column pre example model constructed sequence iden viously reduced chunk size p co p p tical layer containing multiple operator paral eventually allgather process lel dimension carry different volume depending row cost p use four network inter detail model training hyperparameters face time four allreduce ex dimension assume general case ecuted parallel starting different port network use three form parallelism working quarter data thus overall time running p accelerator scheme p p p data dimension data parallelism p summary pipeline ring bucket algorithm process need reduce gradient sparse communication pattern process distribute model p dimension process communicates two four direct neighbor total allreduce size v w n p p reduction hap mapped perfectly hxmesh broadcast col pen end iteration processing lectives implemented similarly example full minibatch draining pipeline second part allreduce follow similar trade lapped per layer using nonblocking allreduce offs furthermore dimension logical job topol pipeline dimension pipeline parallel ogy typically small total number accelerator ism na output activation cut layer product dimension example even large system accelerator dimension could size decompose compared original version paper replaced torus algorithm better bucket algorithm december vol communication acm sbg tuphguorht allreducelarge topology k node fat tree ring hmesh bucket hmesh ring hmesh bucket hmesh ring torus hyperx ring torusrings dragonfly ring ring mib mib mib mib mib gib allreduce size figure overlap pipelinedparallel execution opi opi opi opi time recvi sendirecvi sendirecvi sendiresearch highlight figure hxmesh cost saving relative topology relative cost saving communication overheard dnn workload hmesh resnet moe process sends n output value next feedforward layer multiply p process forward pas volume error matrix per example layer backward pas layer input gpt total layer layer activa output distributed pe total send vol tions size n mb per example ume dimension v p dw p n communication input output choose p pipe hidden accelerator shown figure line stage process one layer data parallelism overlapping nonblocking sendreceive operation bot operator parallelism use scheme tom blue operator computation top green outlined megatronlm performs one allreduce operator dimension operator parallelism ff one mha forward backward pass process send volume depends operator par operation size layer input allelization influenced either p output thus volume pipeline communica operator seen innermost loop tion operatordimension allreduce n per example sense operator distribution scheme forward backward pass one iteration gpt characteristic capture v wn op computes m total runtimes three fattree erator communication volume forward variant m m m respectively backward pas function local minibatch size torus code executes m per iteration hyperx dp per process m hx hxmesh m m respectively dlrm gpt mixtureofexperts moes use dlrm us combination model parallelism data expert gpt ffs b parameter parallelism embedding mlp layer respective fore expert b parameter moes ly two alltoall operation aggregate sparse embedding perform two alltoalls ff forward lookup forward pas corresponding gra backward pass operation size dients backward pas allreduce required synchro inputoutput computation time nize gradient dataparallel mlp layer m total runtime fat tree varies m parallelism dlrm limited minibatch m depending tapering torus code exe size embedding dimension dlrm trained cutis m per iteration hyperx take m gpu node total runtimes fat tree hx hxmesh m m respectively variant m m m respectively figure show relative cost saving hxmesh torus code executes m hyperx m compared topology calculated hxmesh hxmesh m m respec ratio network cost section time inverse tively dlrm computes around u u ratio communication overhead presented u embedding feature interaction mlp section layer respectively communicates mb per alltoall conclude hx hxmesh signifi mb per allreduce cantly reduce network cost dnn workload torus network configuration cheaper transformer hxmesh provide significantly less allocation transformer communication inten management flexibility especially presence fail sive transformer block consists multihead ures moreover also conclude even presence tention mha two feedforward ff layer alltoall communication pattern gpt moe mha ff inputoutputs size embedding dlrm hxmesh topology still offer significant cost ad dimensionbatchsequence length example gpt vantage compared traditional topology scale communication acm december vol gnivas tsoc evitaler gnivas tsoc evitaler nonblocking fat tree dragonfly fat tree tapered hyperx fat tree tapered torus hmesh gpt gpt cosmoflow dlrm resnet gpt gpt cosmoflow dlrm moe network increase hxmesh becomes significantly computing networking storage efficiently training largescale neural analysis sc ieee press network bidirectional pipeline cost efficient hxmesh especially pres hoefler heddes mc proceeding intern conf ence alltoall traffic belk jr distributed processing high performance computing architecture u patent networking storage analysis discussion cover additional related work usb jul sc acm nov comparison topology well significantly hoefler heddes mc goel naumov et al deep learning belk jr distributed recommendation model detail hammingmesh configuration option ta processing architecture u patent personalization recommendation pering diameter cost routing deadlock avoidance usa jul system arxiv preprint hoefler lumsdaine rehm arxiv well scheduling without board failure w implementation performance prisacari b rodriguez g full version paper analysis nonblocking collective minkenberg c hoefler operation mpi proceeding bandwidthoptimal alltoall intern conf high exchange fat tree network conclusion performance computing networking proceeding th intern storage analysis sc ieee acm conf intern conf hammingmesh optimized specifically ml work computer societyacm nov supercomputing acm jun load communication pattern relies ivanov et al data movement need case study optimizing renggli c alistarh observation deeplearning training us three transformer proceeding aghagolzadeh hoefler machine learning system sparcml highperformance sparse dimensional communication pattern rarely need mlsys apr communication machine learning global bandwidth support extreme local bandwidth kaplan j et al scaling law proceeding intern conf neural language model high performance computing controlling cost global bandwidth bank kathareios g et al costeffective networking storage analysis inexpensive local pcbmesh interconnect together diametertwo topology analysis sc nov evaluation proceeding sack p gropp w collective workloadoptimized global connectivity forming intern conf high performance algorithm multiported torus virtual torus network adjustable global bandwidth computing networking storage network acm trans parallel analysis sc acm nov comput feb due lower number switch external ca kim j dally wj scott shoeybi et al megatronlm bles nearly always cost effective torus abts technologydriven highly training multibillion parameter scalable dragony topology language model using model network also offering higher global bandwidth proceeding intern symp parallelism significantly higher flexibility job allocation deal computer architecture thakur r rabenseifner r lepikhin et al gshard scaling gropp w optimization collective ing failure giant model conditional communication operation mpich allinall believe hammingmesh drive fu computation automatic int j high perform comput appl sharding feb ture deep learning system also support adjacent li hoefler chimera workload multilinear algebra quantum simu lation parallel solver cartesian communi torsten hoefler torstenhoeflerinfethz shigang li shigangliinfethzch eth cation pattern ch eth zurich switzerland microsoft zurich switzerland corp marco heddes marcoheddes acknowledgment tommaso bonato tommasobonatoinf microsoftcom microsoft redmond wa ethzch eth zurich switzerland usa thank microsoft hosting th sabbatical daniel de sensi danieledesensiinf deepak goel deepakgoelmicrosoft much idea developed thank whole ethzch eth zurich switzerland com microsoft sunnyvale ca usa azure hardware architecture team especially doug salvatore di girolamo salvatore miguel castro miguelcastrmicrosoft digirolamoinfethzch eth zurich com microsoft cambridge usa burger continued support deep technical switzerland steve scott stevescottmicrosoftcom discussion thank swiss national supercomput microsoft redmond wa usa ing center cscs compute resource piz daint slim fly cluster thanks hussein harake run simulation daniele de sensi supported eth postdoctoral fellowship fel reference adalsteinsson h et al simulator besta hoefler slim fly largescale parallel computer cost effective lowdiameter network architecture int j distrib syst topology proceeding technol apr intern conf high performance alistarh et al convergence computing networking storage sparsified gradient method analysis sc nov advance neural information brown tb et al language model processing system curran fewshot learner associate inc dec de sensi et al indepth bae mm albdaiwi bf bose analysis slingshot interconnect b edgedisjoint hamiltonian cycle proceeding intern conf twodimensional torus int j high performance computing math math sci networking storage analysis sc nov barnett littlefield r payne hoefler et al sparsity deep vandegeijn r global combine learning pruning growth algorithm mesh efficient inference training wormhole routing j parallel distrib neural network j machine comput feb learning research sep bennun hoefler demystifying parallel distributed hoefler et al hammingmesh deep learning indepth network topology largescale concurrency analysis acm comput deep learning proceeding work licensed creative common surv aug intern conf high performance attribution international license december vol communication acm copyrightof communicationsof theacm isthepropertyof associationfor computing machineryanditscontentmaynotbecopiedor emailedtomultiplesitesor postedtoa listservwithoutthecopyrightholders express writtenpermissionhowever user mayprint downloador emailarticlesfor individualuse"]}