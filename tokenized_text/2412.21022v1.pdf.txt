{"words": ["text", "classification", "neural", "network", "v", "machine", "learning", "model", "v", "pretrained", "model", "christospetridis", "christospetridistempleedu", "templeuniversity", "philadelphiapennsylvaniausa", "abstract", "byachievinglevelsoffluencycomprehensionandcontextualun", "textclassificationisaverycommontasknowadaysandthereare", "derstandinglikeneverbeforellmshavetransformedthefield", "manyefficientmethodsandalgorithmsthatwecanemploytoac", "ofnlpwiththeirabilitytolearnpatternsintextfrommassive", "complishittransformershaverevolutionizedthefieldofdeep", "datasetswehaveseenresultsthatwerenotpossiblewithearlier", "learningparticularlyinnaturallanguageprocessingnlpand", "rulebasedmodelsllmscangeneratehumanliketextunderstand", "haverapidlyexpandedtootherdomainssuchascomputervision", "complexqueriesandevenperformtasksonwhichtheywerenot", "timeseriesanalysisandmorethetransformermodel", "explicitlytrainedthankstotheirpretrainedandtransferlearning", "wasfirstlyintroducedinthecontextofmachinetranslationand", "capability", "itsarchitecturereliesonselfattentionmechanismstocapture", "computer", "course", "ai", "model", "understand", "gener", "complexrelationshipswithindatasequencesitisabletohandle", "atehumanlanguagethroughtheuseofembeddingsembeddings", "longrangedependenciesmoreeffectivelythantraditionalneural", "innlprefer", "todensevectorrepresentationsofwords", "phrase", "networkssuchasrecurrentneuralnetworksandmultilayerper", "orevenentiresentencesthatcapturesemanticmeaningandcon", "ceptronsinthisworkwepresentacomparisonbetweendifferent", "textualrelationshipswithinanumericalformatthatmodelscan", "techniquestoperformtextclassificationwetakeintoconsideration", "understandunliketraditionalonehotencodingmethodswhich", "sevenpretrainedmodelsthreestandardneuralnetworksandthree", "areverysparseandfailtoconveyanyinformationabouthowwords", "machinelearningmodelsforstandardneuralnetworksandma", "relatetoeachotherembeddingsencodewordsintocontinuous", "chinelearningmodelswealsocomparetwoembeddingtechniques", "lowdimensionalspacesthisallowsmodelstomapsemantically", "tfidfandglovewiththelatterconsistentlyoutperformingthe", "similarwordsclosetoeachotherinthisspaceforexamplethe", "formerfinallywedemonstratetheresultsfromourexperiments", "wordskingandqueenmayappearclosertoeachotherinthe", "wherepretrainedmodelssuchasbertanddistilbertalways", "vectorspacethankingandcarbecauseoftheirsemanticand", "performbetterthanstandardmodelsalgorithms", "contextualsimilaritiesembeddingsarelearnedbymodelsbasedon", "largecorporaoftextmakingthemeffectiveatcapturingintricate", "keywords", "relationshipspatternsandlinguisticpropertiesfamousembed", "ding", "technique", "wordvec", "glove", "global", "vector", "textclassificationtransformerspretrainedmodelsnaturallan", "contextualembeddingsfromtransformerbasedmodelsrepresenta", "guageprocessingembeddings", "wordincontextconsideringthesurroundingwordsinasentence", "referenceformat", "thismeansthatthewordbankwouldhavedifferentembeddings", "christospetridistextclassificationneuralnetworksvsmachine", "dependingonwhetheritisusedinthecontextofariverbankora", "learningmodelsvspretrainedmodels", "cisneuralcomputation", "financialinstitutionitisclearthatthoseembeddingsofferbetter", "templeuniversityphiladelphiapausa", "resultsinnlptasksthanolderstaticembeddingssuchastfidf", "introduction", "termfrequencyinversedocumentfrequency", "inthisworkwepresentaperformancecomparisonbetween", "nlphasbeenarapidlygrowingareaduetoitsimpactonhow", "pretrainedmodelsandstandardmodelsonaclassificationtask", "humansinteractwithtechnologywehaveseenmanyapplications", "developed", "three", "neural", "network", "experiment", "rangingfromvoiceactivatedassistantsandchatbotstoinformation", "includingmlprnnandtransformerencodermodelsandwerefer", "retrievaltextsummarizationandsentimentanalysisitsrelevance", "tothemasstandardmodelswewillalsopresenttheperformanceof", "continuestoexpandwithadvancesingenaigenerativeaimak", "somemachinelearningmodelssupportvectormachinesrandom", "ingitafieldwithvastresearchpotentialandnumerouspractical", "forestandlogisticregressionandwerefertothemasmachine", "applicationsnlpcontinuestoexpandduetoitsgreatcapabilities", "learningmodelswewillshowtheperformanceofboththestandard", "inunderstandinggeneratingandtransforminghumanlanguage", "andmachinelearningmodelsemployingdifferentembeddingstf", "somepopularexamplesarethegreatadvancesinlargelanguage", "idf", "glove", "regarding", "pretrained", "model", "use", "modelsllmslikechatgptmodelsgptgptogpto", "corresponding", "embeddings", "since", "model", "minigptturbogptetcllamamodelsllamallama", "predefinedembeddingsthestructureofthisworkisasfollows", "etcandothersthesemodelshaverevolutionizedthefield", "sectionreviewsrelevantworksestablishingthecontextforour", "thispaperisnotpublisheditisthefinalprojectreportforthecisneural", "worksectionintroducestheembeddingtechniquesusedsection", "computationtempleuniversityinstructorprofhongchanggao", "describesthedatasetandsectionoutlinesdatapreprocessing", "cisneuralcomputationfallphiladelphiapausa", "stepssectionsandcovertheneuralnetworkarchitecturesand", "copyrightheldbytheownerauthor", "machinelearningmodelsappliedsectionhighlightstheuseof", "ced", "glsc", "vvixracisneuralcomputationfallphiladelphiapausa", "petridisetal", "transferlearningtoenhanceperformanceresultsarepresentedin", "fseglovewikigigaworddatasetwhichprovidesdvec", "sectionfollowedbyadiscussioninsectionandaconclusion", "torstrainedonwikipediaandgigaworddatatheseembeddings", "withkeyfindingsinsection", "encoderichsemanticandsyntacticrelationshipssuchasanalogies", "andwordsimilaritieswhiletfidfemphasizestermimportance", "relatedwork", "inindividualdocumentsglovecapturesbroadersemanticrelation", "wehaveseenmanysurveysinthepastregardingtextclassification", "shipsfromtheentirecorpus", "methodsinawidesurveyispresentedondifferentclassifica", "dataset", "tionalgorithmsandembeddingstheypresentresultsfromother", "paperswithnoexplicitcomparisonbetweenembeddingsandmod", "dataset", "used", "implement", "experiment", "news", "elsinauthorsdemonstrateadeeplearningbasedreviewwhere", "datasetswhichcontainsexamplesarticlesfeaturesand", "theyexperimentwithvarioustextdatasetssuchasagnews", "classesforeachexamplecolumnsintotaltabledemonstrates", "newsgroupsreutersnewsandmanymoremoreovershowsa", "somedetailsregardingthedatasetandbrielydescribeseachcolumn", "comparisonofbertdistilbertrobertaxlnetandelectra", "howeveridentificationsuchasdataidandidtimestampsand", "foremotionrecognitionwheretheyusedbertastheirbaseline", "temporalinformationdonothelpasmuchasthecontentitself", "modelandcompareditwiththefouradditionaltransformerbased", "whentryingtoclassifyarticlesintocategoriesthereforewewill", "modelsdistillbertrobertaxlnetandelectraexceptfor", "useonlyfeaturessource", "title", "contentandauthor", "theelectramodelwhichhadtheworstfscoretheother", "modelshadverysimilarresultsrobertaachievedthebestfscore", "tabletheinitialdataset", "followedbydistillbertxlnetandthenbert", "inspiredbyalltheseworkswearepresentingacomparison", "columnname", "description", "betweenvariouspretrainedmodelsneuralnetworksandstandard", "modelsemployingtwodifferenttechniquesforembeddingstfidf", "dataid", "uniqueidentifiernumberforthearticle", "andglove", "id", "sourcedatearticle", "date", "dateassociatedwiththeentry", "embeddings", "source", "sourceofthearticle", "title", "titleofthearticle", "embeddings", "crucial", "aspect", "computer", "understand", "content", "maincontentofthearticle", "andgeneratehumanlanguagetheyrepresentwordsorphrasesas", "author", "authorofthearticle", "vectorsinahighdimensionalspacecapturingsemanticrelation", "url", "urllinktothearticle", "shipsandcontextualmeaningsinthissectionwewillexploretwo", "published", "dateandtimeofpublication", "techniquesforgeneratingembeddingstfidfandglove", "publishedutc", "theunixtimestampofthepublication", "termfrequencyinversedocumentfrequencytfidfisasta", "collectionutc", "theunixtimestampofthetimetheinci", "tistical", "method", "used", "evaluate", "importance", "word", "dentrecorded", "documentrelativetoacollectionofdocumentscorpusitiscalcu", "categorylevel", "levelcategory", "latedastheproductoftwocomponentstermfrequencytfand", "categorylevel", "levelcategory", "inversedocumentfrequencyidfthetermfrequencyisdefined", "tf", "cid", "distributionofclasses", "isthefrequencyofterm", "indocumentandthede", "thedistributionofclassesinthedatasetisshowninfigurewe", "nominatoristhetotalnumberoftermsin", "havelevelcategoriesandlevelcategoriesundereach", "theinversedocumentfrequencyisdefinedas", "level", "category", "approximately", "instance", "levelcategorybroadlyspeakingwecansaythatitisabalanced", "idflog", "datasetbecausewealwayshaveinstancesforacombinationof", "levelandlevelcategories", "total", "number", "document", "corpus", "isthenumberofdocumentscontainingterm", "datapreprocessing", "thetfidfscoreisthencomputedas", "thisstepisverycrucialintasksthatinvolvenlpthegoalisto", "tfidftfidf", "transformrawtextdataintoaformatthatcanbeusedbyma", "tfidffocusesoncapturingtheimportanceoftermsbypenal", "chinedeeplearningmodels", "izingfrequentlyoccurringtermsacrossthecorpusensuringthat", "mergethefeatures", "commonwordsliketheorandarenotoveremphasized", "globalvectorsforwordrepresentationgloveisapopular", "innlpwehavetomergealltheinformationegfeatureswe", "unsupervisedlearningalgorithmforgeneratingwordembeddings", "haveintoonefeaturecolumnthereforebeforetheactualdata", "unliketfidfglovecapturesthesemanticrelationshipsbetween", "preprocessingwehavetomergesource", "title", "contentand", "wordsbyanalyzingwordcooccurrencestatisticsinacorpusin", "authorintoonecolumnthereforethefinaldatasethascolumns", "work", "use", "pretrained", "glove", "embeddings", "theinformationofeacharticlelevelandlevelcategoriestextclassificationneuralnetworksvsmachinelearningmodelsvspretrainedmodels", "cisneuralcomputationfallphiladelphiapausa", "inasinglevectorrepresentationafterthatthesinglevector", "ispassedthroughafullyconnectedlayertopredicttheclass", "probabilitiesforthespecifiednumberofclassesor", "wealsohaveapplieddropoutduringembeddingand", "withinthetransformerlayerstoreduceoverfitting", "rnnarecurrentneuralnetworkwhichincludesan", "rnnlayerwithhiddenunitsandlayerfollowedby", "afullyconnectedlayertomapthelasthiddenstatetothe", "outputdimensionorduringtheforwardpassthe", "inputsequenceisreshapedtomatchtheexpecteddimen", "sionsandonlytheoutputfromthefinaltimestepispassed", "tothefullyconnectedlayer", "trainingphase", "figuredistributionoftheclassesinourdatasetitisalso", "training", "phase", "employ", "adam", "optimizer", "evident", "many", "level", "category", "crossentropy", "loss", "function", "epoch", "apply", "levelcategory", "torchoptimlrschedulerreducelronplateauasthelearningrate", "schedulerwhichdynamicallyadjuststhelearningratebasedonthe", "modelsperformanceonthetestsetpromotingstableconvergence", "datacleaningandtokenization", "westartwithlranditdecreasesbasedontheperformance", "havingalltheinformationinasinglefeaturethefirstthingthat", "minimumlredependingonavailabilityweuseeithergpu", "weneedtodoistoconvertalltexttolowercaseforconsistency", "cpu", "best", "model", "state", "saved", "evaluation", "sowordswithdifferentcasesaretreatedthesamethenwehave", "inourcasewewerefortunateenoughtohaveaccessinatesla", "toremovenumbersurlsspecialcharactersandpunctuationthat", "vsxmgbgpu", "addnoisetothedataanddonotcontributetoourtask", "neuralnetworksresults", "lemmatizationandstopwordsremoval", "aftereachepochthemodelisbeingevaluatedonunseendata", "step", "transforms", "word", "base", "dictionary", "form", "fromthetestsetwhereitspredictionsarecomparedtotheactual", "whichhelpsgroupsimilarwordstogetherandreducestheoverall", "labelstocalculateaccuracyboththetraininglossandtestaccuracy", "vocabularysizeforexampledifferentformsofawordlikegoing", "arerecordedtomonitorthemodelsprogressthefiguresbelow", "orwentarereducedtotheirrootformgoadditionallycom", "figureandshowthetrainingprocessandtheperformance", "monlyusedwordsknownasstopwordssuchastheaand", "oftheemployedneuralnetworksandtableshowstheirfinal", "isareremovedsincetheydonotusuallycontributesignificant", "test", "accuracy", "tfidf", "glove", "discussion", "regarding", "meaningtothecontextalloftheaboveareperformedusingfunc", "theresultswilltakeplacelaterwithalltheavailableresultsbeing", "tions", "wordnetlemmatizer", "stopwords", "presented", "nltklibraryafterthisstagewearereadyfortokenizationor", "embeddings", "neuralnetworks", "sinceweareusingthedvectorsforgloveembeddingsand", "alsoformaxfeaturesintfidfembeddingstheinputvector", "fortheneuralnetworksisgoingtobe", "mlpafeedforwardneuralnetworkwhichconsistsof", "twohiddenlayersandthenneuronswithrelu", "activationfunctionseachfollowedbyadropoutlayerto", "reduceoverfittingbyrandomlysettingofactivationsto", "zeroduringtrainingthefinallayeroutputspredictionsfor", "levelcategoryorlevelcategoryclassesusinga", "figuretraininglossontheleftandtestaccuracyon", "lineartransformation", "therightemployinggloveforlevelcategory", "transformeratransformerbasedarchitecturewhich", "begin", "applying", "linear", "embedding", "input", "data", "tomapittoadspacetheddataisthenprocessed", "machinelearningmodels", "throughastackoftransformerencoderlayerswhichuse", "selfattentionmechanismstocapturerelationshipsbetween", "itwouldbeveryhelpfultohaveresultsfromsomemachinelearn", "inputfeaturesafterthattheoutputisbeingprocessedby", "ingmlmodelsandthereforewedecidedtoemploysomeofthem", "takingthemeanacrossthesequencedimensionsresulting", "wearepresentingtheperformanceofthreemlmodelssupportcisneuralcomputationfallphiladelphiapausa", "petridisetal", "vectormachinesvmrandomforestclassifierandlogisticre", "gression", "hyperparametertuning", "hyperparametertuninginvolvesoptimizingtheparametersofthese", "threemachinelearningmodelstomaximizetheirperformanceclas", "sificationaccuracyagridsearchmethodologyisemployed", "systematicallyevaluatingvariousparametercombinationstoiden", "tifytheoptimalconfigurationwedonotexplicitlymentionthe", "parametersthatbeingtestedherebutfurtherdetailscanbefound", "inthedeliveredsourcecodepythonnotebooksinsteadintable", "weonlypresentthebesthyperparametersidentifiedforeach", "figuretraininglossontheleftandtestaccuracyon", "modelduringtheprocessofhyperparametertuningtheperfor", "therightemployingtfidfforlevelcategory", "manceforeverymodelisassessedusingfoldcrossvalidation", "ensuringreliableparameterselection", "kfoldcrossvalidation", "kfoldcrossvalidationisatechniqueusedtoassesstheperfor", "manceofamachinelearningmodelbydividingthedatasetinto", "ksubsetsorfoldsthemodelistrainedandevaluatedktimes", "eachtimeusingadifferentfoldasthetestingsetandtheremain", "ingkfoldsfortrainingthisprocesshelpsensureamorerobust", "evaluationofthemodelsperformanceasitconsidersmultiple", "combinationsoftrainingandtestingdatainourcasethefinalac", "curacyistheaverageoftheaccuraciescomputedineachiteration", "providingamorereliableestimateofthemodelsgeneralization", "figuretraininglossontheleftandtestaccuracyon", "ability", "therightemployinggloveforlevelcategory", "machinelearningresults", "havingthebestparametersforeachmodelweapplyfoldcross", "validationtobetterestimatetheirperformancetableandtable", "showthemeanaccuracyfromthefoldcrossvalidationforeach", "modelthestandarddeviationineveryexperimentisalmost", "discussionregardingtheresultswilltakeplacelaterwithallthe", "availableresultsbeingpresented", "tablefoldcrossvalidationresultsforglove", "classifier", "meanaccuracy", "level", "level", "figuretraininglossontheleftandtestaccuracyon", "svmclassifier", "therightemployingtfidfforlevelcategory", "randomforestclassifier", "logisticregressionclassifier", "tabletestaccuracyforbothtfidfandgloveaftertrain", "ing", "classifier", "tfidf", "glove", "tablefoldcrossvalidationresultsfortfidf", "level", "level", "level", "level", "mlp", "classifier", "meanaccuracy", "rnn", "level", "level", "transformerencoder", "svmclassifier", "randomforestclassifier", "logisticregressionclassifier", "textclassificationneuralnetworksvsmachinelearningmodelsvspretrainedmodels", "cisneuralcomputationfallphiladelphiapausa", "tablebesthyperparametersforeachmodelacrosscategoriesandmethodsaftergridsearch", "tfidfcategory", "glovecategory", "logisticregressioncmaxitersolverlbfgs", "logisticregressioncmaxitersolverliblinear", "randomforestmaxdepthnoneminsamplessplitnestimators", "randomforestmaxdepthminsamplessplitnestimators", "svmcgammascalekernelrbf", "svmcgammascalekernelrbf", "tfidfcategory", "glovecategory", "logisticregressioncmaxitersolverlbfgs", "logisticregressioncmaxitersolverlbfgs", "randomforestmaxdepthnoneminsamplessplitnestimators", "randomforestmaxdepthnoneminsamplessplitnestimators", "svmcgammascalekernelrbf", "svmcgammascalekernelrbf", "transferlearning", "electra", "predict", "token", "original", "transferlearningisamachinelearningtechniquewhereamodel", "whichonehasbeenreplaced", "trained", "one", "task", "adapted", "different", "quite", "tinybert", "model", "x", "smaller", "x", "similartaskthemainideaistoapplytheknowledgegainedfroma", "fasteroninferencethanbertbaseandachievescompetitive", "taskwithalotofdatatoataskthathaslimiteddataforinstancea", "performancesinthetasksofnaturallanguageunderstanding", "modeltrainedonalargedatasetofgeneralimagescanbefinetuned", "itperformsanoveltransformerdistillationatboththepre", "toidentifyspecifictypesofobjectswithonlyasmallamountof", "trainingandtaskspecificlearningstages", "labeleddatamostofthetimeoralwaysthisapproachleadsto", "albertalightweightversionofbertwithparam", "betterresultscomparedtotrainingamodelfromscratchinthe", "eterreductiontechniqueslikefactorizedembeddingsopti", "contextofnlptransferlearninghasrevolutionizedthefieldpre", "mizedforscalabilitywithoutsacrificingaccuracy", "trainedlanguagemodelssuchasbertandgptarefirsttrained", "pretrained", "tokenizers", "essential", "component", "onmassivecorporatolearngenerallanguagerepresentationsin", "transformerbased", "language", "model", "responsible", "ordertocapturethestructureofthehumanlanguagethesemodels", "forconvertingrawtextintonumericalrepresentationsthatcan", "arethenfinetunedonspecifictaskslikesentimentanalysistext", "processed", "model", "tokenizers", "specifically", "classificationorquestionansweringusingmuchsmallerdatasets", "trained", "align", "vocabulary", "tokenization", "strategy", "corresponding", "model", "ensuring", "optimal", "performance", "pretrainedmodels", "compatibility", "table", "show", "model", "used", "nowadaystherearemanyefficientlanguagemodelsavailablefor", "experimentstheircorrespondingtokenizersandtheinformation", "useinparticularwetesttheefficacyofdifferentmodelsaxlm", "forthepretrainedmodelweights", "robertabdistilbertcrobertadberteelectraf", "tinybertandgalberteachofthesemodelscanbeloadedwith", "pytorchdataformat", "theirpretrainedweightsallowinguserstoapplytheircapabilities", "wesplitthedatasetintotrainingandtestingsetsandprocesses", "outoftheboxforavarietyofnlptasks", "themintodataloaderobjectswhichareessentialforefficientbatch", "xlmrobertaamultilingualmodeltrainedon", "processingduringtrainingandevaluationweconverttheinput", "differentlanguagesunlikesomexlmmultilingualmodelsit", "dataandlabelsintopytorchtensorsthenwegenerateattention", "doesnotrequirelangtensorstounderstandwhichlanguage", "maskstoindicatenonpaddingtokensandcreatetensordataset", "isusedandshouldbeabletodeterminethecorrectlanguage", "objectsthesedatasetsarethenwrappedintodataloaderswith", "fromtheinputids", "specifiedbatchsizesensuringcompatibilitywithtransformer", "distilbertasmallfastcheapandlighttransformer", "model", "function", "also", "shuffle", "training", "data", "better", "modeltrainedbydistillingbertbaseithaslessparam", "generalization", "keep", "test", "data", "order", "consistent", "etersthangooglebertbertbaseuncasedrunsfaster", "evaluationfiguredemonstratesthewholeprocessinourpipeline", "whilepreservingoverofbertsperformances", "roberta", "optimized", "variant", "bert", "modifieskeyhyperparametersremovingthenextsentence", "pretrainingobjectiveandtrainingwithmuchlargermini", "batchesandlearningrates", "bertthefamousbidirectionaltransformerpretrained", "usingacombinationofmaskedlanguagemodelingobjective", "andnextsentencepredictiononalargecorpuscomprising", "thetorontobookcorpusandwikipedia", "electraapretrainedtransformermodelwiththe", "useofanothersmallmaskedlanguagemodeltheinputs", "figurepipelineforpreparingthedataforclassifiersneural", "arecorruptedbythatlanguagemodelwhichtakesaninput", "network", "textthatisrandomlymaskedandoutputsatextinwhichcisneuralcomputationfallphiladelphiapausa", "petridisetal", "tabletheemployedpretrainedmodelsandtokenizers", "model", "tokenizer", "pretrainedfrom", "albertforsequenceclassification", "alberttokenizer", "albertbasev", "automodelforsequenceclassification", "autotokenizer", "huaweinoahtinybertgeneralld", "electraforsequenceclassification", "electratokenizer", "googleelectrasmalldiscriminator", "bertforsequenceclassification", "berttokenizer", "bertbaseuncased", "robertaforsequenceclassification", "robertatokenizer", "robertabase", "distilbertforsequenceclassification", "distilberttokenizer", "distilbertbaseuncased", "xlmrobertaforsequenceclassification", "xlmrobertatokenizer", "xlmrobertabase", "finetuning", "oncethedataarereadyandtokenizedweproceedtofinetunethe", "pretrainedmodelsonourspecificdatasetfinetuninginvolves", "adaptingthepretrainedweightstothetargettaskthroughaddi", "tionaltrainingourtraininglooprunsforafixednumberofsix", "epochswhichwefoundtobesufficientforconvergenceinourex", "perimentsduringeachepochthefunctiontracksthetrainingloss", "andevaluatesthemodelsperformanceonaseparateunseentest", "datasetcalculatingaccuracyifthemodelachievesasignificantly", "betteraccuracyduringtrainingitsweightsaresavedandwecan", "restorethemattheendoftrainingthefunctionrestoresthese", "bestweightsforafinalevaluationensuringthatthemodelsper", "formanceisbasedonthemostoptimalparameterswealsorecord", "thetimetakenfortheentiretrainingprocessthisisespecially", "usefulforcomparingtheperformanceofdifferentmodelsduring", "experimentsfigureandfiguredemonstratetheperformance", "figuretestaccuracyduringtrainingepochs", "duringtrainingepochsforeachmodelforthetwodifferentcate", "goriestraininglossisavailableinthedeliveredpythonnotebook", "itisclearthatbythethirdorfourthepochtheperformanceof", "mostmodelsstabilizesindicatingthatadditionaltraininggivesus", "minimalornotevenbetterresultsthishighlightstheefficiency", "performanceforpretrainedmodels", "andreliabilityofpretrainedtransformermodelsfordownstream", "wetrainthemodelusingtheadamwoptimizerandcrossentropy", "task", "lossfunctioncalculatetheaveragetraininglossandevaluateper", "formanceaccuracyonthetestunseendatasetaftereachepoch", "alearningrateofeischosenwhichisacommonpracticein", "finetuningasitensuresgradualupdatestomodelweightswith", "outsignificantlychangingthepretrainedparametersduringthe", "epochsoftrainingtheepochsthatweapplyaresufficientfor", "finetuningasthemodelsarepretrainedonlargedatasetsallow", "ingthemtolearngenerallanguagerepresentationsthatrequire", "minimaladjustmentforthespecifictaskwesavethemodelsstate", "wheneverasignificantimprovementisobservedaftertraining", "thebestmodelweightsarerestoredensuringoptimalperformance", "thefinalaccuraciesaftertrainingareshownintableandtable", "along", "size", "model", "term", "number", "parametersandtimetakenforepochsoffinetuning", "result", "wealreadypresentedhowwetrainedandevaluatedallthemodels", "figuretestaccuracyduringtrainingepochs", "section", "presenting", "final", "performance", "termsofaccuracyofeachmodelthatweexperimentedwithtable", "andtableclearlydemonstratetheresultsforbothleveland", "levelcategorieslaterinsectionwediscussabouttheresultstextclassificationneuralnetworksvsmachinelearningmodelsvspretrainedmodels", "cisneuralcomputationfallphiladelphiapausa", "tableperformanceofpretrainedmodelsaftertraining", "tablefinalresultsforlevelcategory", "forlevelcategory", "model", "accuracy", "noof", "final", "finetuning", "model", "xlmroberta", "parameter", "accuracy", "timemin", "distilbert", "xlmroberta", "m", "roberta", "distilbert", "m", "bert", "roberta", "m", "electra", "bert", "m", "tinybert", "electra", "m", "albert", "tinybert", "m", "accuracytfidf", "accuracyglove", "albert", "m", "mlp", "rnn", "tableperformanceofpretrainedmodelsaftertraining", "transformerencoder", "forlevelcategory", "svm", "randomforest", "noof", "final", "finetuning", "logisticregression", "model", "parameter", "accuracy", "timemin", "xlmroberta", "m", "distilbert", "m", "roberta", "m", "distilbertshowedcomparableperformanceachievingaccura", "bert", "m", "ciesofandrespectivelycomparedtootherneural", "electra", "m", "networksandmachinelearningmodelstheseresultshighlightthe", "tinybert", "m", "robustnessofpretrainedmodelsinhandlingsuchclassification", "albert", "m", "task", "howeverweobserveaperformancegapbetweenleveland", "leveltasksforallmodelswithloweraccuracyinthelattercer", "tablefinalresultsforlevelcategory", "trainly", "increased", "complexity", "class", "insteadofinlevelwhichmeansthatthisisamoredifficult", "model", "accuracy", "challengeforbothpretrainedandtraditionalmodels", "traditionalmachinelearningmodelsshowedreasonableperfor", "xlmroberta", "mancewhenpairedwithtfidfandgloveembeddingsnotably", "distilbert", "thecombinationofgloveembeddingswiththesemodelsresultedin", "roberta", "improvedaccuraciessuggestingthatpretrainedwordembeddings", "bert", "canenhancetheeffectivenessoftraditionalmodelsforexample", "electra", "svmachievedaccuracyonleveltaskswithgloveem", "tinybert", "beddingssignificantlyhigherthanitsperformancewithtfidf", "albert", "whereitachievedaccuracyhoweverthesemodelsconsis", "accuracytfidf", "accuracyglove", "tentlyunderperformedcomparedtothepretrainedonesshowing", "thelimitationsoftraditionalapproachesweobservedaverybad", "mlp", "performancefromthealbertmodelinlevelcategorywhereit", "rnn", "achievedjustaccuracywhichissomethingverystrangetaking", "transformerencoder", "intoconsiderationitsperformanceinlevelcategoryonepossible", "svm", "reasonisthatitprobablyneedsmoreepochsduringfinetuning", "randomforest", "howeverwedidnthavethegpucapacitytoexperimentmoreon", "logisticregression", "thisandwekepteverythingconsistentatepochs", "keyobservations", "discussion", "numberofparametersforpretrainedmodelswhile", "theresultsindicatethatpretrainedtransformermodelsoutper", "modelsliketinybertandalbertofferefficiencydueto", "formtraditionalmodelsacrossbothcategoriesamongpretrained", "theirsmallersizestheirperformancewaslowercomparedto", "modelsbertachievedthehighestaccuracyinthelevel", "largermodelssuchasbertandrobertathishighlightsa", "categoryanddemonstratesitsstrongcapabilitytocapturecon", "tradeoffbetweenmodelsizeandperformanceparticularly", "textualinformationsimilarlyinthelevelcategorybertand", "incomplexclassificationtaskssuchaslevelcategorycisneuralcomputationfallphiladelphiapausa", "petridisetal", "embeddingchoicematterstheresultsfortfidfand", "niharranjankaushalmundadakunalphaltaneandsaimahmad", "gloveembeddingsshowtheimportanceoffeaturerepresen", "surveyontechniquesinnlpinternationaljournalofcomputerapplications", "tationintraditionalmodelsgloveembeddingswiththeir", "vsanhdistilbertadistilledversionofbertsmallerfastercheaper", "pretrainedinformationconsistentlyoutperformedtfidf", "andlighterarxivpreprintarxiv", "acrossbothlevelandlevelcategories", "andreassteinerandrsusanopintomichaeltschannendanielkeysersxiao", "wangyonatanbittonalexeygritsenkomatthiasmindereranthonysherbondy", "shangbanglongetal", "paligemmaafamilyofversatilevlmsfor", "conclusion", "transferarxivpreprintarxiv", "xiaobingsunxiangyueliujiajunhuandjunwuzhuempiricalstudies", "inthisworkweevaluatedtheperformanceofpretrainedtrans", "onthenlptechniquesforsourcecodedatapreprocessinginproceedingsofthe", "formermodelsandtraditionalmachinelearningapproacheson", "rdinternationalworkshoponevidentialassessmentofsoftwaretechnologies", "twoclassificationtaskslevelandlevelourfindingsdemon", "hindtaudandjeanfranccoismasmultilayerperceptronmlpgeomatic", "stratethatpretrainedmodelssuchasbertandrobertacon", "approachesformodelinglandchangescenarios", "avaswani", "attentionisallyouneed", "advancesinneuralinformation", "sistentlyoutperformtraditionalmodelshoweverweobserveda", "processingsystems", "performance", "decrease", "model", "complex", "task", "leveladditionallytraditionalmodelspairedwithglovepre", "trainedembeddingsshowedcompetitiveperformanceinsimpler", "taskspointingouttheimportanceofembeddingqualityinfeature", "basedapproachesitisalsonoteworthytosaythatifwedonot", "careaboutaccuracyandcorrectpredictionsingeneralandwehave", "computationallimitationsitisbettertogowithastandardma", "chinelearningmodelwithglovewhichissignificantlysmaller", "andeasiertodeploycomparedtopretrainedonesoverallthefind", "ingsemphasizethedominanceofpretrainedtransformerbased", "architecturesfortextclassificationtasks", "reference", "sabeenahmedianenielsenaakashtripathishamoonsiddiquiravipra", "machandranandghulamrasooltransformersintimeseriesanalysisa", "tutorialcircuitssystemsandsignalprocessing", "danguitalucaghelardonialessandroghiolonetoandsandroridella", "thekinkfoldcrossvalidationintheeuropeansymposiumonartificial", "neuralnetworks", "kclarkelectrapretrainingtextencodersasdiscriminatorsratherthan", "generatorsarxivpreprintarxiv", "aconneauunsupervisedcrosslingualrepresentationlearningatscale", "arxivpreprintarxiv", "diogocortizexploringtransformersmodelsforemotionrecognitiona", "comparisionofbertdistilbertrobertaxlnetandelectrainproceed", "ingsoftherdinternationalconferenceoncontrolroboticsandintelligent", "systemvirtualeventchinaccrisassociationforcomputingmachinery", "newyorknyusa", "httpsdoiorg", "sonainjamilmdjalilpiranandohjinkwonacomprehensivesurveyof", "transformersforcomputervisiondrones", "xiaoqijiaoyichunyinlifengshangxinjiangxiaochenlinlinlifangwang", "andqunliutinybertdistillingbertfornaturallanguageunderstanding", "arxivpreprintarxiv", "thorstenjoachimsetalaprobabilisticanalysisoftherocchioalgorithm", "withtfidffortextcategorizationinicmlvolciteseer", "jacobdevlinmingweichangkentonandleekristinatoutanovabert", "pretrainingofdeepbidirectionaltransformersforlanguageunderstandingin", "proceedingsofnaaclhltvolminneapolisminnesota", "kamrankowsarikianajafarimeimandimojtabaheidarysafasanjanamendu", "laurabarnesanddonaldbrowntextclassificationalgorithmsasurvey", "information", "zlanalbertalitebertforselfsupervisedlearningoflanguagerepresen", "tationsarxivpreprintarxiv", "yinhanliurobertaarobustlyoptimizedbertpretrainingapproacharxiv", "preprintarxiv", "larryrmedskerlakhmijainetalrecurrentneuralnetworksdesign", "andapplications", "shervin", "minaee", "nal", "kalchbrenner", "erik", "cambria", "narjes", "nikzad", "meysam", "chenaghluandjianfenggao", "deeplearningbasedtextclassification", "acomprehensivereviewacmcomputingsurveyscsur", "jeffreypenningtonrichardsocherandchristopherdmanningglove", "globalvectorsforwordrepresentationinproceedingsoftheconferenceon", "empiricalmethodsinnaturallanguageprocessingemnlp", "philippprobstannelaureboulesteixandberndbischltunabilityim", "portanceofhyperparametersofmachinelearningalgorithmsjournalofmachine", "learningresearch"], "sentences": ["text classification neural network v machine learning model v pretrained model christospetridis christospetridistempleedu templeuniversity philadelphiapennsylvaniausa abstract byachievinglevelsoffluencycomprehensionandcontextualun textclassificationisaverycommontasknowadaysandthereare derstandinglikeneverbeforellmshavetransformedthefield manyefficientmethodsandalgorithmsthatwecanemploytoac ofnlpwiththeirabilitytolearnpatternsintextfrommassive complishittransformershaverevolutionizedthefieldofdeep datasetswehaveseenresultsthatwerenotpossiblewithearlier learningparticularlyinnaturallanguageprocessingnlpand rulebasedmodelsllmscangeneratehumanliketextunderstand haverapidlyexpandedtootherdomainssuchascomputervision complexqueriesandevenperformtasksonwhichtheywerenot timeseriesanalysisandmorethetransformermodel explicitlytrainedthankstotheirpretrainedandtransferlearning wasfirstlyintroducedinthecontextofmachinetranslationand capability itsarchitecturereliesonselfattentionmechanismstocapture computer course ai model understand gener complexrelationshipswithindatasequencesitisabletohandle atehumanlanguagethroughtheuseofembeddingsembeddings longrangedependenciesmoreeffectivelythantraditionalneural innlprefer todensevectorrepresentationsofwords phrase networkssuchasrecurrentneuralnetworksandmultilayerper orevenentiresentencesthatcapturesemanticmeaningandcon ceptronsinthisworkwepresentacomparisonbetweendifferent textualrelationshipswithinanumericalformatthatmodelscan techniquestoperformtextclassificationwetakeintoconsideration understandunliketraditionalonehotencodingmethodswhich sevenpretrainedmodelsthreestandardneuralnetworksandthree areverysparseandfailtoconveyanyinformationabouthowwords machinelearningmodelsforstandardneuralnetworksandma relatetoeachotherembeddingsencodewordsintocontinuous chinelearningmodelswealsocomparetwoembeddingtechniques lowdimensionalspacesthisallowsmodelstomapsemantically tfidfandglovewiththelatterconsistentlyoutperformingthe similarwordsclosetoeachotherinthisspaceforexamplethe formerfinallywedemonstratetheresultsfromourexperiments wordskingandqueenmayappearclosertoeachotherinthe wherepretrainedmodelssuchasbertanddistilbertalways vectorspacethankingandcarbecauseoftheirsemanticand performbetterthanstandardmodelsalgorithms contextualsimilaritiesembeddingsarelearnedbymodelsbasedon largecorporaoftextmakingthemeffectiveatcapturingintricate keywords relationshipspatternsandlinguisticpropertiesfamousembed ding technique wordvec glove global vector textclassificationtransformerspretrainedmodelsnaturallan contextualembeddingsfromtransformerbasedmodelsrepresenta guageprocessingembeddings wordincontextconsideringthesurroundingwordsinasentence referenceformat thismeansthatthewordbankwouldhavedifferentembeddings christospetridistextclassificationneuralnetworksvsmachine dependingonwhetheritisusedinthecontextofariverbankora learningmodelsvspretrainedmodels cisneuralcomputation financialinstitutionitisclearthatthoseembeddingsofferbetter templeuniversityphiladelphiapausa resultsinnlptasksthanolderstaticembeddingssuchastfidf introduction termfrequencyinversedocumentfrequency inthisworkwepresentaperformancecomparisonbetween nlphasbeenarapidlygrowingareaduetoitsimpactonhow pretrainedmodelsandstandardmodelsonaclassificationtask humansinteractwithtechnologywehaveseenmanyapplications developed three neural network experiment rangingfromvoiceactivatedassistantsandchatbotstoinformation includingmlprnnandtransformerencodermodelsandwerefer retrievaltextsummarizationandsentimentanalysisitsrelevance tothemasstandardmodelswewillalsopresenttheperformanceof continuestoexpandwithadvancesingenaigenerativeaimak somemachinelearningmodelssupportvectormachinesrandom ingitafieldwithvastresearchpotentialandnumerouspractical forestandlogisticregressionandwerefertothemasmachine applicationsnlpcontinuestoexpandduetoitsgreatcapabilities learningmodelswewillshowtheperformanceofboththestandard inunderstandinggeneratingandtransforminghumanlanguage andmachinelearningmodelsemployingdifferentembeddingstf somepopularexamplesarethegreatadvancesinlargelanguage idf glove regarding pretrained model use modelsllmslikechatgptmodelsgptgptogpto corresponding embeddings since model minigptturbogptetcllamamodelsllamallama predefinedembeddingsthestructureofthisworkisasfollows etcandothersthesemodelshaverevolutionizedthefield sectionreviewsrelevantworksestablishingthecontextforour thispaperisnotpublisheditisthefinalprojectreportforthecisneural worksectionintroducestheembeddingtechniquesusedsection computationtempleuniversityinstructorprofhongchanggao describesthedatasetandsectionoutlinesdatapreprocessing cisneuralcomputationfallphiladelphiapausa stepssectionsandcovertheneuralnetworkarchitecturesand copyrightheldbytheownerauthor machinelearningmodelsappliedsectionhighlightstheuseof ced glsc vvixracisneuralcomputationfallphiladelphiapausa petridisetal transferlearningtoenhanceperformanceresultsarepresentedin fseglovewikigigaworddatasetwhichprovidesdvec sectionfollowedbyadiscussioninsectionandaconclusion torstrainedonwikipediaandgigaworddatatheseembeddings withkeyfindingsinsection encoderichsemanticandsyntacticrelationshipssuchasanalogies andwordsimilaritieswhiletfidfemphasizestermimportance relatedwork inindividualdocumentsglovecapturesbroadersemanticrelation wehaveseenmanysurveysinthepastregardingtextclassification shipsfromtheentirecorpus methodsinawidesurveyispresentedondifferentclassifica dataset tionalgorithmsandembeddingstheypresentresultsfromother paperswithnoexplicitcomparisonbetweenembeddingsandmod dataset used implement experiment news elsinauthorsdemonstrateadeeplearningbasedreviewwhere datasetswhichcontainsexamplesarticlesfeaturesand theyexperimentwithvarioustextdatasetssuchasagnews classesforeachexamplecolumnsintotaltabledemonstrates newsgroupsreutersnewsandmanymoremoreovershowsa somedetailsregardingthedatasetandbrielydescribeseachcolumn comparisonofbertdistilbertrobertaxlnetandelectra howeveridentificationsuchasdataidandidtimestampsand foremotionrecognitionwheretheyusedbertastheirbaseline temporalinformationdonothelpasmuchasthecontentitself modelandcompareditwiththefouradditionaltransformerbased whentryingtoclassifyarticlesintocategoriesthereforewewill modelsdistillbertrobertaxlnetandelectraexceptfor useonlyfeaturessource title contentandauthor theelectramodelwhichhadtheworstfscoretheother modelshadverysimilarresultsrobertaachievedthebestfscore tabletheinitialdataset followedbydistillbertxlnetandthenbert inspiredbyalltheseworkswearepresentingacomparison columnname description betweenvariouspretrainedmodelsneuralnetworksandstandard modelsemployingtwodifferenttechniquesforembeddingstfidf dataid uniqueidentifiernumberforthearticle andglove id sourcedatearticle date dateassociatedwiththeentry embeddings source sourceofthearticle title titleofthearticle embeddings crucial aspect computer understand content maincontentofthearticle andgeneratehumanlanguagetheyrepresentwordsorphrasesas author authorofthearticle vectorsinahighdimensionalspacecapturingsemanticrelation url urllinktothearticle shipsandcontextualmeaningsinthissectionwewillexploretwo published dateandtimeofpublication techniquesforgeneratingembeddingstfidfandglove publishedutc theunixtimestampofthepublication termfrequencyinversedocumentfrequencytfidfisasta collectionutc theunixtimestampofthetimetheinci tistical method used evaluate importance word dentrecorded documentrelativetoacollectionofdocumentscorpusitiscalcu categorylevel levelcategory latedastheproductoftwocomponentstermfrequencytfand categorylevel levelcategory inversedocumentfrequencyidfthetermfrequencyisdefined tf cid distributionofclasses isthefrequencyofterm indocumentandthede thedistributionofclassesinthedatasetisshowninfigurewe nominatoristhetotalnumberoftermsin havelevelcategoriesandlevelcategoriesundereach theinversedocumentfrequencyisdefinedas level category approximately instance levelcategorybroadlyspeakingwecansaythatitisabalanced idflog datasetbecausewealwayshaveinstancesforacombinationof levelandlevelcategories total number document corpus isthenumberofdocumentscontainingterm datapreprocessing thetfidfscoreisthencomputedas thisstepisverycrucialintasksthatinvolvenlpthegoalisto tfidftfidf transformrawtextdataintoaformatthatcanbeusedbyma tfidffocusesoncapturingtheimportanceoftermsbypenal chinedeeplearningmodels izingfrequentlyoccurringtermsacrossthecorpusensuringthat mergethefeatures commonwordsliketheorandarenotoveremphasized globalvectorsforwordrepresentationgloveisapopular innlpwehavetomergealltheinformationegfeatureswe unsupervisedlearningalgorithmforgeneratingwordembeddings haveintoonefeaturecolumnthereforebeforetheactualdata unliketfidfglovecapturesthesemanticrelationshipsbetween preprocessingwehavetomergesource title contentand wordsbyanalyzingwordcooccurrencestatisticsinacorpusin authorintoonecolumnthereforethefinaldatasethascolumns work use pretrained glove embeddings theinformationofeacharticlelevelandlevelcategoriestextclassificationneuralnetworksvsmachinelearningmodelsvspretrainedmodels cisneuralcomputationfallphiladelphiapausa inasinglevectorrepresentationafterthatthesinglevector ispassedthroughafullyconnectedlayertopredicttheclass probabilitiesforthespecifiednumberofclassesor wealsohaveapplieddropoutduringembeddingand withinthetransformerlayerstoreduceoverfitting rnnarecurrentneuralnetworkwhichincludesan rnnlayerwithhiddenunitsandlayerfollowedby afullyconnectedlayertomapthelasthiddenstatetothe outputdimensionorduringtheforwardpassthe inputsequenceisreshapedtomatchtheexpecteddimen sionsandonlytheoutputfromthefinaltimestepispassed tothefullyconnectedlayer trainingphase figuredistributionoftheclassesinourdatasetitisalso training phase employ adam optimizer evident many level category crossentropy loss function epoch apply levelcategory torchoptimlrschedulerreducelronplateauasthelearningrate schedulerwhichdynamicallyadjuststhelearningratebasedonthe modelsperformanceonthetestsetpromotingstableconvergence datacleaningandtokenization westartwithlranditdecreasesbasedontheperformance havingalltheinformationinasinglefeaturethefirstthingthat minimumlredependingonavailabilityweuseeithergpu weneedtodoistoconvertalltexttolowercaseforconsistency cpu best model state saved evaluation sowordswithdifferentcasesaretreatedthesamethenwehave inourcasewewerefortunateenoughtohaveaccessinatesla toremovenumbersurlsspecialcharactersandpunctuationthat vsxmgbgpu addnoisetothedataanddonotcontributetoourtask neuralnetworksresults lemmatizationandstopwordsremoval aftereachepochthemodelisbeingevaluatedonunseendata step transforms word base dictionary form fromthetestsetwhereitspredictionsarecomparedtotheactual whichhelpsgroupsimilarwordstogetherandreducestheoverall labelstocalculateaccuracyboththetraininglossandtestaccuracy vocabularysizeforexampledifferentformsofawordlikegoing arerecordedtomonitorthemodelsprogressthefiguresbelow orwentarereducedtotheirrootformgoadditionallycom figureandshowthetrainingprocessandtheperformance monlyusedwordsknownasstopwordssuchastheaand oftheemployedneuralnetworksandtableshowstheirfinal isareremovedsincetheydonotusuallycontributesignificant test accuracy tfidf glove discussion regarding meaningtothecontextalloftheaboveareperformedusingfunc theresultswilltakeplacelaterwithalltheavailableresultsbeing tions wordnetlemmatizer stopwords presented nltklibraryafterthisstagewearereadyfortokenizationor embeddings neuralnetworks sinceweareusingthedvectorsforgloveembeddingsand alsoformaxfeaturesintfidfembeddingstheinputvector fortheneuralnetworksisgoingtobe mlpafeedforwardneuralnetworkwhichconsistsof twohiddenlayersandthenneuronswithrelu activationfunctionseachfollowedbyadropoutlayerto reduceoverfittingbyrandomlysettingofactivationsto zeroduringtrainingthefinallayeroutputspredictionsfor levelcategoryorlevelcategoryclassesusinga figuretraininglossontheleftandtestaccuracyon lineartransformation therightemployinggloveforlevelcategory transformeratransformerbasedarchitecturewhich begin applying linear embedding input data tomapittoadspacetheddataisthenprocessed machinelearningmodels throughastackoftransformerencoderlayerswhichuse selfattentionmechanismstocapturerelationshipsbetween itwouldbeveryhelpfultohaveresultsfromsomemachinelearn inputfeaturesafterthattheoutputisbeingprocessedby ingmlmodelsandthereforewedecidedtoemploysomeofthem takingthemeanacrossthesequencedimensionsresulting wearepresentingtheperformanceofthreemlmodelssupportcisneuralcomputationfallphiladelphiapausa petridisetal vectormachinesvmrandomforestclassifierandlogisticre gression hyperparametertuning hyperparametertuninginvolvesoptimizingtheparametersofthese threemachinelearningmodelstomaximizetheirperformanceclas sificationaccuracyagridsearchmethodologyisemployed systematicallyevaluatingvariousparametercombinationstoiden tifytheoptimalconfigurationwedonotexplicitlymentionthe parametersthatbeingtestedherebutfurtherdetailscanbefound inthedeliveredsourcecodepythonnotebooksinsteadintable weonlypresentthebesthyperparametersidentifiedforeach figuretraininglossontheleftandtestaccuracyon modelduringtheprocessofhyperparametertuningtheperfor therightemployingtfidfforlevelcategory manceforeverymodelisassessedusingfoldcrossvalidation ensuringreliableparameterselection kfoldcrossvalidation kfoldcrossvalidationisatechniqueusedtoassesstheperfor manceofamachinelearningmodelbydividingthedatasetinto ksubsetsorfoldsthemodelistrainedandevaluatedktimes eachtimeusingadifferentfoldasthetestingsetandtheremain ingkfoldsfortrainingthisprocesshelpsensureamorerobust evaluationofthemodelsperformanceasitconsidersmultiple combinationsoftrainingandtestingdatainourcasethefinalac curacyistheaverageoftheaccuraciescomputedineachiteration providingamorereliableestimateofthemodelsgeneralization figuretraininglossontheleftandtestaccuracyon ability therightemployinggloveforlevelcategory machinelearningresults havingthebestparametersforeachmodelweapplyfoldcross validationtobetterestimatetheirperformancetableandtable showthemeanaccuracyfromthefoldcrossvalidationforeach modelthestandarddeviationineveryexperimentisalmost discussionregardingtheresultswilltakeplacelaterwithallthe availableresultsbeingpresented tablefoldcrossvalidationresultsforglove classifier meanaccuracy level level figuretraininglossontheleftandtestaccuracyon svmclassifier therightemployingtfidfforlevelcategory randomforestclassifier logisticregressionclassifier tabletestaccuracyforbothtfidfandgloveaftertrain ing classifier tfidf glove tablefoldcrossvalidationresultsfortfidf level level level level mlp classifier meanaccuracy rnn level level transformerencoder svmclassifier randomforestclassifier logisticregressionclassifier textclassificationneuralnetworksvsmachinelearningmodelsvspretrainedmodels cisneuralcomputationfallphiladelphiapausa tablebesthyperparametersforeachmodelacrosscategoriesandmethodsaftergridsearch tfidfcategory glovecategory logisticregressioncmaxitersolverlbfgs logisticregressioncmaxitersolverliblinear randomforestmaxdepthnoneminsamplessplitnestimators randomforestmaxdepthminsamplessplitnestimators svmcgammascalekernelrbf svmcgammascalekernelrbf tfidfcategory glovecategory logisticregressioncmaxitersolverlbfgs logisticregressioncmaxitersolverlbfgs randomforestmaxdepthnoneminsamplessplitnestimators randomforestmaxdepthnoneminsamplessplitnestimators svmcgammascalekernelrbf svmcgammascalekernelrbf transferlearning electra predict token original transferlearningisamachinelearningtechniquewhereamodel whichonehasbeenreplaced trained one task adapted different quite tinybert model x smaller x similartaskthemainideaistoapplytheknowledgegainedfroma fasteroninferencethanbertbaseandachievescompetitive taskwithalotofdatatoataskthathaslimiteddataforinstancea performancesinthetasksofnaturallanguageunderstanding modeltrainedonalargedatasetofgeneralimagescanbefinetuned itperformsanoveltransformerdistillationatboththepre toidentifyspecifictypesofobjectswithonlyasmallamountof trainingandtaskspecificlearningstages labeleddatamostofthetimeoralwaysthisapproachleadsto albertalightweightversionofbertwithparam betterresultscomparedtotrainingamodelfromscratchinthe eterreductiontechniqueslikefactorizedembeddingsopti contextofnlptransferlearninghasrevolutionizedthefieldpre mizedforscalabilitywithoutsacrificingaccuracy trainedlanguagemodelssuchasbertandgptarefirsttrained pretrained tokenizers essential component onmassivecorporatolearngenerallanguagerepresentationsin transformerbased language model responsible ordertocapturethestructureofthehumanlanguagethesemodels forconvertingrawtextintonumericalrepresentationsthatcan arethenfinetunedonspecifictaskslikesentimentanalysistext processed model tokenizers specifically classificationorquestionansweringusingmuchsmallerdatasets trained align vocabulary tokenization strategy corresponding model ensuring optimal performance pretrainedmodels compatibility table show model used nowadaystherearemanyefficientlanguagemodelsavailablefor experimentstheircorrespondingtokenizersandtheinformation useinparticularwetesttheefficacyofdifferentmodelsaxlm forthepretrainedmodelweights robertabdistilbertcrobertadberteelectraf tinybertandgalberteachofthesemodelscanbeloadedwith pytorchdataformat theirpretrainedweightsallowinguserstoapplytheircapabilities wesplitthedatasetintotrainingandtestingsetsandprocesses outoftheboxforavarietyofnlptasks themintodataloaderobjectswhichareessentialforefficientbatch xlmrobertaamultilingualmodeltrainedon processingduringtrainingandevaluationweconverttheinput differentlanguagesunlikesomexlmmultilingualmodelsit dataandlabelsintopytorchtensorsthenwegenerateattention doesnotrequirelangtensorstounderstandwhichlanguage maskstoindicatenonpaddingtokensandcreatetensordataset isusedandshouldbeabletodeterminethecorrectlanguage objectsthesedatasetsarethenwrappedintodataloaderswith fromtheinputids specifiedbatchsizesensuringcompatibilitywithtransformer distilbertasmallfastcheapandlighttransformer model function also shuffle training data better modeltrainedbydistillingbertbaseithaslessparam generalization keep test data order consistent etersthangooglebertbertbaseuncasedrunsfaster evaluationfiguredemonstratesthewholeprocessinourpipeline whilepreservingoverofbertsperformances roberta optimized variant bert modifieskeyhyperparametersremovingthenextsentence pretrainingobjectiveandtrainingwithmuchlargermini batchesandlearningrates bertthefamousbidirectionaltransformerpretrained usingacombinationofmaskedlanguagemodelingobjective andnextsentencepredictiononalargecorpuscomprising thetorontobookcorpusandwikipedia electraapretrainedtransformermodelwiththe useofanothersmallmaskedlanguagemodeltheinputs figurepipelineforpreparingthedataforclassifiersneural arecorruptedbythatlanguagemodelwhichtakesaninput network textthatisrandomlymaskedandoutputsatextinwhichcisneuralcomputationfallphiladelphiapausa petridisetal tabletheemployedpretrainedmodelsandtokenizers model tokenizer pretrainedfrom albertforsequenceclassification alberttokenizer albertbasev automodelforsequenceclassification autotokenizer huaweinoahtinybertgeneralld electraforsequenceclassification electratokenizer googleelectrasmalldiscriminator bertforsequenceclassification berttokenizer bertbaseuncased robertaforsequenceclassification robertatokenizer robertabase distilbertforsequenceclassification distilberttokenizer distilbertbaseuncased xlmrobertaforsequenceclassification xlmrobertatokenizer xlmrobertabase finetuning oncethedataarereadyandtokenizedweproceedtofinetunethe pretrainedmodelsonourspecificdatasetfinetuninginvolves adaptingthepretrainedweightstothetargettaskthroughaddi tionaltrainingourtraininglooprunsforafixednumberofsix epochswhichwefoundtobesufficientforconvergenceinourex perimentsduringeachepochthefunctiontracksthetrainingloss andevaluatesthemodelsperformanceonaseparateunseentest datasetcalculatingaccuracyifthemodelachievesasignificantly betteraccuracyduringtrainingitsweightsaresavedandwecan restorethemattheendoftrainingthefunctionrestoresthese bestweightsforafinalevaluationensuringthatthemodelsper formanceisbasedonthemostoptimalparameterswealsorecord thetimetakenfortheentiretrainingprocessthisisespecially usefulforcomparingtheperformanceofdifferentmodelsduring experimentsfigureandfiguredemonstratetheperformance figuretestaccuracyduringtrainingepochs duringtrainingepochsforeachmodelforthetwodifferentcate goriestraininglossisavailableinthedeliveredpythonnotebook itisclearthatbythethirdorfourthepochtheperformanceof mostmodelsstabilizesindicatingthatadditionaltraininggivesus minimalornotevenbetterresultsthishighlightstheefficiency performanceforpretrainedmodels andreliabilityofpretrainedtransformermodelsfordownstream wetrainthemodelusingtheadamwoptimizerandcrossentropy task lossfunctioncalculatetheaveragetraininglossandevaluateper formanceaccuracyonthetestunseendatasetaftereachepoch alearningrateofeischosenwhichisacommonpracticein finetuningasitensuresgradualupdatestomodelweightswith outsignificantlychangingthepretrainedparametersduringthe epochsoftrainingtheepochsthatweapplyaresufficientfor finetuningasthemodelsarepretrainedonlargedatasetsallow ingthemtolearngenerallanguagerepresentationsthatrequire minimaladjustmentforthespecifictaskwesavethemodelsstate wheneverasignificantimprovementisobservedaftertraining thebestmodelweightsarerestoredensuringoptimalperformance thefinalaccuraciesaftertrainingareshownintableandtable along size model term number parametersandtimetakenforepochsoffinetuning result wealreadypresentedhowwetrainedandevaluatedallthemodels figuretestaccuracyduringtrainingepochs section presenting final performance termsofaccuracyofeachmodelthatweexperimentedwithtable andtableclearlydemonstratetheresultsforbothleveland levelcategorieslaterinsectionwediscussabouttheresultstextclassificationneuralnetworksvsmachinelearningmodelsvspretrainedmodels cisneuralcomputationfallphiladelphiapausa tableperformanceofpretrainedmodelsaftertraining tablefinalresultsforlevelcategory forlevelcategory model accuracy noof final finetuning model xlmroberta parameter accuracy timemin distilbert xlmroberta m roberta distilbert m bert roberta m electra bert m tinybert electra m albert tinybert m accuracytfidf accuracyglove albert m mlp rnn tableperformanceofpretrainedmodelsaftertraining transformerencoder forlevelcategory svm randomforest noof final finetuning logisticregression model parameter accuracy timemin xlmroberta m distilbert m roberta m distilbertshowedcomparableperformanceachievingaccura bert m ciesofandrespectivelycomparedtootherneural electra m networksandmachinelearningmodelstheseresultshighlightthe tinybert m robustnessofpretrainedmodelsinhandlingsuchclassification albert m task howeverweobserveaperformancegapbetweenleveland leveltasksforallmodelswithloweraccuracyinthelattercer tablefinalresultsforlevelcategory trainly increased complexity class insteadofinlevelwhichmeansthatthisisamoredifficult model accuracy challengeforbothpretrainedandtraditionalmodels traditionalmachinelearningmodelsshowedreasonableperfor xlmroberta mancewhenpairedwithtfidfandgloveembeddingsnotably distilbert thecombinationofgloveembeddingswiththesemodelsresultedin roberta improvedaccuraciessuggestingthatpretrainedwordembeddings bert canenhancetheeffectivenessoftraditionalmodelsforexample electra svmachievedaccuracyonleveltaskswithgloveem tinybert beddingssignificantlyhigherthanitsperformancewithtfidf albert whereitachievedaccuracyhoweverthesemodelsconsis accuracytfidf accuracyglove tentlyunderperformedcomparedtothepretrainedonesshowing thelimitationsoftraditionalapproachesweobservedaverybad mlp performancefromthealbertmodelinlevelcategorywhereit rnn achievedjustaccuracywhichissomethingverystrangetaking transformerencoder intoconsiderationitsperformanceinlevelcategoryonepossible svm reasonisthatitprobablyneedsmoreepochsduringfinetuning randomforest howeverwedidnthavethegpucapacitytoexperimentmoreon logisticregression thisandwekepteverythingconsistentatepochs keyobservations discussion numberofparametersforpretrainedmodelswhile theresultsindicatethatpretrainedtransformermodelsoutper modelsliketinybertandalbertofferefficiencydueto formtraditionalmodelsacrossbothcategoriesamongpretrained theirsmallersizestheirperformancewaslowercomparedto modelsbertachievedthehighestaccuracyinthelevel largermodelssuchasbertandrobertathishighlightsa categoryanddemonstratesitsstrongcapabilitytocapturecon tradeoffbetweenmodelsizeandperformanceparticularly textualinformationsimilarlyinthelevelcategorybertand incomplexclassificationtaskssuchaslevelcategorycisneuralcomputationfallphiladelphiapausa petridisetal embeddingchoicematterstheresultsfortfidfand niharranjankaushalmundadakunalphaltaneandsaimahmad gloveembeddingsshowtheimportanceoffeaturerepresen surveyontechniquesinnlpinternationaljournalofcomputerapplications tationintraditionalmodelsgloveembeddingswiththeir vsanhdistilbertadistilledversionofbertsmallerfastercheaper pretrainedinformationconsistentlyoutperformedtfidf andlighterarxivpreprintarxiv acrossbothlevelandlevelcategories andreassteinerandrsusanopintomichaeltschannendanielkeysersxiao wangyonatanbittonalexeygritsenkomatthiasmindereranthonysherbondy shangbanglongetal paligemmaafamilyofversatilevlmsfor conclusion transferarxivpreprintarxiv xiaobingsunxiangyueliujiajunhuandjunwuzhuempiricalstudies inthisworkweevaluatedtheperformanceofpretrainedtrans onthenlptechniquesforsourcecodedatapreprocessinginproceedingsofthe formermodelsandtraditionalmachinelearningapproacheson rdinternationalworkshoponevidentialassessmentofsoftwaretechnologies twoclassificationtaskslevelandlevelourfindingsdemon hindtaudandjeanfranccoismasmultilayerperceptronmlpgeomatic stratethatpretrainedmodelssuchasbertandrobertacon approachesformodelinglandchangescenarios avaswani attentionisallyouneed advancesinneuralinformation sistentlyoutperformtraditionalmodelshoweverweobserveda processingsystems performance decrease model complex task leveladditionallytraditionalmodelspairedwithglovepre trainedembeddingsshowedcompetitiveperformanceinsimpler taskspointingouttheimportanceofembeddingqualityinfeature basedapproachesitisalsonoteworthytosaythatifwedonot careaboutaccuracyandcorrectpredictionsingeneralandwehave computationallimitationsitisbettertogowithastandardma chinelearningmodelwithglovewhichissignificantlysmaller andeasiertodeploycomparedtopretrainedonesoverallthefind ingsemphasizethedominanceofpretrainedtransformerbased architecturesfortextclassificationtasks reference sabeenahmedianenielsenaakashtripathishamoonsiddiquiravipra machandranandghulamrasooltransformersintimeseriesanalysisa tutorialcircuitssystemsandsignalprocessing danguitalucaghelardonialessandroghiolonetoandsandroridella thekinkfoldcrossvalidationintheeuropeansymposiumonartificial neuralnetworks kclarkelectrapretrainingtextencodersasdiscriminatorsratherthan generatorsarxivpreprintarxiv aconneauunsupervisedcrosslingualrepresentationlearningatscale arxivpreprintarxiv diogocortizexploringtransformersmodelsforemotionrecognitiona comparisionofbertdistilbertrobertaxlnetandelectrainproceed ingsoftherdinternationalconferenceoncontrolroboticsandintelligent systemvirtualeventchinaccrisassociationforcomputingmachinery newyorknyusa httpsdoiorg sonainjamilmdjalilpiranandohjinkwonacomprehensivesurveyof transformersforcomputervisiondrones xiaoqijiaoyichunyinlifengshangxinjiangxiaochenlinlinlifangwang andqunliutinybertdistillingbertfornaturallanguageunderstanding arxivpreprintarxiv thorstenjoachimsetalaprobabilisticanalysisoftherocchioalgorithm withtfidffortextcategorizationinicmlvolciteseer jacobdevlinmingweichangkentonandleekristinatoutanovabert pretrainingofdeepbidirectionaltransformersforlanguageunderstandingin proceedingsofnaaclhltvolminneapolisminnesota kamrankowsarikianajafarimeimandimojtabaheidarysafasanjanamendu laurabarnesanddonaldbrowntextclassificationalgorithmsasurvey information zlanalbertalitebertforselfsupervisedlearningoflanguagerepresen tationsarxivpreprintarxiv yinhanliurobertaarobustlyoptimizedbertpretrainingapproacharxiv preprintarxiv larryrmedskerlakhmijainetalrecurrentneuralnetworksdesign andapplications shervin minaee nal kalchbrenner erik cambria narjes nikzad meysam chenaghluandjianfenggao deeplearningbasedtextclassification acomprehensivereviewacmcomputingsurveyscsur jeffreypenningtonrichardsocherandchristopherdmanningglove globalvectorsforwordrepresentationinproceedingsoftheconferenceon empiricalmethodsinnaturallanguageprocessingemnlp philippprobstannelaureboulesteixandberndbischltunabilityim portanceofhyperparametersofmachinelearningalgorithmsjournalofmachine learningresearch"]}