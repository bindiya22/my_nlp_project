{"words": ["preprint", "iclr", "incontext", "learning", "representation", "corefranciscoparkandrewleeekdeepsinghlubanayongyiyang", "mayaokawakentonishimartinwattenberghidenoritanaka", "cbsnttprograminphysicsofintelligenceharvarduniversity", "departmentofphysicsharvarduniversity", "physicsinformaticslabnttresearchinc", "seasharvarduniversity", "cseuniversityofmichiganannarbor", "abstract", "recentworkhasdemonstratedthatsemanticsspecifiedbypretrainingdatainflu", "encehowrepresentationsofdifferentconceptsareorganizedinalargelanguage", "modelllmhowevergiventheopenendednatureofllmsegtheirability", "toincontextlearn", "wecanaskwhethermodelsalterthesepretrainingsemantics", "toadoptalternativecontextspecifiedones", "specificallyifweprovideincontext", "exemplarswhereinaconceptplaysadifferentrolethanwhatthepretrainingdata", "suggests", "model", "reorganize", "representation", "accordance", "novel", "semantics", "answer", "question", "take", "inspiration", "theory", "conceptual", "role", "semantics", "define", "toy", "graph", "tracing", "task", "wherein", "nodesofthegrapharereferencedviaconceptsseenduringtrainingegapple", "birdetcandtheconnectivityofthegraphisdefinedviasomepredefinedstruc", "tureegasquaregrid", "givenexemplarsthatindicatetracesofrandomwalkson", "thegraph", "weanalyzeintermediaterepresentationsofthemodelandfindthatas", "theamountofcontextisscaledthereisasuddenreorganizationfrompretrained", "semantic", "representation", "incontext", "representation", "aligned", "graph", "structure", "furtherwefindthatwhenreferenceconceptshavecorrelationsintheir", "semanticseg", "monday", "tuesday", "etc", "thecontextspecified", "graphstructure", "isstillpresentintherepresentationsbutisunabletodominatethepretrainedstruc", "ture", "explain", "result", "analogize", "task", "energy", "minimization", "apredefinedgraphtopologyprovidingevidencetowardsanimplicitoptimization", "processtoinfercontextspecifiedsemanticsoverallourfindingsindicatescaling", "contextsize", "flexibly", "reorganize", "model", "representation", "possibly", "unlocking", "novelcapabilities", "introduction", "agrowinglineofworkdemonstratesthatlargelanguagemodelsllmsorganizerepresentationsof", "specificconceptsinamannerthatreflectstheirstructureinpretrainingdataparketalcden", "gelsetalabdouetalpatelpavlickanthropicaigurneetegmark", "vafa", "et", "al", "li", "et", "al", "pennington", "et", "al", "targeted", "experiment", "syntheticdomainshavefurthercorroboratedthesefindingsshowinghowmodelrepresentationsare", "organizedaccordingtothedatageneratingprocesslietaljenneretaltrayloretal", "liuetalbshaietalparketalbgopalanietal", "howeverwhen", "amodelisdeployedinopenendedenvironmentswecanexpectittoencounternovelsemanticsfor", "aconceptthatitdidnotseeduringpretraining", "forexample", "assumethatwedescribetoanllm", "new", "product", "called", "strawberry", "announced", "ideally", "based", "context", "modelwouldaltertherepresentationforstrawberryandreflectthatwearenotreferringtothe", "pretrainingsemanticsegthefruitstrawberry", "doesthisidealsolutiontranspireinllms", "motivatedbytheaboveweevaluatewhetherwhenprovidedanincontextspecificationofaconcept", "anllmaltersitsrepresentationstoreflectthecontextspecifiedsemantics", "specificallywepropose", "equal", "contribution", "contact", "corefranciscoparkandrewleegharvardedu", "yongyiumicheduekdeeplubana", "hidenori", "tanakafasharvardedu", "ced", "lcsc", "vvixrapreprint", "word", "grid", "c", "emergent", "grid", "representation", "context", "apple", "bird", "car", "egg", "house", "milk", "plane", "opera", "box", "sand", "sun", "mango", "rock", "math", "code", "phone", "b", "data", "generation", "random", "walk", "grid", "apple", "bird", "milk", "sand", "sun", "plane", "opera", "context", "length", "context", "length", "context", "length", "figure", "alteration", "representation", "accordance", "contextspecified", "semantics", "grid", "structure", "randomly", "arrange", "set", "concept", "grid", "reflect", "correla", "tionalsemanticsbetweenthetokens", "bwethengeneratesequencesoftokensfollowingarandom", "walk", "grid", "inputting", "context", "llamab", "model", "c", "model", "mean", "token", "representationsprojectedontothetoptwoprincipalcomponents", "asthenumberofincontextex", "emplarsincreasesthereisaformationofrepresentationsmirroringthegridstructureunderlyingthe", "datageneratingprocess", "representationsarefromtheresidualstreamactivationfollowinglayer", "incontext", "learning", "task", "involves", "simple", "graph", "tracing", "problem", "wherein", "model", "shownedgescorrespondingtoarandomtraversalofagraphseefigthenodesofthisgraphare", "intentionallyreferencedviaconceptsthemodelisextremelylikelytohaveseenduringtrainingeg", "applebirdetcwhileitsconnectivitystructureisdefinedusingapredefinedgeometrythatis", "ambivalenttocorrelationsbetweenconceptssemanticsegasquaregrid", "basedontheprovided", "context", "themodelisexpectedtooutputavalidnextnodeprediction", "ie", "anodeconnectedtothe", "lastpresentedone", "asweshowincreasingtheamountofcontextleadstoasuddenreorganization", "ofrepresentationsinaccordancewiththegraphsconnectivity", "thissuggestsllmscanmanipulate", "representation", "order", "reflect", "concept", "semantics", "specified", "entirely", "incontext", "inline", "theory", "inferential", "semantics", "cognitive", "science", "harman", "block", "characterizetheseresultsbyanalyzingtheproblemofdirichletenergyminimizationshowingthat", "modelsindeedidentifythestructureoftheunderlyinggraphtoachieveanontrivialaccuracyonour", "task", "thissuggestsanimplicitoptimizationprocessashypothesizedbytheoreticalworkoniclin", "toysetupsegincontextlinearregressioncantranspireinmorenaturalisticsettingsvonoswald", "etalabakyureketal", "overallourcontributionscanbesummarizedasfollows", "graphnavigationasasimplisticmodelofnovelsemantics", "weintroduceatoygraphnav", "igation", "task", "requires", "model", "interpret", "semantically", "meaningful", "concept", "referent", "nodesinastructurallyconstrainedgraph", "inputtingtracesofrandomwalksonthisgraphintoan", "llmweanalyzewhetherthemodelaltersitsintermediaterepresentationsforreferentconcepts", "topredictvalidnextnodesasdefinedbytheunderlyinggraphconnectivityhenceinferringinline", "withtheoriesofsemanticsfromcognitivesciencenovelsemanticsofaconceptharman", "emergent", "incontext", "reorganization", "concept", "representation", "result", "show", "contextsizeisscaledieasweaddmoreexemplarsincontextthereisasuddenreorganization", "ofconceptrepresentationsthatreflectsthegraphsconnectivitystructure", "intriguingly", "thesere", "sults", "similar", "one", "achieved", "similar", "setup", "human", "subject", "garvert", "et", "al", "whittington", "et", "al", "show", "contextspecified", "graph", "structure", "emerges", "even", "use", "concept", "correlation", "semantics", "eg", "mon", "tues", "etc", "terestinglyisunabletodominatethepretrainedstructure", "morebroadlywenotethatthissudden", "reorganization", "reminiscent", "emergent", "capability", "llm", "relevant", "ax", "eg", "computeormodelsizearescaledweietalsrivastavaetallubanaetal", "energy", "minimization", "model", "semantics", "inference", "provide", "quantitative", "ac", "count", "result", "compute", "dirichlet", "energy", "model", "representation", "respect", "groundtruth", "graph", "structure", "find", "energy", "decrease", "function", "context", "size", "offer", "precise", "hypothesis", "mechanism", "employed", "llm", "reorganize", "repre", "sentationsaccordingtothecontextspecifiedsemanticsofaconcept", "theseresultsalsoserveas", "preprint", "htgnel", "txetnoc", "htgnel", "txetnoc", "word", "ring", "c", "emergent", "ring", "representation", "context", "banana", "apple", "layer", "layer", "layer", "layer", "tomato", "lettuce", "onion", "grape", "orange", "fig", "pear", "carrot", "b", "data", "generation", "randomly", "pick", "pair", "neighbor", "apple", "banana", "orange", "onion", "fig", "carrot", "grape", "lettuce", "figure", "alteration", "representation", "accordance", "contextspecified", "semantics", "ring", "structure", "awerandomlyplaceconceptsonaringstructureunrelatedtotheirsemantics", "bwe", "thengeneratesequencesoftokensbyrandomlysamplingneighboringpairsfromtheringwhichis", "usedastheinputcontexttoallamabmodel", "cthemodelsmeanrepresentationoftokens", "projectedontothetoptwoprincipalcomponents", "asthenumberofincontextexemplarsincreases", "formation", "representation", "mirroring", "ring", "structure", "underlying", "datagenerating", "process", "therepresentationsarefromtheresidualstreamactivations", "evidence", "towards", "theory", "incontext", "learning", "implicit", "optimization", "naturalistic", "settingvonoswaldetalabakyureketal", "experimental", "setup", "incontext", "graph", "tracing", "wefirstdefineoursetupforassessingtheimpactofcontextspecificationonhowamodelorganizes", "representation", "main", "paper", "primarily", "focus", "llamab", "henceforth", "llama", "dubeyetalaccessedviandifnnsightfiottokaufmanetal", "wepresentresults", "modelsllamab", "llamabinstruct", "dubey", "et", "al", "gemmab", "gemmab", "gemmateaminappc", "task", "proposed", "task", "call", "incontext", "graph", "tracing", "involves", "random", "walk", "predefinedgraphgspecificallyinspiredbypriorworkanalyzingstructuredrepresentationslearned", "bysequencemodelsweexperimentwiththreegraphicalstructuresasquaregridfigaaring", "figaandahexagonalgridfig", "resultsonhexagonalgridaredeferredtoappendixdue", "tospaceconstraints", "toconstructthesquaregrid", "werandomlyarrangethesetoftokensinagrid", "andaddedgesbetweenhorizontalandverticalneighbors", "wethenperformarandomwalkonthe", "graph", "emitting", "visited", "token", "sequence", "fig", "b", "ring", "add", "edge", "neighboringnodesandsimplysamplerandompairsofneighboringtokensonthegraphfigb", "node", "graph", "denoted", "referenced", "via", "concept", "model", "n", "extremely", "likely", "seen", "pretraining", "choice", "concept", "plausible", "selectrandomtokensthatunlessmentionedotherwisehavenoobvioussemanticcorrelationswith", "one", "another", "eg", "apple", "sand", "math", "etc", "however", "concept", "precise", "meaning", "associated", "training", "data", "necessitating", "extent", "model", "relies", "providedcontexttherepresentationsaremorphedaccordingtotheincontextgraph", "wehighlight", "thatavisualanalogofourtaskwhereinoneusesimagesinsteadoftexttokenstorepresentaconcept", "hasbeenusedtoelicitverysimilarresultswithhumansubjectsastheoneswereportinthispaper", "using", "llm", "garvert", "et", "al", "whittington", "et", "al", "mark", "et", "al", "brady", "et", "al", "wealsonotethatourproposedtaskissimilartoonesstudiedinliteratureonincontextrl", "whereinoneprovidesexplorationtrajectoriesincontexttoamodelandexpectsittounderstandthe", "environmentanditsdynamicsakaaworldmodelleeetalblaskinetal", "preprint", "result", "visualizinginternalactivationusingprincipalcomponents", "sinceweareinterestedinuncoveringcontextspecificrepresentationsweinputsequencesfromour", "datageneratingprocesstothemodelandfirstcomputethemeanactivationsforeachuniquetoken", "namely", "assume", "given", "context", "c", "c", "c", "c", "originates", "n", "anunderlyinggraphg", "ateachtimestep", "welookatawindowofn", "precedingtokensor", "w", "alltokensifthecontextlengthissmallerthann", "andcollectallactivationscorrespondingtoeach", "w", "token", "atagivenlayer", "wethencomputethemeanactivationspertokendenotedash", "rd", "wefurtherdenotethestackofmeantokenrepresentationsasht", "rnd", "finallywerun", "pcaonht", "andusethefirsttwoprincipalcomponentstovisualizemodelactivationsunless", "stated", "otherwise", "note", "pca", "visualization", "known", "suffer", "pitfall", "representationanalysismethodweprovideathoroughquantitativeanalysisinsectodemonstrate", "thatthemodelreorganizesconceptrepresentationsaccordingtotheincontextgraphstructureand", "proveinsecthatthestructureofthegraphisreflectedinthepcavisualizationsbecauseofthis", "reorganizationofrepresentations", "wealsoprovidefurtherevidenceonthefaithfulnessofpcaby", "conducting", "preliminary", "causal", "analysis", "principal", "component", "finding", "intervening", "conceptrepresentationsprojectionsalongthesecomponentsaffectsthemodelsabilitytoaccurately", "predictvalidnextnodegenerationsappc", "result", "fig", "demonstrate", "resulting", "visualization", "square", "grid", "ring", "graph", "spectively", "example", "provided", "appendix", "see", "fig", "strikingly", "enough", "exemplarswefindrepresentationsareinfactorganizedinaccordancewiththegraphstructureun", "derlying", "context", "interestingly", "result", "skewed", "earlier", "layer", "towards", "semantic", "priorsthemodelmayhaveinternalizedduringtraininghoweverthesepriorsareoverriddenaswe", "godeeperinthemodel", "forexampleintheringgraphseefigconceptsappleandorange", "areclosertoeachotherinlayerofthemodelbutbecomeessentiallyantipodalaroundlayer", "asdictatedbythegraphtheantipodalnatureisalsomoreprominentascontextlengthisincreased", "also", "observe", "despite", "developing", "squaregrid", "structure", "sufficient", "context", "length", "given", "see", "fig", "structure", "partially", "irregular", "eg", "wider", "central", "region", "narrowlyarrangedintheperiphery", "wefindthistobeanartifactoffrequencywithwhichaconcept", "seen", "context", "specifically", "due", "lack", "periodic", "boundary", "condition", "concept", "presentintheinnerregionofthegridarevisitedmorefrequentlyduringarandomwalkonthe", "graphwhiletheperipheryofthegraphhasalowervisitationfrequency", "therepresentationsreflect", "thisthusorganizinginaccordancewithbothstructureandfrequencyofconceptsinthecontext", "overalltheresultsaboveindicatethataswescalecontextsizemodelscanreorganizesemantically", "unrelatedconceptstoformtaskspecificrepresentationswhichwecallincontextrepresentations", "intriguingly", "result", "broadly", "inline", "theory", "inferential", "semantics", "cognitive", "scienceaswellharmanblock", "semanticpriorvs", "incontexttaskrepresentations", "buildingonresultsfromtheprevioussectionwenowinvestigatetheimpactofusingsemantically", "correlatedconcepts", "specificallywebuildontheresultsfromengelsetalwhoshowthat", "representationsfordaysoftheweek", "ie", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday", "organize", "circular", "geometry", "randomly", "permute", "orderingoftheseconceptsarrangethemonanoderinggraphsimilartotheprevioussectionsee", "figaandevaluatewhethertheincontextrepresentationscanoverridethestrongpretrainingprior", "internalizedbythemodel", "result", "figbcdemonstratetheresultingvisualizationswefindthatwhenthereisaconflict", "semantic", "prior", "incontext", "task", "observe", "original", "semantic", "ring", "first", "twoprincipalcomponents", "howeverthecomponentsrightafterinfactencodethecontextspecific", "structure", "visualizingthethirdandfourthprincipalcomponentsshowsthenewlydefinedringstruc", "ture", "thisindicatesthatthecontextspecifiedstructureispresentintherepresentationsbutdoesnot", "dominatethem", "infig", "wereportthemodelsaccuracyontheincontexttask", "findingthatthe", "modeloverridesthesemanticpriortoperformwellonourtaskwhenenoughcontextisgiven", "preprint", "tnenopmoc", "lapicnirp", "principal", "component", "tnenopmoc", "lapicnirp", "b", "c", "tuesday", "monday", "wednesday", "sunday", "thursday", "saturday", "principal", "component", "friday", "semantic", "link", "incontext", "link", "figure", "incontext", "representation", "form", "higher", "principal", "component", "presence", "semantic", "prior", "purple", "semantic", "link", "underlying", "day", "week", "dashed", "blue", "define", "nonsemantic", "graph", "structure", "linking", "nonneighboring", "day", "generate", "token", "thisgraph", "bpurpletheringgeometryformedbysemanticlinksestablishedduringpretraining", "remainsintactinthefirsttwoprincipalcomponents", "cdashedbluethenonsemanticstructure", "provided", "incontext", "seen", "third", "fourth", "principal", "component", "note", "star", "structureinthefirsttwocomponentsbwhichmatchthegroundtruthgraphicalstructureofourdata", "generatingprocessabecomesaringinthenexttwoprincipalcomponentsctherepresentations", "arefromtheresidualstreamactivationfollowinglayer", "effect", "context", "scaling", "emergent", "reorganization", "representation", "result", "previous", "section", "demonstrate", "model", "reorganize", "concept", "representation", "accordance", "contextspecified", "semantics", "next", "aim", "study", "behavior", "arises", "ascontextisscaledisthereacontinuous", "monotonicimprovementtowardsthecontextspecified", "structureascontextisaddedifsoisthereatrivialsolutionegregurgitationbasedoncontextthat", "helpsexplaintheseresults", "toanalyzethesequestions", "wemustfirstdefineametricthathelpsus", "gaugehowalignedtherepresentationsarewiththestructureofthegraphthatunderliesthecontext", "dirichlet", "energy", "measure", "dirichlet", "energy", "graph", "g", "structure", "defining", "energy", "function", "model", "representation", "specifically", "undirected", "graph", "g", "n", "nodesletarnnbeitsadjacencymatrixandxrnbeasignalvectorthatassignsavaluex", "toeachnodei", "thenthedirichletenergyofthegraphwithrespecttoxisdefinedas", "cid", "e", "x", "x", "x", "g", "ij", "j", "ij", "multidimensional", "signal", "dirichlet", "energy", "defined", "summation", "energy", "dimension", "specifically", "let", "x", "rnd", "matrix", "assigns", "node", "ddimensionalvectorx", "thenthedirichletenergyofx", "isdefinedby", "cidcid", "cid", "e", "x", "x", "x", "x", "x", "g", "ij", "ik", "jk", "ij", "j", "k", "ij", "ij", "overall", "empirically", "quantify", "formation", "geometric", "representation", "measure", "dirichlet", "energy", "respect", "graph", "underlying", "data", "generating", "process", "dgps", "ourmeantokenactivationsh", "cid", "e", "ht", "hh", "g", "ij", "j", "ij", "ht", "rnd", "stack", "mean", "token", "representation", "h", "layer", "ij", "token", "dgp", "certain", "context", "length", "note", "ht", "function", "context", "preprint", "context", "length", "ygrene", "telhcirid", "dezilamron", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "hex", "ycarucca", "ycarucca", "ycarucca", "layer", "layer", "accuracy", "figure", "model", "continuously", "develops", "task", "representation", "learns", "traverse", "novel", "graphsincontext", "weplottheaccuracyofgraphtraversalandthedirichletenergyofthegraph", "computed", "model", "internal", "representation", "function", "context", "length", "note", "thedirichletenergyneverreachesaperfectzerorulingoutthattherepresentationsarelearninga", "degeneratestructureaswasalsoseeninthepcavisualizationsinsec", "aaxgridgraphwith", "node", "bacircularringwithnodes", "cahoneycombhexagonallatticewithnodes", "length", "well", "omit", "notation", "brevity", "intuitively", "measure", "indicates", "whetherneighboringtokensnodesinthegroundtruthgraphhaveasmalldistancebetweentheir", "representation", "thus", "model", "correctly", "infers", "correct", "underlying", "structure", "expect", "seeadecreaseindirichletenergy", "wedonotethatinpracticedirichletenergyminimizationhasa", "trivialsolutionwhereallnodesareassignedthesamerepresentationwhilewecanbeconfidentthis", "trivialsolutiondoesnotexistinourresultsforelsewewouldnotseedistinctnoderepresentationsin", "pcavisualizationsnorhighaccuracyforsolvingourtaskswestillprovideanalternativeanalysisin", "appcwheretherepresentationsarestandardizedmeancenteredandnormalizedbyvarianceto", "renderthistrivialsolutioninfeasiblewefindresultsarequalitativelysimilarwithsuchstandardized", "representationsbutmorenoisysincestandardizationcaninducesensitivitytonoise", "result", "emergentorganizationandtaskaccuracyimprovements", "weplotllamasaccuracyattheincontextgraphtracingtaskalongsidethedirichletenergymea", "sure", "different", "layer", "function", "context", "specifically", "compute", "rule", "following", "accuracy", "add", "model", "output", "probability", "graph", "node", "valid", "neighbor", "forinstanceifthegraphstructureisapplecarbirdwaterandthecurrentstate", "iscarweaddupthepredictedprobabilitiesforappleandbird", "thismetricsimplymeasures", "howwellthemodelabidesbythegraphstructure", "result", "reported", "fig", "see", "critical", "amount", "context", "seen", "model", "accuracystartstorapidlyimprove", "wefindthispointinfactcloselymatcheswhendirichletenergy", "reachesitsminimumvalue", "energyisminimizedshortlybeforetherapidincreaseinincontexttask", "accuracy", "suggesting", "structure", "data", "correctly", "learned", "model", "make", "valid", "prediction", "lead", "u", "claim", "amount", "context", "scaled", "emergentreorganizationofrepresentationsthatallowsthemodeltoperformwellonourincontext", "graphtracingtask", "wenotetheseresultsalsoprovideamorequantitativecounterpartofourpca", "visualizationresultsbefore", "trivial", "solution", "play", "simple", "baseline", "would", "exhibit", "increase", "per", "formance", "increasing", "context", "involves", "model", "merely", "regurgitating", "node", "neighbor", "copyingthemfromitscontext", "wecallthisthememorizationsolution", "whilesuchasolutionwould", "notexplainthereorganizationofrepresentationsweuseitasabaselinetoshowthemodelislikely", "engaginginamoreintriguingmechanism", "sinceouraccuracymetricmeasuresrulefollowingthis", "memorizationsolutionwillachievevalueifthenodehasbeenobservedinthecontextandother", "wise", "followingourdatasamplingprocessthenifwesimplychooseaninitialnodeatrandomwith", "replacementwecanexpresstheprobabilityofanodeexistinginacontextoflengthlas", "cid", "ncidl", "p", "x", "seen", "n", "preprint", "fig", "node", "b", "node", "context", "size", "context", "size", "ycarucca", "ycarucca", "llamab", "shot", "memorization", "shot", "memorization", "piecewise", "linear", "fit", "transition", "point", "figure", "amemorizationsolutioncannotexplainllamasiclgraphtracingperformance", "weplottherulefollowingaccuracyfromllamaboutputsandaccuraciesfromasimpleshot", "andshotmemorizationhypothesis", "aaringgraphwithnodes", "basquaregridgraphwith", "node", "inbothcaseswefindthatthememorizationsolutioncannotexplaintheaccuracyascent", "curve", "insteadwefindaslowphaseandafastphasewhichwefitwithapiecewiselinearfit", "x", "context", "n", "number", "node", "available", "note", "current", "node", "doesnotmatterasthesamplingprobabilityisuniformwithreplacement", "wealsoevaluateanother", "similarbaselinethatassumesthesametokenmuchbeencounteredtwiceforthemodeltorecognize", "incontext", "exemplar", "define", "closedform", "expression", "solution", "probabilitythatanodehasappearedtwiceasfollows", "cid", "cidcid", "ncidl", "p", "xp", "xl", "seen", "seen", "n", "n", "toevaluatewhetherthememorizationsolutionsaboveexplainourresultsweplottheirperformance", "alongsidetheobservedperformanceofllama", "figshowstheresultaonaringgraphwith", "nodesandbonagridgraphwithnodes", "wefindinbothcasesthatneithertheshotnorthe", "shotmemorizationcurvecanexplainthebehaviorofllamainsteadweobservethattheaccuracy", "two", "phase", "first", "phase", "accuracy", "improves", "slowly", "second", "phase", "loglinear", "slope", "suddenly", "change", "steeper", "ascent", "find", "piecewise", "linear", "fit", "extractthistransitionpointfairlywellwhichwillbeofinterestinthenextsection", "explaining", "emergent", "reorganization", "representation", "energy", "minimization", "hypothesis", "buildingontheresultsfromprevioussectionwenowputforwardahypothesisforwhyweareable", "toidentifysuchstructuredrepresentationsfromamodel", "wehypothesizethemodelinternallyruns", "anenergyminimizationprocessinsearchofthecorrectstructuralrepresentationofthedatayang", "etalsimilartoclaimsofimplicitoptimizationinincontextlearningproposedbypriorwork", "intoysettingsvonoswaldetalab", "moreformallyweclaimthefollowinghypothesis", "hypothesis", "letnbethenumberoftokensdbethedimensionalityoftherepresentationsand", "htt", "rnd", "stack", "representation", "token", "learned", "model", "layer", "andcontextlengthtthene", "cid", "httcid", "decayswithcontextlengtht", "g", "minimizersofdirichletenergyandspectralembeddings", "wecallthekthenergyminimizerofe", "theoptimalsolutionthatminimizese", "andisorthogonal", "g", "g", "tothefirstkenergyminimizers", "formallytheenergyminimizerscid", "zkcidn", "aredefinedasthe", "k", "preprint", "figurespectralembeddingofaringgraph", "figurespectralembeddingofagridgraph", "solutiontothefollowingproblem", "zk", "arg", "min", "e", "z", "g", "zsn", "st", "z", "zjj", "k", "wheresnistheunitsphereinndimensionaleuclideanspace", "theenergyminimizersareknown", "tohavethefollowingpropertiesspielman", "z", "cforsomeconstantc", "whichisadegeneratedsolutionthatassignsthesame", "valuetoeverynodeand", "cid", "cid", "ifweuse", "zz", "asthecoordinateofnodeiitwillbeagoodplanarembedding", "callthemdimensionalspectralembeddings", "spectralembeddingsareoftenusedtoadrawgraphonaplaneandinmanycasescanpreservethe", "structure", "graph", "tutte", "fig", "show", "spectral", "embedding", "result", "aringgraphandagridgraphrespectively", "noticehowsuchspectralembeddingsaresimilartothe", "representationsfromourmodelsinfigand", "asweshowintheorembthisisinfactexpected", "energy", "minimization", "hypothesis", "true", "representation", "h", "model", "minimize", "thedirichletenergyandarenondegeneratedthenthefirsttwoprincipalcomponentsofpcawill", "exactly", "produce", "spectral", "embeddings", "zz", "present", "informal", "version", "theoremanddeferthefullversionandprooftotheappendix", "theorem", "informal", "version", "theorem", "b", "let", "g", "graph", "h", "rnd", "n", "dbeamatrixthatminimizesdirichletenergyongwithnondegeneratedsingularvaluesthen", "thefirsttwoprincipalcomponentsofh", "willbezandz", "see", "app", "b", "formal", "version", "proof", "theorem", "see", "also", "tab", "empirical", "validationofthetheoremwhereinweshowtheprincipalcomponentsalignverywellwithspectral", "embeddingsofthegraph", "energyminimizationandgraphconnectivity", "giventherelationshipbetweenspectralembeddingsieenergyminimizersandtheprincipalcom", "ponentsobservedinourresultsfigs", "weclaimthatthemodelsinferenceoftheunderlying", "structureisakintoanimplicitenergyminimizationtofurtheranalyzetheimplicationofthisclaim", "weshowthatthemomentatwhichwecanvisualizeagraphusingpcaisthemomentatwhichthe", "model", "found", "large", "connected", "component", "ie", "graph", "structure", "specifically", "consider", "unconnected", "graph", "g", "ie", "ghas", "multiple", "connected", "component", "multiple", "de", "generatesolutionstotheenergyminimizationproblemwhichwillbefoundbypcaspecifically", "suppose", "ghas", "q", "connected", "component", "u", "denoting", "set", "node", "ith", "component", "preprint", "node", "b", "figure", "incontext", "emergence", "analyze", "incontext", "accuracy", "curve", "function", "contextsize", "inputted", "model", "graph", "used", "experiment", "mm", "grid", "varying", "value", "rule", "following", "accuracy", "graph", "tracing", "task", "accuracy", "show", "two", "phase", "ascent", "fit", "piecewise", "linear", "function", "observed", "ascent", "extract", "transi", "tionpointwhichmovesrightwardswithincreasinggraphsize", "binterestinglythetransitionpoint", "scalesasapowerlawinmiethenumberofnodesinthegraph", "thenwecanconstructthefirstqenergyminimizersasfollowsiqletthejthvalueofzibe", "j", "cid", "u", "zi", "k", "j", "othek", "rw", "ise", "cidq", "cid", "zi", "cidk", "cidjuk", "zj", "foriq", "k", "juk", "j", "easy", "check", "zi", "constructed", "q", "energy", "thus", "global", "minimizerofe", "moreoverallzisareorthogonaltoeachotherandhencesatisfyourdefinition", "g", "first", "q", "energy", "minimizers", "important", "notice", "zis", "q", "contain", "information", "structure", "graph", "identifying", "connected", "component", "theorem", "b", "tell", "u", "principal", "component", "nondegenerated", "rank", "solutionhthatminimizestheenergywillbezzsthusifthegraphisunconnectedthen", "theenergyminimizingrepresentationswillbedominatedbyinformationlessprincipalcomponents", "expect", "meaningful", "visualization", "acute", "reader", "may", "recall", "first", "minimizer", "z", "trivial", "solution", "energy", "minimization", "assigns", "value", "everynode", "convenientlytheaboveargumentalsoimpliesthatthisisnotaconcern", "pcawillrule", "outthisdegeneratesolutionasdemonstratedintheoremb", "incontextemergence", "ahypothesis", "ourresultsinfigshowedanintriguingbreakpointthat", "isreminiscentofasecondorderphasetransitionieanundefinedsecondderivative", "asshownin", "figweinfactfindthisbehaviorisextremelyrobustacrossgraphsofdifferentsizesandshowsa", "powerlawscalingtrendwithincreasinggraphsizeseeappcforseveralmoreresultsinthisvein", "includingdifferentgraphtopologies", "giventherelationshipofferedbetweenenergyminimization", "anddiscoveryofaconnectedcomponentgraphstructureinouranalysisaboveapossibleframe", "worktoexplaintheseresultsmaybetheproblemofbondpercolationonagraphnewman", "hooyberghs", "et", "al", "bondpercolation", "one", "start", "unconnected", "graph", "slowly", "fill", "edge", "connect", "node", "edge", "filled", "secondorder", "transition", "large", "connected", "component", "emerges", "graph", "nature", "transition", "observed", "experimentsfigandthetheoreticalconnectionbetweenenergyminimizationandexistenceofa", "connectedcomponentprovidesomeevidencetowardstheplausibilityofthishypothesis", "however", "webelievetheanalogyisstilllooseforourgraphsizesarerelativelysmalllikelycausingsignifi", "cantfinitesizeeffectsandtheexperimentsneedtocorroborateanyscalingtheoryofthetransition", "pointfrompercolationliteraturewouldrequirerunninggraphswithatleastordersofmagnitude", "differenceintheirsizes", "howevertheconsistencyofthehypothesiswithourempiricalresultsand", "analysisimpliesthatinvestigatingitfurthermaybefruitful", "preprint", "related", "work", "model", "representation", "researcher", "recently", "discovered", "numerous", "structured", "representa", "tions", "neural", "network", "mikolov", "et", "al", "suggests", "concept", "linearly", "represented", "activation", "park", "et", "al", "recently", "suggests", "may", "case", "contemporary", "language", "model", "numerous", "researcher", "found", "concrete", "example", "linear", "representation", "humanlevel", "concept", "including", "truthfulness", "burn", "et", "al", "li", "et", "al", "b", "mark", "tegmark", "refusal", "arditi", "et", "al", "toxicity", "lee", "et", "al", "sycophancy", "rimsky", "etalandevenworldmodelslietalnandaetal", "parketalcfinds", "thathierarchicalconceptsarerepresentedwithatreelikestructureconsistingoforthogonalvectors", "relevant", "line", "work", "includes", "todd", "et", "al", "hendel", "et", "al", "paper", "findthatonecancomputeavectorfromincontextexemplarsthatencodethetasksuchthatadding", "suchavectorduringtesttimeforanewinputcancorrectlysolvethetask", "languagemodelsdonot", "always", "form", "linear", "representation", "however", "engels", "et", "al", "find", "circular", "feature", "representa", "tionsforperiodicconceptssuchasdaysoftheweekormonthsoftheyearusingacombinationof", "sparseautoencodersandpcacsordasetalfindsthatrecurrentneuralnetworkstrainedon", "tokenrepetitioncaneitherlearnanonionlikerepresentationoralinearrepresentationdepending", "onthemodelswidth", "unlikesuchpriorworkwefindthattaskspecificrepresentationswithade", "siredstructuralpatterncanbeinducedincontext", "toourknowledgeourworkoffersthefirstsuch", "investigationofincontextrepresentationlearning", "scaling", "incontext", "learning", "numerous", "work", "demonstrated", "incontext", "accuracy", "im", "proveswithmoreexemplarsbrownetalluetalbigelowetal", "withlonger", "contextlengthsbecomingavailableresearchershavebeguntostudytheeffectofmanyshotprompt", "ingasopposedtofewshotagarwaletalaniletallietalc", "forinstance", "agarwaletalreportsimprovedperformanceoniclusinghundredstothousandsofexem", "plars", "wide", "range", "task", "similarly", "anil", "et", "al", "demonstrate", "ability", "jailbreak", "llm", "scaling", "number", "exemplar", "unlike", "work", "evaluates", "model", "behavior", "study", "effect", "scaling", "context", "underlying", "representation", "provide", "framework", "predictingwhendiscontinuouschangesinbehaviorcanbeexpectedviamerecontextscaling", "syntheticdataforinterpretability", "recentworkshavedemonstratedthevalueofinterpretable", "syntheticdatageneratingprocessesforunderstandingtransformersbehaviorincludingincontext", "learningparketalarameshetalgargetallanguageacquisitionlubana", "etalqinetalallenzhulibfinetuningjainetallubanaetal", "junejaetalreasoningabilitiesprystawskietalkhonaetalwenetal", "liuetalaandknowledgerepresentationsnishietalallenzhulia", "whilepriorworktypicallypretrainstransformersonsyntheticdataweleveragesyntheticdatato", "studyrepresentationformationduringincontextlearninginpretrainedlargelanguagemodels", "discussion", "work", "show", "llm", "flexibly", "manipulate", "representation", "semanatics", "internalized", "based", "pretraining", "data", "semantics", "defined", "entirely", "incontext", "arrive", "result", "propose", "simple", "rich", "task", "graph", "tracing", "wherein", "trace", "random", "walk", "graph", "shown", "model", "incontext", "graph", "instantiated", "using", "predefined", "structure", "eg", "lattice", "concept", "semantically", "interesting", "eg", "define", "node", "meaning", "less", "overall", "context", "problem", "interestingly", "find", "ability", "flexibly", "manipulate", "representationsisinfactemergentwithrespecttocontextsizeweproposeamodelbasedonen", "ergyminimizationtohypothesizeamechanismfortheunderlyingdynamicsofthisbehavior", "resultssuggestcontextscalingcanunlocknewcapabilities", "morebroadly", "thisaxismayhave", "yet", "underappreciated", "improving", "model", "fact", "note", "knowledge", "work", "first", "investigate", "formation", "representation", "entirely", "incontext", "study", "also", "naturally", "motivates", "future", "work", "towards", "formation", "world", "representation", "li", "et", "al", "world", "model", "ha", "schmidhuber", "incontext", "significant", "implication", "toward", "building", "general", "openended", "system", "well", "forecasting", "safety", "concern", "alsohighlighttherelationofourexperimentalsetuptosimilartasksstudiedinneurosciencelitera", "turegarvertetalmarketalwhereinhumansareshownrandomwalksofa", "graphofvisualconceptsfmriimagesofthesesubjectsdemonstratetheformationofastructured", "representationofthegraphinthehippocampalentorhinalcortexsimilartoourresultswithllms", "preprint", "limitation", "wedoemphasizethatourworkhasafewlimitations", "namelypcaormorebroadly", "low", "dimensional", "visualization", "high", "dimensional", "data", "difficult", "interpret", "sometimes", "evenmisleading", "despitesuchdifficultiesweprovidetheoreticalconnectionsbetweenenergymin", "imizationandprincipalcomponentstoprovideacompellingexplanationforwhystructureselicited", "via", "pca", "faithfully", "represent", "incontext", "graph", "structure", "second", "find", "strong", "never", "thelessincompletecausalrelationshipbetweentherepresentationsfoundbypcaandthemodels", "prediction", "view", "exact", "understanding", "representation", "form", "exact", "relationship", "representation", "model", "prediction", "interesting", "future", "direction", "especiallygiventhatsuchunderlyingmechanismseemstodependonthescaleofthecontext", "acknowledgment", "wegreatlythankthenationaldeepinferencefabricndifpilotprogramfiottokaufmanetal", "especiallyemmabortzjadenfiottokaufmanadambelfkidavidbauandtheteamwho", "provideduswithaccesstorepresentationsofllamamodelsmakingthisworkpossiblecfpandht", "acknowledge", "support", "aravinthan", "dt", "samuel", "cecilia", "garraffo", "douglas", "p", "finkbeiner", "cfp", "esl", "kn", "mo", "ht", "supported", "cbsntt", "program", "physic", "intelligence", "mw", "acknowledges", "support", "superalignment", "fast", "grant", "openai", "effective", "venture", "foundationeffektivspendenschweizandtheopenphilanthropyprojectpartofthecomputations", "paper", "run", "fasrc", "cluster", "supported", "fa", "division", "science", "research", "computing", "group", "harvard", "university", "esl", "thanks", "eric", "bigelow", "crucial", "discussion", "helped", "define", "several", "hypothesis", "pursued", "work", "harvard", "cocodev", "lab", "especially", "peng", "qian", "talia", "konkle", "feedback", "earlier", "version", "paper", "cfp", "thanks", "zechen", "zhang", "eric", "todd", "clement", "duma", "shivam", "raval", "useful", "discussion", "author", "thank", "davidbauandhislabforusefulfeedbackonthepapersresults", "author", "contribution", "cfpaleslandhtconceivedtheincontextgraphtraversaltaskinspiredbydiscussionsandex", "perimentationswithmoandkninarelatedworkonpretraining", "cfpandalcoledexperiments", "wherecfpdiscoveredtheincontextlearningoftheringstructuredrepresentationwithinputfrom", "ht", "kicking", "study", "al", "suggested", "extending", "grid", "structure", "cfp", "connecting", "world", "representation", "esl", "proposed", "hexagonal", "configuration", "esl", "hypothesized", "incontext", "transitionswithincreasedcontextandpercolationmechanism", "cfpconductedsemanticoverriding", "percolation", "experiment", "al", "performed", "accuracyenergy", "experiment", "causal", "inter", "ventions", "cfp", "al", "esl", "developing", "transition", "point", "detection", "method", "contributing", "toenergynormalization", "yyformulatedtheenergyminimizationtheoryanditsproofscollaborat", "ingwithesltoconnecttheframeworktocomponentexistenceanddemonstratepcasoptimality", "mwprovidedalternativehypothesesforexperimentalrobustness", "cfpaleslyyandhtco", "developed", "initial", "manuscript", "feedback", "mw", "esl", "leading", "final", "writing", "projectnarrativedevelopment", "cfpalandhtcreatedfigureswhilecfpandaljointlydevel", "oped", "appendix", "al", "conducted", "substantial", "experiment", "verify", "main", "claim", "generalize", "othermodelswithcfpssupport", "htsupervisedtheproject", "preprint", "reference", "mostafa", "abdou", "artur", "kulmizev", "daniel", "hershcovich", "stella", "frank", "ellie", "pavlick", "anders", "sgaard", "language", "model", "encode", "perceptual", "structure", "without", "grounding", "case", "study", "incolor", "arxivpreprintarxiv", "rishabh", "agarwal", "avi", "singh", "lei", "zhang", "bernd", "bohnet", "luis", "rosias", "stephanie", "chan", "biao", "zhang", "ankesh", "anand", "zaheer", "abbas", "azade", "nova", "john", "coreyes", "eric", "chu", "feryal", "hbahani", "aleksandra", "faust", "hugo", "larochelle", "manyshot", "incontext", "learning", "url", "httpsarxivorgabs", "ekin", "akyurek", "dale", "schuurmans", "jacob", "andreas", "tengyu", "denny", "zhou", "learning", "algorithm", "incontext", "learning", "investigation", "linear", "model", "url", "http", "arxivorgabs", "zeyuan", "allenzhu", "yuanzhi", "li", "physic", "language", "model", "part", "knowledge", "storage", "extraction", "arxivpreprintarxiva", "zeyuanallenzhuandyuanzhili", "physicsoflanguagemodels", "part", "learninghierarchicallan", "guagestructures", "arxiveprintsabsmayb", "cem", "anil", "esin", "durmus", "mrinank", "sharma", "joe", "benton", "sandipan", "kundu", "joshua", "batson", "nina", "rimskymegtongjessemudanielfordetalmanyshotjailbreakinganthropicapril", "anthropic", "ai", "scaling", "monosemanticity", "extracting", "interpretable", "feature", "claude", "sonnet", "httpstransformercircuitspub", "scalingmonosemanticityindexhtml", "andy", "arditi", "oscar", "obeso", "aaquib", "syed", "daniel", "paleka", "nina", "panickssery", "wes", "gurnee", "neel", "nanda", "refusal", "language", "model", "mediated", "single", "direction", "arxiv", "preprint", "arxiv", "eric", "j", "bigelow", "ekdeep", "singh", "lubana", "robert", "p", "dick", "hidenori", "tanaka", "tomer", "ullman", "incontextlearningdynamicswithrandombinarysequences", "arxivpreprintarxiv", "nedblock", "semanticsconceptualrole", "routledgeencyclopediaofphilosophy", "timothyfbrady", "taliakonkle", "andgeorgea", "alvarez", "compressioninvisualworking", "memory", "using", "statistical", "regularity", "form", "efficient", "memory", "representation", "journal", "experi", "mentalpsychology", "general", "tom", "brown", "benjamin", "mann", "nick", "ryder", "melanie", "subbiah", "jared", "kaplan", "prafulla", "dhari", "wal", "arvind", "neelakantan", "pranav", "shyam", "girish", "sastry", "amanda", "askell", "sandhini", "agar", "wal", "ariel", "herbertvoss", "gretchen", "krueger", "tom", "henighan", "rewon", "child", "aditya", "ramesh", "daniel", "ziegler", "jeffrey", "wu", "clemens", "winter", "chris", "hesse", "mark", "chen", "eric", "sigler", "mateusz", "litwin", "scott", "gray", "benjamin", "chess", "jack", "clark", "christopher", "berner", "sam", "mccandlish", "alec", "radford", "ilya", "sutskever", "dario", "amodei", "language", "model", "fewshot", "learner", "h", "larochelle", "ranzato", "r", "hadsell", "mf", "balcan", "h", "lin", "ed", "advance", "neu", "ral", "information", "processing", "system", "volume", "pp", "curran", "associate", "inc", "url", "httpsproceedingsneuripsccpaperfilespaper", "filecdbfcbbfbacfapaperpdf", "collinburns", "haotianye", "danklein", "andjacobsteinhardt", "discoveringlatentknowledgeinlan", "guagemodelswithoutsupervision", "arxivpreprintarxiv", "robert", "csordas", "christopher", "potts", "christopher", "manning", "atticus", "geiger", "recurrent", "neural", "networkslearntostoreandgeneratesequencesusingnonlinearrepresentations", "arxivpreprint", "arxiv", "abhimanyudubeyabhinavjauhriabhinavpandeyandetal", "thellamaherdofmodels", "urlhttpsarxivorgabs", "preprint", "joshuaengelsisaacliaoericjmichaudwesgurneeandmaxtegmarknotalllanguagemodel", "featuresarelinear", "urlhttpsarxivorgabs", "kyfan", "onatheoremofweylconcerningeigenvaluesoflineartransformationsi", "proceedingsof", "thenationalacademyofsciences", "jaden", "fiottokaufman", "alexander", "r", "loftus", "eric", "todd", "jannik", "brinkmann", "caden", "juang", "koyena", "palcanrageraaronmuellersamuelmarksarnabsensharmafrancescalucchettimichael", "ripaadambelfkinikhilprakashsumeetmultanicarlabrodleyarjunguhajonathanbell", "byron", "wallace", "david", "bau", "nnsight", "ndif", "democratizing", "access", "foundation", "model", "internals", "urlhttpsarxivorgabs", "shivam", "garg", "dimitris", "tsipras", "percy", "liang", "gregory", "valiant", "transformer", "learn", "incontext", "acasestudyofsimplefunctionclasses", "urlhttpsarxivorgabs", "monamgarvertraymondjdolanandtimothyejbehrens", "amapofabstractrelationalknowl", "edgeinthehumanhippocampalentorhinalcortex", "elifee", "gemmateamgemmaimprovingopenlanguagemodelsatapracticalsizeurlhttps", "arxivorgabs", "pulkitgopalaniekdeepsinghlubanaandweihu", "abruptlearningintransformers", "acasestudy", "onmatrixcompletion", "arxivpreprintarxiv", "wes", "gurnee", "max", "tegmark", "language", "model", "represent", "space", "time", "arxiv", "preprint", "arxiv", "davidhaandjurgenschmidhuber", "worldmodels", "arxivpreprintarxiv", "gilbertharman", "conceptualrolesemantics", "notredamejournalofformallogic", "roee", "hendel", "mor", "geva", "amir", "globerson", "incontext", "learning", "creates", "task", "vector", "houda", "bouamor", "juan", "pino", "kalika", "bali", "ed", "finding", "association", "compu", "tational", "linguistics", "emnlp", "pp", "singapore", "december", "association", "computational", "linguistics", "doi", "vfindingsemnlp", "url", "http", "aclanthologyorgfindingsemnlp", "h", "hooyberghs", "b", "van", "schaeybroeck", "j", "indekeu", "percolation", "bipartite", "scalefree", "net", "work", "physicaastatisticalmechanicsanditsapplicationsaugust", "issn", "samyakjainrobertkirkekdeepsinghlubanarobertpdickhidenoritanakaedwardgrefen", "stettetimrocktaschelanddavidscottkrueger", "mechanisticallyanalyzingtheeffectsoffine", "tuningonprocedurallydefinedtasks", "arxivpreprintarxiv", "erikjennershreyaskapurvasilgeorgievcameronallenscottemmonsandstuartrussellevi", "denceoflearnedlookaheadinachessplayingneuralnetworkarxivpreprintarxiv", "jeeveshjunejarachitbansalkyunghyunchojoaosedocandnaomisaphralinearconnectivity", "revealsgeneralizationstrategies", "arxivpreprintarxiv", "mikail", "khona", "maya", "okawa", "jan", "hula", "rahul", "ramesh", "kento", "nishi", "robert", "dick", "ekdeep", "singh", "lubanaandhidenoritanaka", "towardsanunderstandingofstepwiseinferenceintransformers", "asyntheticgraphnavigationmodel", "arxivpreprintarxiv", "michaellaskinluyuwangjunhyukohemilioparisottostephenspencerrichiesteigerwald", "djstrousestevenhansenangelosfilosethanbrooksetalincontextreinforcementlearning", "withalgorithmdistillation", "arxivpreprintarxiv", "andrewleexiaoyanbaiitamarpresmartinwattenbergjonathankkummerfeldandradami", "halcea", "amechanisticunderstandingofalignmentalgorithms", "acasestudyondpoandtoxicity", "arxivpreprintarxiva", "preprint", "jonathanleeanniexiealdopacchianoyashchandakchelseafinnofirnachumandemma", "brunskillsupervisedpretrainingcanlearnincontextreinforcementlearningadvancesinneural", "informationprocessingsystemsb", "belinda", "z", "li", "maxwell", "nye", "jacob", "andreas", "implicit", "representation", "meaning", "neural", "languagemodels", "arxivpreprintarxiv", "kennethliaspenkhopkinsdavidbaufernandaviegashanspeterpfisterandmartinwatten", "berg", "emergentworldrepresentations", "exploringasequencemodeltrainedonasynthetictask", "intheeleventhinternationalconferenceonlearningrepresentations", "kenneth", "li", "aspen", "k", "hopkins", "david", "bau", "fernanda", "viegas", "hanspeter", "pfister", "martin", "wat", "tenberg", "emergent", "world", "representation", "exploring", "sequence", "model", "trained", "synthetic", "task", "eleventh", "international", "conference", "learning", "representation", "url", "httpsopenreviewnetforumiddegtczvt", "kennethlioampatelfernandaviegashanspeterpfisterandmartinwattenberg", "inferencetime", "intervention", "elicitingtruthfulanswersfromalanguagemodel", "inthirtyseventhconferenceon", "neuralinformationprocessingsystemsburlhttpsopenreviewnetforum", "idalluypny", "mukailishansangongjiangtaofengyihengxujunzhangzhiyongwuandlingpengkong", "incontextlearningwithmanydemonstrationexamplesarxivpreprintarxivc", "bingbin", "liu", "jordan", "ash", "surbhi", "goel", "akshay", "krishnamurthy", "cyril", "zhang", "transformer", "learnshortcutstoautomata", "arxivpreprintarxiva", "zimingliuouailkitouniniklassnolteericmichaudmaxtegmarkandmikewilliams", "wardsunderstandinggrokking", "aneffectivetheoryofrepresentationlearning", "advancesinneu", "ralinformationprocessingsystemsb", "yaolumaxbartoloalastairmooresebastianriedelandpontusstenetorpfantasticallyordered", "prompt", "find", "overcoming", "fewshot", "prompt", "order", "sensitivity", "smaranda", "muresan", "preslav", "nakov", "aline", "villavicencio", "ed", "proceeding", "th", "annual", "meet", "ing", "association", "computational", "linguistics", "volume", "long", "paper", "pp", "dublinirelandmayassociationforcomputationallinguistics", "doi", "v", "acllong", "urlhttpsaclanthologyorgacllong", "ekdeepsinghlubanaericjbigelowrobertpdickdavidkruegerandhidenoritanaka", "mech", "anisticmodeconnectivity", "ininternationalconferenceonmachinelearningpp", "pmlr", "ekdeep", "singh", "lubana", "kyogo", "kawaguchi", "robert", "p", "dick", "hidenori", "tanaka", "percolation", "model", "emergence", "analyzing", "transformer", "trained", "formal", "language", "arxiv", "preprint", "arxiv", "shirleymarkranimoranthomasparrstevewkennerleyandtimothyejbehrenstransferring", "structuralknowledgeacrosscognitivemapsinhumansandmodels", "naturecommunications", "shirley", "mark", "phillipp", "schwartenbeck", "avital", "hahamy", "veronika", "samborska", "alon", "b", "baram", "timothyebehrensflexibleneuralrepresentationsofabstractstructuralknowledgeinthehuman", "entorhinalcortex", "elife", "samuelmarksandmaxtegmarkthegeometryoftruthemergentlinearstructureinlargelanguage", "modelrepresentationsoftruefalsedatasets", "urlhttpsarxivorgabs", "tomasmikolovkaichengregcorradoandjeffreydean", "efficientestimationofwordrepresen", "tationsinvectorspace", "urlhttpsarxivorgabs", "neelnandaandrewleeandmartinwattenberg", "emergentlinearrepresentationsinworldmodels", "ofselfsupervisedsequencemodels", "arxivpreprintarxiv", "preprint", "mejnewmanthestructureandfunctionofcomplexnetworkssiamreview", "january", "issn", "kento", "nishi", "maya", "okawa", "rahul", "ramesh", "mikail", "khona", "hidenori", "lubana", "tanaka", "ekdeep", "singh", "representation", "shattering", "transformer", "synthetic", "study", "knowledge", "editing", "arxivpreprintarxiv", "corefranciscoparkekdeepsinghlubanaitamarpresandhidenoritanakacompetitiondynam", "icsshapealgorithmicphasesofincontextlearning", "arxivpreprintarxiva", "core", "francisco", "park", "maya", "okawa", "andrew", "lee", "ekdeep", "singh", "lubana", "hidenori", "tanaka", "emergenceof", "hiddencapabilities", "exploringlearningdynamics", "inconceptspace", "b", "url", "httpsarxivorgabs", "kihoparkyojoongchoeyibojiangandvictorveitch", "thegeometryofcategoricalandhierar", "chicalconceptsinlargelanguagemodels", "arxivpreprintarxivc", "kihoparkyojoongchoeandvictorveitchthelinearrepresentationhypothesisandthegeometry", "oflargelanguagemodelsd", "urlhttpsarxivorgabs", "romapatelandelliepavlick", "mappinglanguagemodelstogroundedconceptualspaces", "ininter", "nationalconferenceonlearningrepresentations", "jeffreypenningtonrichardsocherandchristopherdmanning", "glove", "globalvectorsforword", "representation", "inproceedingsoftheconferenceonempiricalmethodsinnaturallanguage", "processingemnlppp", "benprystawskimichaelliandnoahgoodman", "whythinkstepbystep", "reasoningemergesfrom", "thelocalityofexperience", "advancesinneuralinformationprocessingsystems", "tian", "qin", "naomi", "saphra", "david", "alvarezmelis", "sometimes", "tree", "data", "drive", "unstable", "hierarchicalgeneralization", "arxivpreprintarxiv", "rahulrameshekdeepsinghlubanamikailkhonarobertpdickandhidenoritanaka", "com", "positional", "capability", "autoregressive", "transformer", "study", "synthetic", "interpretable", "task", "arxivpreprintarxiv", "nina", "rimsky", "nick", "gabrieli", "julian", "schulz", "meg", "tong", "evan", "hubinger", "alexander", "turner", "steering", "llama", "via", "contrastive", "activation", "addition", "lunwei", "ku", "andre", "martin", "vivek", "srikumar", "ed", "proceeding", "nd", "annual", "meeting", "association", "com", "putational", "linguistics", "volume", "long", "paper", "pp", "bangkok", "thailand", "august", "association", "computational", "linguistics", "doi", "vacllong", "url", "httpsaclanthologyorgacllong", "adamsshaisarahemarzenlucasteixeiraalexandergietelinkoldenzielandpaulmriech", "er", "transformer", "represent", "belief", "state", "geometry", "residual", "stream", "arxiv", "preprint", "arxiv", "danielspielman", "spectralandalgebraicgraphtheory", "yalelecturenotesdraftofdecember", "aarohi", "srivastava", "abhinav", "rastogi", "abhishek", "rao", "abu", "awal", "md", "shoeb", "abubakar", "abid", "adam", "fisch", "adam", "r", "brown", "adam", "santoro", "aditya", "gupta", "adria", "garrigaalonso", "et", "al", "beyond", "imitationgamequantifyingandextrapolatingthecapabilitiesoflanguagemodelsarxivpreprint", "arxiv", "eric", "todd", "millicent", "l", "li", "arnab", "sen", "sharma", "aaron", "mueller", "byron", "c", "wallace", "david", "bau", "functionvectorsinlargelanguagemodels", "arxivpreprintarxiv", "aaron", "traylor", "roman", "feiman", "ellie", "pavlick", "neural", "network", "learn", "implicit", "logic", "physicalreasoning", "intheeleventhinternationalconferenceonlearningrepresentations", "williamthomastutte", "howtodrawagraph", "proceedingsofthelondonmathematicalsociety", "preprint", "keyonvafajustinychenjonkleinbergsendhilmullainathanandasheshrambachan", "evalu", "atingtheworldmodelimplicitinagenerativemodel", "arxivpreprintarxiv", "johannes", "von", "oswald", "eyvind", "niklasson", "ettore", "randazzo", "joao", "sacramento", "alexander", "mordv", "intsev", "andrey", "zhmoginov", "max", "vladymyrov", "transformer", "learn", "incontext", "gradient", "descent", "ininternationalconferenceonmachinelearningpppmlra", "johannes", "von", "oswald", "maximilian", "schlegel", "alexander", "meulemans", "seijin", "kobayashi", "eyvind", "niklassonnicolaszucchetninoscherrernolanmillermarksandlermaxvladymyrovetal", "uncovering", "mesaoptimization", "algorithm", "transformer", "arxiv", "preprint", "arxiv", "b", "jason", "wei", "yi", "tay", "rishi", "bommasani", "colin", "raffel", "barret", "zoph", "sebastian", "borgeaud", "dani", "yo", "gatamamaartenbosmadennyzhoudonaldmetzleretalemergentabilitiesoflargelanguage", "model", "arxivpreprintarxiv", "kaiyuewenyuchenlibingbinliuandandrejristeski", "transformersareuninterpretablewith", "myopic", "method", "case", "study", "bounded", "dyck", "grammar", "advance", "neural", "information", "processingsystems", "jamescrwhittingtontimothyhmullershirleymarkguifenchencaswellbarryneilburgess", "andtimothyejbehrens", "thetolmaneichenbaummachine", "unifyingspaceandrelationalmem", "orythroughgeneralizationinthehippocampalformation", "cell", "yongyi", "yang", "david", "p", "wipf", "et", "al", "transformer", "optimization", "perspective", "advance", "neuralinformationprocessingsystems", "preprint", "additional", "experimental", "detail", "hereweprovidesomeadditionaldetailsregardingourexperimentalsetups", "contextwindows", "ouranalysesrequirecomputingmeantokenrepresentationsh", "foreverytoken", "graph", "grab", "activation", "per", "token", "recent", "context", "windowofn", "token", "becausewefurtherrequirethateachtokenisobservedatleastonceinour", "w", "window", "use", "batch", "prompt", "batch", "size", "equal", "number", "node", "graph", "foreachpromptinthebatchwestartourrandomtraversalorrandompairwisesampling", "different", "node", "ensuring", "node", "show", "least", "context", "case", "whenourcontextlengthn", "islongerthanthewindowwesimplyuseeverytokenn", "n", "c", "w", "c", "computationalresources", "werunourexperimentsoneitheranodesorbyusingtheapis", "providedbyndiffiottokaufmanetal", "b", "connection", "energy", "minimization", "pca", "stucture", "inthissectionforamatrixm", "rndweuselowercaseboldletterswithsubscripttorepresentthe", "columnsformeg", "representsthekthcolumnofm", "moreoverweuse", "mtorepresent", "k", "k", "kth", "largest", "singular", "value", "psd", "use", "represent", "kth", "k", "largesteigenvalueofm", "moreover", "weusee", "torepresentavectorwithallzeroentriesexcepta", "k", "atentrykwhosedimensionisinferredfromcontextandtorepresentavectorwithallentries", "foranaturalnumbernweusentorepresent", "n", "furthermoreweusecid", "zkcidn", "torepresenttheenergyminimizersofthedirichletenergydefined", "k", "insection", "leta", "rnn", "betheadjacencymatrixofthegraph", "diagabethedegree", "matrixandl", "dabethelaplacianmatrix", "throughaneasycalculationonecanknowthat", "foranyvectorxrn", "e", "xxlx", "g", "thereforefromthespectraltheoremeg", "theoreminspielmanweknowthatz", "k", "theeigenvectoroflcorrespondingto", "le", "z", "nk", "g", "k", "wewillshowthatifamatrixh", "rndminimizestheenergyandisnondegeneratedhasseveral", "distinctandnonzerosingularvaluesthenthepcamustexactlygivetheleadingenergyminimiz", "ersstartingfromz", "theoremb", "letgbeagraphand", "besminnddistinctpositive", "number", "letmatrixh", "rndbethesolutionofthefollowingoptimizationproblem", "h", "arg", "min", "e", "x", "g", "xrnd", "st", "x", "k", "r", "k", "k", "thenthekthprinciplecomponentofh", "fork", "risbez", "k", "proof", "wefirstprovethattheleadingleftsingularvectorsofh", "areexactlyenergyminimizers", "let", "r", "minnd", "let", "svd", "h", "h", "uv", "diag", "singularvaluesofhandu", "rnrv", "rrd", "preprint", "leth", "representstheithrowofh", "noticethat", "e", "ghcid", "ijcid", "cidh", "ih", "jcid", "cid", "ij", "cid", "cid", "cide", "e", "hcid", "cid", "ijcid", "j", "cid", "ij", "cid", "cid", "cide", "e", "ucid", "cid", "ijcid", "j", "cid", "ij", "r", "cidcid", "e", "e", "u", "k", "j", "k", "ij", "k", "r", "cid", "e", "u", "k", "g", "k", "k", "since", "u", "independent", "matter", "value", "u", "know", "k", "k", "k", "k", "take", "smallest", "possible", "value", "given", "condition", "k", "k", "k", "k", "r", "k", "since", "u", "singular", "vector", "u", "orthogonal", "using", "theorem", "k", "k", "fanweknowthatforanys", "ntheminimizerofcids", "e", "u", "isu", "z", "k", "k", "g", "k", "k", "k", "therefore", "evident", "minimizer", "cid", "e", "u", "must", "satisfies", "u", "z", "k", "k", "k", "g", "k", "k", "k", "sincefromtheaboveargumentof", "sandthegivenconditionconditionweknowthat", "k", "nowwehaveprovedthatu", "z", "k", "nextweconsidertheoutputofpcaletp", "bethe", "k", "k", "k", "kthprinciplecomponentoutputbythepcaofh", "weknowthatp", "istheeigenvectorof", "k", "c", "hcidhcid", "thatcorrespondstothekthlargesteigenvalueofcwherehcid", "h", "h", "isthecentralized", "n", "h", "fromthespectraltheoremwehave", "p", "arg", "max", "pcp", "k", "psn", "ppiik", "letj", "spanbethesetofvectorswhoseeveryentryhasthesamevalueletjbethesubspace", "inrnthatisorthogonaltojforasubspacekofrnlet", "rn", "rnbetheprojectionoperator", "k", "ontok", "wehavethat", "p", "arg", "max", "pcp", "psn", "cid", "cid", "cid", "cid", "cid", "cid", "arg", "max", "p", "hh", "p", "psn", "n", "n", "arg", "max", "cid", "phh", "pcid", "j", "j", "psn", "arg", "max", "cid", "phhpcid", "psn", "pj", "whichagainfromspectraltheoremistheeigenvectorofthesecondlargesteigenvalueofhh", "whichisu", "z", "usinganinductionandthesamereasoningitfollowsthatforanyk", "swe", "havep", "z", "thisprovestheproposition", "k", "k", "preprint", "c", "additional", "result", "c", "detailedlayerwisevisualizationofrepresentations", "infigureandfigureweprovideadditionalvisualizationsperlayerforeachofourmodelsand", "eachofourdatageneratingprocesses", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "figure", "weplotdpcaprojectionsfromeveryotherlayerinllamabdubeyetal", "giventhegridtraversaltask", "indeeperlayerswecanseeaclearvisualizationofthegrid", "preprint", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "layer", "figure", "weplotdpcaprojectionsfromeveryotherlayerinllamabdubeyetal", "forthehexagonalgridtask", "preprint", "c", "pcadirichletenergyandaccuracyresultsonothermodels", "hereweprovideresultsfromotherlanguagemodelsiellamabdubeyetalllama", "binstruct", "gemmab", "gemma", "team", "gemmab", "figure", "plot", "pca", "projection", "last", "layer", "various", "model", "various", "data", "generating", "process", "figure", "plot", "normalized", "dirichlet", "energy", "curve", "accuracy", "various", "language", "modelsonvarioustasks", "acrossallmodelsandtasksweseeresultssimilartothemainpaper", "llamab", "grid", "llamabinstruct", "grid", "gemmab", "grid", "gemmab", "grid", "llamab", "ring", "llamabinstruct", "ring", "gemmab", "ring", "gemmab", "ring", "llamab", "hex", "llamabinstruct", "hex", "gemmab", "hex", "gemmab", "hex", "figure", "weplotdpcaprojectionsfromthelastlayerofvariouslanguagemodelsgivenvarious", "datageneratingprocesses", "forthegridandhexagonalgraphsweapplypcaonthelastlayers", "ring", "visualize", "layer", "respectively", "interestingly", "llamab", "findtheringrepresentationinthendandrdprincipalcomponents", "c", "standardizeddirichletenergy", "fig", "report", "dirichlet", "energy", "value", "computed", "standardization", "representation", "ie", "aftermeancenteringthemandnormalizingbythestandarddeviation", "thisrendersthetrivial", "solution", "dirichlet", "energy", "minimization", "infeasible", "since", "assigning", "constant", "representation", "node", "yield", "infinite", "energy", "due", "zero", "variance", "seen", "result", "plot", "arequalitativelysimilartothenonstandardizedenergyresultsfigbutmorenoisyespecially", "ring", "graph", "expected", "since", "standardization", "exacerbate", "influence", "noise", "yieldingfluctuationsintheenergycalculation", "preprint", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "llamab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "llamab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "llamabinstruct", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "gemmab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "gemmab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamabinstruct", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamabinstruct", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "hex", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "figure", "accuracy", "versus", "normalized", "dirichlet", "energy", "curve", "various", "language", "model", "various", "task", "every", "model", "task", "see", "energy", "minimized", "accuracy", "starting", "improve", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "llamab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "llamab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "llamabinstruct", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "gemmab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "layer", "layer", "accuracy", "gemmab", "grid", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamabinstruct", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "ring", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamab", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "llamabinstruct", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "hex", "context", "length", "ygrene", "telhcirid", "dezilamron", "gemmab", "hex", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "ycarucca", "figure", "accuracy", "versus", "zero", "mean", "centered", "normalized", "dirichlet", "energy", "curve", "various", "languagemodelsonvarioustasks", "zeromeancenteringensuresthatgraphrepresentationsarenot", "using", "trivial", "solution", "energy", "minimization", "ie", "assigning", "representation", "every", "node", "preprint", "c", "causalanalysisofrepresentations", "inthissectionwereportpreliminarycausalanalysesofourgraphrepresentationswhilefullyunder", "standing", "mechanism", "behind", "formation", "representation", "well", "relationship", "said", "representation", "model", "output", "interesting", "future", "direction", "focusofourworkandthusweonlyranproofofconceptexperiments", "withthatsaidweask", "dotheprincipalcomponentsthatencodeourgraphrepresentationshaveany", "causalroleinthemodelspredictions", "totestthisweattempttomovethelocationoftheactivationsforonenodeofthegraphtoanother", "bysimplyrescalingitsprincipalcomponents", "namelyassumeactivationhcorrespondingtonode", "layer", "say", "wish", "move", "activation", "different", "target", "node", "j", "first", "compute", "mean", "representation", "node", "j", "using", "activation", "corresponding", "node", "j", "within", "recent", "n", "timesteps", "notated", "h", "assuming", "first", "two", "principal", "component", "encode", "w", "j", "thecoordinatesofthenode", "wesimplyrescaletheprincipalcomponentsofh", "tomatchthatof", "h", "j", "weviewthisapproachasratherrudimentary", "namelytherearelikelymoreinformativevectorsthat", "encodericherinformationsuchasinformationaboutneighboringnodes", "howeverwedofindthat", "thefirsttwoprincipalcomponentshavesomecausalroleinthemodelspredictions", "wetestourrescalinginterventiononrandomlygeneratedcontexts", "foreachcontextassum", "ingourunderlyinggraphhasnnodeswetestmovingtheactivationsofthelasttokenitoalln", "otherlocationsinthegraph", "wethenreporttheaveragedmetricacrosstheresultingn", "testcases", "wereportmetricsaccuracyhithitandaccumulatedprobabilitymassonvalidtokens", "hit", "hit", "report", "percentage", "time", "top", "top", "predicted", "token", "valid", "neighbor", "target", "node", "j", "accumulated", "probability", "mass", "simply", "sum", "probabilitymassallocatedtoallneighborsievalidpredictionsofthetargetnodej", "tablereportsourresultsforourringandgridtasks", "weincluderesultsforrescalingwithor", "principalcomponentsaswellasnullinterventionsandinterventionswitharandomvector", "overall", "wefindthattheprincipalcomponentshavesomecausaleffectonthemodelsoutputpredictionsbut", "doesnotprovideafullexplanation", "ring", "grid", "hex", "hit", "hit", "prob", "hit", "hit", "prob", "hit", "hit", "prob", "interv", "n", "interv", "n", "nullinterv", "randominterv", "table", "interventionresultsforourringandgridtasks", "wedemonstratethatoftentimessimply", "rescalingtheprincipalcomponentforeachtokenrepresentationcanmovethetokentoadifferent", "position", "graph", "however", "note", "simple", "rescaling", "approach", "perfectly", "captureacausalrelationshipbetweenprincipalcomponentsandmodelpredictions", "c", "empiricalsimilarityofprincipalcomponentsandspectralembeddings", "theorempredictsthatifthemodelrepresentationsareminimizingthedirichletenergythefirst", "twoprincipalcomponentswillbeequivalenttothespectralembeddingszz", "empirically", "measure", "whether", "first", "two", "principal", "component", "indeed", "equivalent", "thespectralembeddings", "intablewemeasurethecosinesimilarityscoresbetweentheprincipal", "componentsandspectralembeddings", "c", "accuracyofincontexttaskswithaconflictingsemanticprior", "whatwouldhappenwhenanincontexttaskwhichcontradictsasemanticpriorisgiventoamodel", "namelyengelsetalshowthatwordslikedaysoftheweekhaveacircularrepresentation", "preprint", "cospcz", "cospcz", "grid", "ring", "hex", "table", "absolute", "value", "cosine", "distance", "principal", "component", "model", "activation", "spectralembeddings", "weempiricallyobservethatinpracticethesecoordinatesendupbeingvery", "similar", "grid", "hexagon", "use", "principal", "component", "last", "layer", "ringweuseanearlierlayerlayerinwhichtheringisobserved", "experiment", "randomly", "shuffle", "token", "day", "week", "ie", "token", "mon", "tue", "wed", "thu", "fri", "sat", "sun", "define", "new", "ring", "give", "random", "neighboring", "pair", "thenewlydefinedringasourincontexttask", "figuredemonstratestheaccuracywhengivenanincontexttaskthatiscontradictorytoasemantic", "prior", "interestingly", "wefirstobservethemodelmakepredictionsthatreflectstheoriginalsemantic", "prior", "pink", "accuracy", "drop", "quickly", "model", "capture", "semantic", "rule", "beingfollowed", "withmoreexemplarsweseeaslowdecayoftheremainingsemanticaccuracyand", "transition", "model", "behavior", "begin", "make", "prediction", "reflect", "newly", "defined", "orderingofourringblue", "number", "example", "ycarucca", "shuffled", "semantic", "figure", "incontextstructureoverridessemanticprior", "givenanincontexttaskthatcontra", "dictsamodelssemanticpriorweobservethemodeltransitionfrommakingpredictionsthatadhere", "tothesemanticpriorpinktopredictionsthatreflectthenewlydefinedincontexttask", "furthermore", "fig", "quantify", "dirichlet", "energy", "computed", "certain", "pc", "dimen", "sion", "wefindthatenergyminimizationhappensinthedimensionscorrespondingtotheincontext", "structure", "context", "length", "ygrene", "total", "pca", "pca", "figure", "energyminimizationhappensintheincontextcomponentdimensions", "weshow", "dirichlet", "energy", "depending", "context", "given", "taking", "semantic", "pca", "incontext", "pca", "dimension", "show", "energy", "minimization", "happens", "pca", "corre", "spondingtotheincontextdimensions", "preprint", "c", "additionalempiricalverificationsoftransitionpredictions", "hereweprovideadditionaldetailsforempiricallyverifyingourpredictionsformodeltransitions", "figuresanddemonstratedetailedaccuracycurvesforawiderangeofgraphsizes", "figureemergentbehaviorforvaryingtaskcomplexitygraphsizeforthehexagonaltask", "plot", "accuracy", "varying", "level", "complexity", "graph", "size", "hexagonal", "incontext", "task", "interestinglyregardlessofgraphsizeweseeanabruptdiscontinuouschangeinthemodels", "performance", "figuredemonstratesthatwecanpredictwhensuchabruptchangecanbeexpected", "asafunctionoftaskcomplexity", "example", "index", "robhgienp", "graph", "size", "llama", "icl", "raw", "llama", "icl", "smoothed", "piecewise", "linear", "fit", "transition", "point", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "figure", "emergentbehaviorforvaryingtaskcomplexitygraphsizeforthegridtask", "plottheaccuracyforvaryinglevelsofcomplexitygraphsizeforthegridincontexttask", "interest", "inglyregardlessofgraphsizeweseeanabruptdiscontinuouschangeinthemodelsperformance", "figuredemonstratesthatwecanpredictwhensuchabruptchangescanbeexpectedasafunction", "oftaskcomplexity", "preprint", "example", "index", "robhgienp", "graph", "size", "llama", "icl", "raw", "llama", "icl", "smoothed", "piecewise", "linear", "fit", "transition", "point", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "example", "index", "robhgienp", "graph", "size", "figure", "emergent", "behavior", "varying", "task", "complexity", "graph", "size", "ring", "task", "plot", "accuracy", "varying", "level", "complexity", "graph", "size", "ring", "incontext", "task", "interestinglyregardlessofgraphsizeweagainseeanabruptdiscontinuouschangeinthemodels", "performance", "node", "b", "figure", "incontextemergenceinahexagonalgraphtracingtask", "weanalyzetheincontext", "accuracycurvesasafunctionofcontextsizeinputtedtothemodel", "thegraphusedinthisexper", "iment", "mm", "grid", "varying", "value", "rule", "following", "accuracy", "graph", "tracing", "task", "accuracy", "show", "two", "phase", "ascent", "fit", "piecewise", "linear", "function", "ob", "servedascenttoextractthetransitionpointwhichmovesrightwardswithincreasinggraphsize", "b", "interestinglythetransitionpointscalesasapowerlawinmiethenumberofnodesinthegraph", "node", "b", "node", "c", "node", "node", "figure", "hexagonal", "graph", "tracing", "accuracy", "compared", "memorization", "solution", "rulefollowingaccuraciesonthehexagonalgraphcomparedtothememorizationmodelinsec", "hexagonalgraphwithabcdnodes", "generallywefindthatthehexagonalgraph", "trackingaccuracyfromllamabdubeyetalislowerthantheshotmemorization", "modelindicatingthattheremightbeadifferentunderlyingprocess"], "sentences": ["preprint iclr incontext learning representation corefranciscoparkandrewleeekdeepsinghlubanayongyiyang mayaokawakentonishimartinwattenberghidenoritanaka cbsnttprograminphysicsofintelligenceharvarduniversity departmentofphysicsharvarduniversity physicsinformaticslabnttresearchinc seasharvarduniversity cseuniversityofmichiganannarbor abstract recentworkhasdemonstratedthatsemanticsspecifiedbypretrainingdatainflu encehowrepresentationsofdifferentconceptsareorganizedinalargelanguage modelllmhowevergiventheopenendednatureofllmsegtheirability toincontextlearn wecanaskwhethermodelsalterthesepretrainingsemantics toadoptalternativecontextspecifiedones specificallyifweprovideincontext exemplarswhereinaconceptplaysadifferentrolethanwhatthepretrainingdata suggests model reorganize representation accordance novel semantics answer question take inspiration theory conceptual role semantics define toy graph tracing task wherein nodesofthegrapharereferencedviaconceptsseenduringtrainingegapple birdetcandtheconnectivityofthegraphisdefinedviasomepredefinedstruc tureegasquaregrid givenexemplarsthatindicatetracesofrandomwalkson thegraph weanalyzeintermediaterepresentationsofthemodelandfindthatas theamountofcontextisscaledthereisasuddenreorganizationfrompretrained semantic representation incontext representation aligned graph structure furtherwefindthatwhenreferenceconceptshavecorrelationsintheir semanticseg monday tuesday etc thecontextspecified graphstructure isstillpresentintherepresentationsbutisunabletodominatethepretrainedstruc ture explain result analogize task energy minimization apredefinedgraphtopologyprovidingevidencetowardsanimplicitoptimization processtoinfercontextspecifiedsemanticsoverallourfindingsindicatescaling contextsize flexibly reorganize model representation possibly unlocking novelcapabilities introduction agrowinglineofworkdemonstratesthatlargelanguagemodelsllmsorganizerepresentationsof specificconceptsinamannerthatreflectstheirstructureinpretrainingdataparketalcden gelsetalabdouetalpatelpavlickanthropicaigurneetegmark vafa et al li et al pennington et al targeted experiment syntheticdomainshavefurthercorroboratedthesefindingsshowinghowmodelrepresentationsare organizedaccordingtothedatageneratingprocesslietaljenneretaltrayloretal liuetalbshaietalparketalbgopalanietal howeverwhen amodelisdeployedinopenendedenvironmentswecanexpectittoencounternovelsemanticsfor aconceptthatitdidnotseeduringpretraining forexample assumethatwedescribetoanllm new product called strawberry announced ideally based context modelwouldaltertherepresentationforstrawberryandreflectthatwearenotreferringtothe pretrainingsemanticsegthefruitstrawberry doesthisidealsolutiontranspireinllms motivatedbytheaboveweevaluatewhetherwhenprovidedanincontextspecificationofaconcept anllmaltersitsrepresentationstoreflectthecontextspecifiedsemantics specificallywepropose equal contribution contact corefranciscoparkandrewleegharvardedu yongyiumicheduekdeeplubana hidenori tanakafasharvardedu ced lcsc vvixrapreprint word grid c emergent grid representation context apple bird car egg house milk plane opera box sand sun mango rock math code phone b data generation random walk grid apple bird milk sand sun plane opera context length context length context length figure alteration representation accordance contextspecified semantics grid structure randomly arrange set concept grid reflect correla tionalsemanticsbetweenthetokens bwethengeneratesequencesoftokensfollowingarandom walk grid inputting context llamab model c model mean token representationsprojectedontothetoptwoprincipalcomponents asthenumberofincontextex emplarsincreasesthereisaformationofrepresentationsmirroringthegridstructureunderlyingthe datageneratingprocess representationsarefromtheresidualstreamactivationfollowinglayer incontext learning task involves simple graph tracing problem wherein model shownedgescorrespondingtoarandomtraversalofagraphseefigthenodesofthisgraphare intentionallyreferencedviaconceptsthemodelisextremelylikelytohaveseenduringtrainingeg applebirdetcwhileitsconnectivitystructureisdefinedusingapredefinedgeometrythatis ambivalenttocorrelationsbetweenconceptssemanticsegasquaregrid basedontheprovided context themodelisexpectedtooutputavalidnextnodeprediction ie anodeconnectedtothe lastpresentedone asweshowincreasingtheamountofcontextleadstoasuddenreorganization ofrepresentationsinaccordancewiththegraphsconnectivity thissuggestsllmscanmanipulate representation order reflect concept semantics specified entirely incontext inline theory inferential semantics cognitive science harman block characterizetheseresultsbyanalyzingtheproblemofdirichletenergyminimizationshowingthat modelsindeedidentifythestructureoftheunderlyinggraphtoachieveanontrivialaccuracyonour task thissuggestsanimplicitoptimizationprocessashypothesizedbytheoreticalworkoniclin toysetupsegincontextlinearregressioncantranspireinmorenaturalisticsettingsvonoswald etalabakyureketal overallourcontributionscanbesummarizedasfollows graphnavigationasasimplisticmodelofnovelsemantics weintroduceatoygraphnav igation task requires model interpret semantically meaningful concept referent nodesinastructurallyconstrainedgraph inputtingtracesofrandomwalksonthisgraphintoan llmweanalyzewhetherthemodelaltersitsintermediaterepresentationsforreferentconcepts topredictvalidnextnodesasdefinedbytheunderlyinggraphconnectivityhenceinferringinline withtheoriesofsemanticsfromcognitivesciencenovelsemanticsofaconceptharman emergent incontext reorganization concept representation result show contextsizeisscaledieasweaddmoreexemplarsincontextthereisasuddenreorganization ofconceptrepresentationsthatreflectsthegraphsconnectivitystructure intriguingly thesere sults similar one achieved similar setup human subject garvert et al whittington et al show contextspecified graph structure emerges even use concept correlation semantics eg mon tues etc terestinglyisunabletodominatethepretrainedstructure morebroadlywenotethatthissudden reorganization reminiscent emergent capability llm relevant ax eg computeormodelsizearescaledweietalsrivastavaetallubanaetal energy minimization model semantics inference provide quantitative ac count result compute dirichlet energy model representation respect groundtruth graph structure find energy decrease function context size offer precise hypothesis mechanism employed llm reorganize repre sentationsaccordingtothecontextspecifiedsemanticsofaconcept theseresultsalsoserveas preprint htgnel txetnoc htgnel txetnoc word ring c emergent ring representation context banana apple layer layer layer layer tomato lettuce onion grape orange fig pear carrot b data generation randomly pick pair neighbor apple banana orange onion fig carrot grape lettuce figure alteration representation accordance contextspecified semantics ring structure awerandomlyplaceconceptsonaringstructureunrelatedtotheirsemantics bwe thengeneratesequencesoftokensbyrandomlysamplingneighboringpairsfromtheringwhichis usedastheinputcontexttoallamabmodel cthemodelsmeanrepresentationoftokens projectedontothetoptwoprincipalcomponents asthenumberofincontextexemplarsincreases formation representation mirroring ring structure underlying datagenerating process therepresentationsarefromtheresidualstreamactivations evidence towards theory incontext learning implicit optimization naturalistic settingvonoswaldetalabakyureketal experimental setup incontext graph tracing wefirstdefineoursetupforassessingtheimpactofcontextspecificationonhowamodelorganizes representation main paper primarily focus llamab henceforth llama dubeyetalaccessedviandifnnsightfiottokaufmanetal wepresentresults modelsllamab llamabinstruct dubey et al gemmab gemmab gemmateaminappc task proposed task call incontext graph tracing involves random walk predefinedgraphgspecificallyinspiredbypriorworkanalyzingstructuredrepresentationslearned bysequencemodelsweexperimentwiththreegraphicalstructuresasquaregridfigaaring figaandahexagonalgridfig resultsonhexagonalgridaredeferredtoappendixdue tospaceconstraints toconstructthesquaregrid werandomlyarrangethesetoftokensinagrid andaddedgesbetweenhorizontalandverticalneighbors wethenperformarandomwalkonthe graph emitting visited token sequence fig b ring add edge neighboringnodesandsimplysamplerandompairsofneighboringtokensonthegraphfigb node graph denoted referenced via concept model n extremely likely seen pretraining choice concept plausible selectrandomtokensthatunlessmentionedotherwisehavenoobvioussemanticcorrelationswith one another eg apple sand math etc however concept precise meaning associated training data necessitating extent model relies providedcontexttherepresentationsaremorphedaccordingtotheincontextgraph wehighlight thatavisualanalogofourtaskwhereinoneusesimagesinsteadoftexttokenstorepresentaconcept hasbeenusedtoelicitverysimilarresultswithhumansubjectsastheoneswereportinthispaper using llm garvert et al whittington et al mark et al brady et al wealsonotethatourproposedtaskissimilartoonesstudiedinliteratureonincontextrl whereinoneprovidesexplorationtrajectoriesincontexttoamodelandexpectsittounderstandthe environmentanditsdynamicsakaaworldmodelleeetalblaskinetal preprint result visualizinginternalactivationusingprincipalcomponents sinceweareinterestedinuncoveringcontextspecificrepresentationsweinputsequencesfromour datageneratingprocesstothemodelandfirstcomputethemeanactivationsforeachuniquetoken namely assume given context c c c c originates n anunderlyinggraphg ateachtimestep welookatawindowofn precedingtokensor w alltokensifthecontextlengthissmallerthann andcollectallactivationscorrespondingtoeach w token atagivenlayer wethencomputethemeanactivationspertokendenotedash rd wefurtherdenotethestackofmeantokenrepresentationsasht rnd finallywerun pcaonht andusethefirsttwoprincipalcomponentstovisualizemodelactivationsunless stated otherwise note pca visualization known suffer pitfall representationanalysismethodweprovideathoroughquantitativeanalysisinsectodemonstrate thatthemodelreorganizesconceptrepresentationsaccordingtotheincontextgraphstructureand proveinsecthatthestructureofthegraphisreflectedinthepcavisualizationsbecauseofthis reorganizationofrepresentations wealsoprovidefurtherevidenceonthefaithfulnessofpcaby conducting preliminary causal analysis principal component finding intervening conceptrepresentationsprojectionsalongthesecomponentsaffectsthemodelsabilitytoaccurately predictvalidnextnodegenerationsappc result fig demonstrate resulting visualization square grid ring graph spectively example provided appendix see fig strikingly enough exemplarswefindrepresentationsareinfactorganizedinaccordancewiththegraphstructureun derlying context interestingly result skewed earlier layer towards semantic priorsthemodelmayhaveinternalizedduringtraininghoweverthesepriorsareoverriddenaswe godeeperinthemodel forexampleintheringgraphseefigconceptsappleandorange areclosertoeachotherinlayerofthemodelbutbecomeessentiallyantipodalaroundlayer asdictatedbythegraphtheantipodalnatureisalsomoreprominentascontextlengthisincreased also observe despite developing squaregrid structure sufficient context length given see fig structure partially irregular eg wider central region narrowlyarrangedintheperiphery wefindthistobeanartifactoffrequencywithwhichaconcept seen context specifically due lack periodic boundary condition concept presentintheinnerregionofthegridarevisitedmorefrequentlyduringarandomwalkonthe graphwhiletheperipheryofthegraphhasalowervisitationfrequency therepresentationsreflect thisthusorganizinginaccordancewithbothstructureandfrequencyofconceptsinthecontext overalltheresultsaboveindicatethataswescalecontextsizemodelscanreorganizesemantically unrelatedconceptstoformtaskspecificrepresentationswhichwecallincontextrepresentations intriguingly result broadly inline theory inferential semantics cognitive scienceaswellharmanblock semanticpriorvs incontexttaskrepresentations buildingonresultsfromtheprevioussectionwenowinvestigatetheimpactofusingsemantically correlatedconcepts specificallywebuildontheresultsfromengelsetalwhoshowthat representationsfordaysoftheweek ie monday tuesday wednesday thursday friday saturday sunday organize circular geometry randomly permute orderingoftheseconceptsarrangethemonanoderinggraphsimilartotheprevioussectionsee figaandevaluatewhethertheincontextrepresentationscanoverridethestrongpretrainingprior internalizedbythemodel result figbcdemonstratetheresultingvisualizationswefindthatwhenthereisaconflict semantic prior incontext task observe original semantic ring first twoprincipalcomponents howeverthecomponentsrightafterinfactencodethecontextspecific structure visualizingthethirdandfourthprincipalcomponentsshowsthenewlydefinedringstruc ture thisindicatesthatthecontextspecifiedstructureispresentintherepresentationsbutdoesnot dominatethem infig wereportthemodelsaccuracyontheincontexttask findingthatthe modeloverridesthesemanticpriortoperformwellonourtaskwhenenoughcontextisgiven preprint tnenopmoc lapicnirp principal component tnenopmoc lapicnirp b c tuesday monday wednesday sunday thursday saturday principal component friday semantic link incontext link figure incontext representation form higher principal component presence semantic prior purple semantic link underlying day week dashed blue define nonsemantic graph structure linking nonneighboring day generate token thisgraph bpurpletheringgeometryformedbysemanticlinksestablishedduringpretraining remainsintactinthefirsttwoprincipalcomponents cdashedbluethenonsemanticstructure provided incontext seen third fourth principal component note star structureinthefirsttwocomponentsbwhichmatchthegroundtruthgraphicalstructureofourdata generatingprocessabecomesaringinthenexttwoprincipalcomponentsctherepresentations arefromtheresidualstreamactivationfollowinglayer effect context scaling emergent reorganization representation result previous section demonstrate model reorganize concept representation accordance contextspecified semantics next aim study behavior arises ascontextisscaledisthereacontinuous monotonicimprovementtowardsthecontextspecified structureascontextisaddedifsoisthereatrivialsolutionegregurgitationbasedoncontextthat helpsexplaintheseresults toanalyzethesequestions wemustfirstdefineametricthathelpsus gaugehowalignedtherepresentationsarewiththestructureofthegraphthatunderliesthecontext dirichlet energy measure dirichlet energy graph g structure defining energy function model representation specifically undirected graph g n nodesletarnnbeitsadjacencymatrixandxrnbeasignalvectorthatassignsavaluex toeachnodei thenthedirichletenergyofthegraphwithrespecttoxisdefinedas cid e x x x g ij j ij multidimensional signal dirichlet energy defined summation energy dimension specifically let x rnd matrix assigns node ddimensionalvectorx thenthedirichletenergyofx isdefinedby cidcid cid e x x x x x g ij ik jk ij j k ij ij overall empirically quantify formation geometric representation measure dirichlet energy respect graph underlying data generating process dgps ourmeantokenactivationsh cid e ht hh g ij j ij ht rnd stack mean token representation h layer ij token dgp certain context length note ht function context preprint context length ygrene telhcirid dezilamron grid context length ygrene telhcirid dezilamron ring context length ygrene telhcirid dezilamron hex ycarucca ycarucca ycarucca layer layer accuracy figure model continuously develops task representation learns traverse novel graphsincontext weplottheaccuracyofgraphtraversalandthedirichletenergyofthegraph computed model internal representation function context length note thedirichletenergyneverreachesaperfectzerorulingoutthattherepresentationsarelearninga degeneratestructureaswasalsoseeninthepcavisualizationsinsec aaxgridgraphwith node bacircularringwithnodes cahoneycombhexagonallatticewithnodes length well omit notation brevity intuitively measure indicates whetherneighboringtokensnodesinthegroundtruthgraphhaveasmalldistancebetweentheir representation thus model correctly infers correct underlying structure expect seeadecreaseindirichletenergy wedonotethatinpracticedirichletenergyminimizationhasa trivialsolutionwhereallnodesareassignedthesamerepresentationwhilewecanbeconfidentthis trivialsolutiondoesnotexistinourresultsforelsewewouldnotseedistinctnoderepresentationsin pcavisualizationsnorhighaccuracyforsolvingourtaskswestillprovideanalternativeanalysisin appcwheretherepresentationsarestandardizedmeancenteredandnormalizedbyvarianceto renderthistrivialsolutioninfeasiblewefindresultsarequalitativelysimilarwithsuchstandardized representationsbutmorenoisysincestandardizationcaninducesensitivitytonoise result emergentorganizationandtaskaccuracyimprovements weplotllamasaccuracyattheincontextgraphtracingtaskalongsidethedirichletenergymea sure different layer function context specifically compute rule following accuracy add model output probability graph node valid neighbor forinstanceifthegraphstructureisapplecarbirdwaterandthecurrentstate iscarweaddupthepredictedprobabilitiesforappleandbird thismetricsimplymeasures howwellthemodelabidesbythegraphstructure result reported fig see critical amount context seen model accuracystartstorapidlyimprove wefindthispointinfactcloselymatcheswhendirichletenergy reachesitsminimumvalue energyisminimizedshortlybeforetherapidincreaseinincontexttask accuracy suggesting structure data correctly learned model make valid prediction lead u claim amount context scaled emergentreorganizationofrepresentationsthatallowsthemodeltoperformwellonourincontext graphtracingtask wenotetheseresultsalsoprovideamorequantitativecounterpartofourpca visualizationresultsbefore trivial solution play simple baseline would exhibit increase per formance increasing context involves model merely regurgitating node neighbor copyingthemfromitscontext wecallthisthememorizationsolution whilesuchasolutionwould notexplainthereorganizationofrepresentationsweuseitasabaselinetoshowthemodelislikely engaginginamoreintriguingmechanism sinceouraccuracymetricmeasuresrulefollowingthis memorizationsolutionwillachievevalueifthenodehasbeenobservedinthecontextandother wise followingourdatasamplingprocessthenifwesimplychooseaninitialnodeatrandomwith replacementwecanexpresstheprobabilityofanodeexistinginacontextoflengthlas cid ncidl p x seen n preprint fig node b node context size context size ycarucca ycarucca llamab shot memorization shot memorization piecewise linear fit transition point figure amemorizationsolutioncannotexplainllamasiclgraphtracingperformance weplottherulefollowingaccuracyfromllamaboutputsandaccuraciesfromasimpleshot andshotmemorizationhypothesis aaringgraphwithnodes basquaregridgraphwith node inbothcaseswefindthatthememorizationsolutioncannotexplaintheaccuracyascent curve insteadwefindaslowphaseandafastphasewhichwefitwithapiecewiselinearfit x context n number node available note current node doesnotmatterasthesamplingprobabilityisuniformwithreplacement wealsoevaluateanother similarbaselinethatassumesthesametokenmuchbeencounteredtwiceforthemodeltorecognize incontext exemplar define closedform expression solution probabilitythatanodehasappearedtwiceasfollows cid cidcid ncidl p xp xl seen seen n n toevaluatewhetherthememorizationsolutionsaboveexplainourresultsweplottheirperformance alongsidetheobservedperformanceofllama figshowstheresultaonaringgraphwith nodesandbonagridgraphwithnodes wefindinbothcasesthatneithertheshotnorthe shotmemorizationcurvecanexplainthebehaviorofllamainsteadweobservethattheaccuracy two phase first phase accuracy improves slowly second phase loglinear slope suddenly change steeper ascent find piecewise linear fit extractthistransitionpointfairlywellwhichwillbeofinterestinthenextsection explaining emergent reorganization representation energy minimization hypothesis buildingontheresultsfromprevioussectionwenowputforwardahypothesisforwhyweareable toidentifysuchstructuredrepresentationsfromamodel wehypothesizethemodelinternallyruns anenergyminimizationprocessinsearchofthecorrectstructuralrepresentationofthedatayang etalsimilartoclaimsofimplicitoptimizationinincontextlearningproposedbypriorwork intoysettingsvonoswaldetalab moreformallyweclaimthefollowinghypothesis hypothesis letnbethenumberoftokensdbethedimensionalityoftherepresentationsand htt rnd stack representation token learned model layer andcontextlengthtthene cid httcid decayswithcontextlengtht g minimizersofdirichletenergyandspectralembeddings wecallthekthenergyminimizerofe theoptimalsolutionthatminimizese andisorthogonal g g tothefirstkenergyminimizers formallytheenergyminimizerscid zkcidn aredefinedasthe k preprint figurespectralembeddingofaringgraph figurespectralembeddingofagridgraph solutiontothefollowingproblem zk arg min e z g zsn st z zjj k wheresnistheunitsphereinndimensionaleuclideanspace theenergyminimizersareknown tohavethefollowingpropertiesspielman z cforsomeconstantc whichisadegeneratedsolutionthatassignsthesame valuetoeverynodeand cid cid ifweuse zz asthecoordinateofnodeiitwillbeagoodplanarembedding callthemdimensionalspectralembeddings spectralembeddingsareoftenusedtoadrawgraphonaplaneandinmanycasescanpreservethe structure graph tutte fig show spectral embedding result aringgraphandagridgraphrespectively noticehowsuchspectralembeddingsaresimilartothe representationsfromourmodelsinfigand asweshowintheorembthisisinfactexpected energy minimization hypothesis true representation h model minimize thedirichletenergyandarenondegeneratedthenthefirsttwoprincipalcomponentsofpcawill exactly produce spectral embeddings zz present informal version theoremanddeferthefullversionandprooftotheappendix theorem informal version theorem b let g graph h rnd n dbeamatrixthatminimizesdirichletenergyongwithnondegeneratedsingularvaluesthen thefirsttwoprincipalcomponentsofh willbezandz see app b formal version proof theorem see also tab empirical validationofthetheoremwhereinweshowtheprincipalcomponentsalignverywellwithspectral embeddingsofthegraph energyminimizationandgraphconnectivity giventherelationshipbetweenspectralembeddingsieenergyminimizersandtheprincipalcom ponentsobservedinourresultsfigs weclaimthatthemodelsinferenceoftheunderlying structureisakintoanimplicitenergyminimizationtofurtheranalyzetheimplicationofthisclaim weshowthatthemomentatwhichwecanvisualizeagraphusingpcaisthemomentatwhichthe model found large connected component ie graph structure specifically consider unconnected graph g ie ghas multiple connected component multiple de generatesolutionstotheenergyminimizationproblemwhichwillbefoundbypcaspecifically suppose ghas q connected component u denoting set node ith component preprint node b figure incontext emergence analyze incontext accuracy curve function contextsize inputted model graph used experiment mm grid varying value rule following accuracy graph tracing task accuracy show two phase ascent fit piecewise linear function observed ascent extract transi tionpointwhichmovesrightwardswithincreasinggraphsize binterestinglythetransitionpoint scalesasapowerlawinmiethenumberofnodesinthegraph thenwecanconstructthefirstqenergyminimizersasfollowsiqletthejthvalueofzibe j cid u zi k j othek rw ise cidq cid zi cidk cidjuk zj foriq k juk j easy check zi constructed q energy thus global minimizerofe moreoverallzisareorthogonaltoeachotherandhencesatisfyourdefinition g first q energy minimizers important notice zis q contain information structure graph identifying connected component theorem b tell u principal component nondegenerated rank solutionhthatminimizestheenergywillbezzsthusifthegraphisunconnectedthen theenergyminimizingrepresentationswillbedominatedbyinformationlessprincipalcomponents expect meaningful visualization acute reader may recall first minimizer z trivial solution energy minimization assigns value everynode convenientlytheaboveargumentalsoimpliesthatthisisnotaconcern pcawillrule outthisdegeneratesolutionasdemonstratedintheoremb incontextemergence ahypothesis ourresultsinfigshowedanintriguingbreakpointthat isreminiscentofasecondorderphasetransitionieanundefinedsecondderivative asshownin figweinfactfindthisbehaviorisextremelyrobustacrossgraphsofdifferentsizesandshowsa powerlawscalingtrendwithincreasinggraphsizeseeappcforseveralmoreresultsinthisvein includingdifferentgraphtopologies giventherelationshipofferedbetweenenergyminimization anddiscoveryofaconnectedcomponentgraphstructureinouranalysisaboveapossibleframe worktoexplaintheseresultsmaybetheproblemofbondpercolationonagraphnewman hooyberghs et al bondpercolation one start unconnected graph slowly fill edge connect node edge filled secondorder transition large connected component emerges graph nature transition observed experimentsfigandthetheoreticalconnectionbetweenenergyminimizationandexistenceofa connectedcomponentprovidesomeevidencetowardstheplausibilityofthishypothesis however webelievetheanalogyisstilllooseforourgraphsizesarerelativelysmalllikelycausingsignifi cantfinitesizeeffectsandtheexperimentsneedtocorroborateanyscalingtheoryofthetransition pointfrompercolationliteraturewouldrequirerunninggraphswithatleastordersofmagnitude differenceintheirsizes howevertheconsistencyofthehypothesiswithourempiricalresultsand analysisimpliesthatinvestigatingitfurthermaybefruitful preprint related work model representation researcher recently discovered numerous structured representa tions neural network mikolov et al suggests concept linearly represented activation park et al recently suggests may case contemporary language model numerous researcher found concrete example linear representation humanlevel concept including truthfulness burn et al li et al b mark tegmark refusal arditi et al toxicity lee et al sycophancy rimsky etalandevenworldmodelslietalnandaetal parketalcfinds thathierarchicalconceptsarerepresentedwithatreelikestructureconsistingoforthogonalvectors relevant line work includes todd et al hendel et al paper findthatonecancomputeavectorfromincontextexemplarsthatencodethetasksuchthatadding suchavectorduringtesttimeforanewinputcancorrectlysolvethetask languagemodelsdonot always form linear representation however engels et al find circular feature representa tionsforperiodicconceptssuchasdaysoftheweekormonthsoftheyearusingacombinationof sparseautoencodersandpcacsordasetalfindsthatrecurrentneuralnetworkstrainedon tokenrepetitioncaneitherlearnanonionlikerepresentationoralinearrepresentationdepending onthemodelswidth unlikesuchpriorworkwefindthattaskspecificrepresentationswithade siredstructuralpatterncanbeinducedincontext toourknowledgeourworkoffersthefirstsuch investigationofincontextrepresentationlearning scaling incontext learning numerous work demonstrated incontext accuracy im proveswithmoreexemplarsbrownetalluetalbigelowetal withlonger contextlengthsbecomingavailableresearchershavebeguntostudytheeffectofmanyshotprompt ingasopposedtofewshotagarwaletalaniletallietalc forinstance agarwaletalreportsimprovedperformanceoniclusinghundredstothousandsofexem plars wide range task similarly anil et al demonstrate ability jailbreak llm scaling number exemplar unlike work evaluates model behavior study effect scaling context underlying representation provide framework predictingwhendiscontinuouschangesinbehaviorcanbeexpectedviamerecontextscaling syntheticdataforinterpretability recentworkshavedemonstratedthevalueofinterpretable syntheticdatageneratingprocessesforunderstandingtransformersbehaviorincludingincontext learningparketalarameshetalgargetallanguageacquisitionlubana etalqinetalallenzhulibfinetuningjainetallubanaetal junejaetalreasoningabilitiesprystawskietalkhonaetalwenetal liuetalaandknowledgerepresentationsnishietalallenzhulia whilepriorworktypicallypretrainstransformersonsyntheticdataweleveragesyntheticdatato studyrepresentationformationduringincontextlearninginpretrainedlargelanguagemodels discussion work show llm flexibly manipulate representation semanatics internalized based pretraining data semantics defined entirely incontext arrive result propose simple rich task graph tracing wherein trace random walk graph shown model incontext graph instantiated using predefined structure eg lattice concept semantically interesting eg define node meaning less overall context problem interestingly find ability flexibly manipulate representationsisinfactemergentwithrespecttocontextsizeweproposeamodelbasedonen ergyminimizationtohypothesizeamechanismfortheunderlyingdynamicsofthisbehavior resultssuggestcontextscalingcanunlocknewcapabilities morebroadly thisaxismayhave yet underappreciated improving model fact note knowledge work first investigate formation representation entirely incontext study also naturally motivates future work towards formation world representation li et al world model ha schmidhuber incontext significant implication toward building general openended system well forecasting safety concern alsohighlighttherelationofourexperimentalsetuptosimilartasksstudiedinneurosciencelitera turegarvertetalmarketalwhereinhumansareshownrandomwalksofa graphofvisualconceptsfmriimagesofthesesubjectsdemonstratetheformationofastructured representationofthegraphinthehippocampalentorhinalcortexsimilartoourresultswithllms preprint limitation wedoemphasizethatourworkhasafewlimitations namelypcaormorebroadly low dimensional visualization high dimensional data difficult interpret sometimes evenmisleading despitesuchdifficultiesweprovidetheoreticalconnectionsbetweenenergymin imizationandprincipalcomponentstoprovideacompellingexplanationforwhystructureselicited via pca faithfully represent incontext graph structure second find strong never thelessincompletecausalrelationshipbetweentherepresentationsfoundbypcaandthemodels prediction view exact understanding representation form exact relationship representation model prediction interesting future direction especiallygiventhatsuchunderlyingmechanismseemstodependonthescaleofthecontext acknowledgment wegreatlythankthenationaldeepinferencefabricndifpilotprogramfiottokaufmanetal especiallyemmabortzjadenfiottokaufmanadambelfkidavidbauandtheteamwho provideduswithaccesstorepresentationsofllamamodelsmakingthisworkpossiblecfpandht acknowledge support aravinthan dt samuel cecilia garraffo douglas p finkbeiner cfp esl kn mo ht supported cbsntt program physic intelligence mw acknowledges support superalignment fast grant openai effective venture foundationeffektivspendenschweizandtheopenphilanthropyprojectpartofthecomputations paper run fasrc cluster supported fa division science research computing group harvard university esl thanks eric bigelow crucial discussion helped define several hypothesis pursued work harvard cocodev lab especially peng qian talia konkle feedback earlier version paper cfp thanks zechen zhang eric todd clement duma shivam raval useful discussion author thank davidbauandhislabforusefulfeedbackonthepapersresults author contribution cfpaleslandhtconceivedtheincontextgraphtraversaltaskinspiredbydiscussionsandex perimentationswithmoandkninarelatedworkonpretraining cfpandalcoledexperiments wherecfpdiscoveredtheincontextlearningoftheringstructuredrepresentationwithinputfrom ht kicking study al suggested extending grid structure cfp connecting world representation esl proposed hexagonal configuration esl hypothesized incontext transitionswithincreasedcontextandpercolationmechanism cfpconductedsemanticoverriding percolation experiment al performed accuracyenergy experiment causal inter ventions cfp al esl developing transition point detection method contributing toenergynormalization yyformulatedtheenergyminimizationtheoryanditsproofscollaborat ingwithesltoconnecttheframeworktocomponentexistenceanddemonstratepcasoptimality mwprovidedalternativehypothesesforexperimentalrobustness cfpaleslyyandhtco developed initial manuscript feedback mw esl leading final writing projectnarrativedevelopment cfpalandhtcreatedfigureswhilecfpandaljointlydevel oped appendix al conducted substantial experiment verify main claim generalize othermodelswithcfpssupport htsupervisedtheproject preprint reference mostafa abdou artur kulmizev daniel hershcovich stella frank ellie pavlick anders sgaard language model encode perceptual structure without grounding case study incolor arxivpreprintarxiv rishabh agarwal avi singh lei zhang bernd bohnet luis rosias stephanie chan biao zhang ankesh anand zaheer abbas azade nova john coreyes eric chu feryal hbahani aleksandra faust hugo larochelle manyshot incontext learning url httpsarxivorgabs ekin akyurek dale schuurmans jacob andreas tengyu denny zhou learning algorithm incontext learning investigation linear model url http arxivorgabs zeyuan allenzhu yuanzhi li physic language model part knowledge storage extraction arxivpreprintarxiva zeyuanallenzhuandyuanzhili physicsoflanguagemodels part learninghierarchicallan guagestructures arxiveprintsabsmayb cem anil esin durmus mrinank sharma joe benton sandipan kundu joshua batson nina rimskymegtongjessemudanielfordetalmanyshotjailbreakinganthropicapril anthropic ai scaling monosemanticity extracting interpretable feature claude sonnet httpstransformercircuitspub scalingmonosemanticityindexhtml andy arditi oscar obeso aaquib syed daniel paleka nina panickssery wes gurnee neel nanda refusal language model mediated single direction arxiv preprint arxiv eric j bigelow ekdeep singh lubana robert p dick hidenori tanaka tomer ullman incontextlearningdynamicswithrandombinarysequences arxivpreprintarxiv nedblock semanticsconceptualrole routledgeencyclopediaofphilosophy timothyfbrady taliakonkle andgeorgea alvarez compressioninvisualworking memory using statistical regularity form efficient memory representation journal experi mentalpsychology general tom brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhari wal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agar wal ariel herbertvoss gretchen krueger tom henighan rewon child aditya ramesh daniel ziegler jeffrey wu clemens winter chris hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever dario amodei language model fewshot learner h larochelle ranzato r hadsell mf balcan h lin ed advance neu ral information processing system volume pp curran associate inc url httpsproceedingsneuripsccpaperfilespaper filecdbfcbbfbacfapaperpdf collinburns haotianye danklein andjacobsteinhardt discoveringlatentknowledgeinlan guagemodelswithoutsupervision arxivpreprintarxiv robert csordas christopher potts christopher manning atticus geiger recurrent neural networkslearntostoreandgeneratesequencesusingnonlinearrepresentations arxivpreprint arxiv abhimanyudubeyabhinavjauhriabhinavpandeyandetal thellamaherdofmodels urlhttpsarxivorgabs preprint joshuaengelsisaacliaoericjmichaudwesgurneeandmaxtegmarknotalllanguagemodel featuresarelinear urlhttpsarxivorgabs kyfan onatheoremofweylconcerningeigenvaluesoflineartransformationsi proceedingsof thenationalacademyofsciences jaden fiottokaufman alexander r loftus eric todd jannik brinkmann caden juang koyena palcanrageraaronmuellersamuelmarksarnabsensharmafrancescalucchettimichael ripaadambelfkinikhilprakashsumeetmultanicarlabrodleyarjunguhajonathanbell byron wallace david bau nnsight ndif democratizing access foundation model internals urlhttpsarxivorgabs shivam garg dimitris tsipras percy liang gregory valiant transformer learn incontext acasestudyofsimplefunctionclasses urlhttpsarxivorgabs monamgarvertraymondjdolanandtimothyejbehrens amapofabstractrelationalknowl edgeinthehumanhippocampalentorhinalcortex elifee gemmateamgemmaimprovingopenlanguagemodelsatapracticalsizeurlhttps arxivorgabs pulkitgopalaniekdeepsinghlubanaandweihu abruptlearningintransformers acasestudy onmatrixcompletion arxivpreprintarxiv wes gurnee max tegmark language model represent space time arxiv preprint arxiv davidhaandjurgenschmidhuber worldmodels arxivpreprintarxiv gilbertharman conceptualrolesemantics notredamejournalofformallogic roee hendel mor geva amir globerson incontext learning creates task vector houda bouamor juan pino kalika bali ed finding association compu tational linguistics emnlp pp singapore december association computational linguistics doi vfindingsemnlp url http aclanthologyorgfindingsemnlp h hooyberghs b van schaeybroeck j indekeu percolation bipartite scalefree net work physicaastatisticalmechanicsanditsapplicationsaugust issn samyakjainrobertkirkekdeepsinghlubanarobertpdickhidenoritanakaedwardgrefen stettetimrocktaschelanddavidscottkrueger mechanisticallyanalyzingtheeffectsoffine tuningonprocedurallydefinedtasks arxivpreprintarxiv erikjennershreyaskapurvasilgeorgievcameronallenscottemmonsandstuartrussellevi denceoflearnedlookaheadinachessplayingneuralnetworkarxivpreprintarxiv jeeveshjunejarachitbansalkyunghyunchojoaosedocandnaomisaphralinearconnectivity revealsgeneralizationstrategies arxivpreprintarxiv mikail khona maya okawa jan hula rahul ramesh kento nishi robert dick ekdeep singh lubanaandhidenoritanaka towardsanunderstandingofstepwiseinferenceintransformers asyntheticgraphnavigationmodel arxivpreprintarxiv michaellaskinluyuwangjunhyukohemilioparisottostephenspencerrichiesteigerwald djstrousestevenhansenangelosfilosethanbrooksetalincontextreinforcementlearning withalgorithmdistillation arxivpreprintarxiv andrewleexiaoyanbaiitamarpresmartinwattenbergjonathankkummerfeldandradami halcea amechanisticunderstandingofalignmentalgorithms acasestudyondpoandtoxicity arxivpreprintarxiva preprint jonathanleeanniexiealdopacchianoyashchandakchelseafinnofirnachumandemma brunskillsupervisedpretrainingcanlearnincontextreinforcementlearningadvancesinneural informationprocessingsystemsb belinda z li maxwell nye jacob andreas implicit representation meaning neural languagemodels arxivpreprintarxiv kennethliaspenkhopkinsdavidbaufernandaviegashanspeterpfisterandmartinwatten berg emergentworldrepresentations exploringasequencemodeltrainedonasynthetictask intheeleventhinternationalconferenceonlearningrepresentations kenneth li aspen k hopkins david bau fernanda viegas hanspeter pfister martin wat tenberg emergent world representation exploring sequence model trained synthetic task eleventh international conference learning representation url httpsopenreviewnetforumiddegtczvt kennethlioampatelfernandaviegashanspeterpfisterandmartinwattenberg inferencetime intervention elicitingtruthfulanswersfromalanguagemodel inthirtyseventhconferenceon neuralinformationprocessingsystemsburlhttpsopenreviewnetforum idalluypny mukailishansangongjiangtaofengyihengxujunzhangzhiyongwuandlingpengkong incontextlearningwithmanydemonstrationexamplesarxivpreprintarxivc bingbin liu jordan ash surbhi goel akshay krishnamurthy cyril zhang transformer learnshortcutstoautomata arxivpreprintarxiva zimingliuouailkitouniniklassnolteericmichaudmaxtegmarkandmikewilliams wardsunderstandinggrokking aneffectivetheoryofrepresentationlearning advancesinneu ralinformationprocessingsystemsb yaolumaxbartoloalastairmooresebastianriedelandpontusstenetorpfantasticallyordered prompt find overcoming fewshot prompt order sensitivity smaranda muresan preslav nakov aline villavicencio ed proceeding th annual meet ing association computational linguistics volume long paper pp dublinirelandmayassociationforcomputationallinguistics doi v acllong urlhttpsaclanthologyorgacllong ekdeepsinghlubanaericjbigelowrobertpdickdavidkruegerandhidenoritanaka mech anisticmodeconnectivity ininternationalconferenceonmachinelearningpp pmlr ekdeep singh lubana kyogo kawaguchi robert p dick hidenori tanaka percolation model emergence analyzing transformer trained formal language arxiv preprint arxiv shirleymarkranimoranthomasparrstevewkennerleyandtimothyejbehrenstransferring structuralknowledgeacrosscognitivemapsinhumansandmodels naturecommunications shirley mark phillipp schwartenbeck avital hahamy veronika samborska alon b baram timothyebehrensflexibleneuralrepresentationsofabstractstructuralknowledgeinthehuman entorhinalcortex elife samuelmarksandmaxtegmarkthegeometryoftruthemergentlinearstructureinlargelanguage modelrepresentationsoftruefalsedatasets urlhttpsarxivorgabs tomasmikolovkaichengregcorradoandjeffreydean efficientestimationofwordrepresen tationsinvectorspace urlhttpsarxivorgabs neelnandaandrewleeandmartinwattenberg emergentlinearrepresentationsinworldmodels ofselfsupervisedsequencemodels arxivpreprintarxiv preprint mejnewmanthestructureandfunctionofcomplexnetworkssiamreview january issn kento nishi maya okawa rahul ramesh mikail khona hidenori lubana tanaka ekdeep singh representation shattering transformer synthetic study knowledge editing arxivpreprintarxiv corefranciscoparkekdeepsinghlubanaitamarpresandhidenoritanakacompetitiondynam icsshapealgorithmicphasesofincontextlearning arxivpreprintarxiva core francisco park maya okawa andrew lee ekdeep singh lubana hidenori tanaka emergenceof hiddencapabilities exploringlearningdynamics inconceptspace b url httpsarxivorgabs kihoparkyojoongchoeyibojiangandvictorveitch thegeometryofcategoricalandhierar chicalconceptsinlargelanguagemodels arxivpreprintarxivc kihoparkyojoongchoeandvictorveitchthelinearrepresentationhypothesisandthegeometry oflargelanguagemodelsd urlhttpsarxivorgabs romapatelandelliepavlick mappinglanguagemodelstogroundedconceptualspaces ininter nationalconferenceonlearningrepresentations jeffreypenningtonrichardsocherandchristopherdmanning glove globalvectorsforword representation inproceedingsoftheconferenceonempiricalmethodsinnaturallanguage processingemnlppp benprystawskimichaelliandnoahgoodman whythinkstepbystep reasoningemergesfrom thelocalityofexperience advancesinneuralinformationprocessingsystems tian qin naomi saphra david alvarezmelis sometimes tree data drive unstable hierarchicalgeneralization arxivpreprintarxiv rahulrameshekdeepsinghlubanamikailkhonarobertpdickandhidenoritanaka com positional capability autoregressive transformer study synthetic interpretable task arxivpreprintarxiv nina rimsky nick gabrieli julian schulz meg tong evan hubinger alexander turner steering llama via contrastive activation addition lunwei ku andre martin vivek srikumar ed proceeding nd annual meeting association com putational linguistics volume long paper pp bangkok thailand august association computational linguistics doi vacllong url httpsaclanthologyorgacllong adamsshaisarahemarzenlucasteixeiraalexandergietelinkoldenzielandpaulmriech er transformer represent belief state geometry residual stream arxiv preprint arxiv danielspielman spectralandalgebraicgraphtheory yalelecturenotesdraftofdecember aarohi srivastava abhinav rastogi abhishek rao abu awal md shoeb abubakar abid adam fisch adam r brown adam santoro aditya gupta adria garrigaalonso et al beyond imitationgamequantifyingandextrapolatingthecapabilitiesoflanguagemodelsarxivpreprint arxiv eric todd millicent l li arnab sen sharma aaron mueller byron c wallace david bau functionvectorsinlargelanguagemodels arxivpreprintarxiv aaron traylor roman feiman ellie pavlick neural network learn implicit logic physicalreasoning intheeleventhinternationalconferenceonlearningrepresentations williamthomastutte howtodrawagraph proceedingsofthelondonmathematicalsociety preprint keyonvafajustinychenjonkleinbergsendhilmullainathanandasheshrambachan evalu atingtheworldmodelimplicitinagenerativemodel arxivpreprintarxiv johannes von oswald eyvind niklasson ettore randazzo joao sacramento alexander mordv intsev andrey zhmoginov max vladymyrov transformer learn incontext gradient descent ininternationalconferenceonmachinelearningpppmlra johannes von oswald maximilian schlegel alexander meulemans seijin kobayashi eyvind niklassonnicolaszucchetninoscherrernolanmillermarksandlermaxvladymyrovetal uncovering mesaoptimization algorithm transformer arxiv preprint arxiv b jason wei yi tay rishi bommasani colin raffel barret zoph sebastian borgeaud dani yo gatamamaartenbosmadennyzhoudonaldmetzleretalemergentabilitiesoflargelanguage model arxivpreprintarxiv kaiyuewenyuchenlibingbinliuandandrejristeski transformersareuninterpretablewith myopic method case study bounded dyck grammar advance neural information processingsystems jamescrwhittingtontimothyhmullershirleymarkguifenchencaswellbarryneilburgess andtimothyejbehrens thetolmaneichenbaummachine unifyingspaceandrelationalmem orythroughgeneralizationinthehippocampalformation cell yongyi yang david p wipf et al transformer optimization perspective advance neuralinformationprocessingsystems preprint additional experimental detail hereweprovidesomeadditionaldetailsregardingourexperimentalsetups contextwindows ouranalysesrequirecomputingmeantokenrepresentationsh foreverytoken graph grab activation per token recent context windowofn token becausewefurtherrequirethateachtokenisobservedatleastonceinour w window use batch prompt batch size equal number node graph foreachpromptinthebatchwestartourrandomtraversalorrandompairwisesampling different node ensuring node show least context case whenourcontextlengthn islongerthanthewindowwesimplyuseeverytokenn n c w c computationalresources werunourexperimentsoneitheranodesorbyusingtheapis providedbyndiffiottokaufmanetal b connection energy minimization pca stucture inthissectionforamatrixm rndweuselowercaseboldletterswithsubscripttorepresentthe columnsformeg representsthekthcolumnofm moreoverweuse mtorepresent k k kth largest singular value psd use represent kth k largesteigenvalueofm moreover weusee torepresentavectorwithallzeroentriesexcepta k atentrykwhosedimensionisinferredfromcontextandtorepresentavectorwithallentries foranaturalnumbernweusentorepresent n furthermoreweusecid zkcidn torepresenttheenergyminimizersofthedirichletenergydefined k insection leta rnn betheadjacencymatrixofthegraph diagabethedegree matrixandl dabethelaplacianmatrix throughaneasycalculationonecanknowthat foranyvectorxrn e xxlx g thereforefromthespectraltheoremeg theoreminspielmanweknowthatz k theeigenvectoroflcorrespondingto le z nk g k wewillshowthatifamatrixh rndminimizestheenergyandisnondegeneratedhasseveral distinctandnonzerosingularvaluesthenthepcamustexactlygivetheleadingenergyminimiz ersstartingfromz theoremb letgbeagraphand besminnddistinctpositive number letmatrixh rndbethesolutionofthefollowingoptimizationproblem h arg min e x g xrnd st x k r k k thenthekthprinciplecomponentofh fork risbez k proof wefirstprovethattheleadingleftsingularvectorsofh areexactlyenergyminimizers let r minnd let svd h h uv diag singularvaluesofhandu rnrv rrd preprint leth representstheithrowofh noticethat e ghcid ijcid cidh ih jcid cid ij cid cid cide e hcid cid ijcid j cid ij cid cid cide e ucid cid ijcid j cid ij r cidcid e e u k j k ij k r cid e u k g k k since u independent matter value u know k k k k take smallest possible value given condition k k k k r k since u singular vector u orthogonal using theorem k k fanweknowthatforanys ntheminimizerofcids e u isu z k k g k k k therefore evident minimizer cid e u must satisfies u z k k k g k k k sincefromtheaboveargumentof sandthegivenconditionconditionweknowthat k nowwehaveprovedthatu z k nextweconsidertheoutputofpcaletp bethe k k k kthprinciplecomponentoutputbythepcaofh weknowthatp istheeigenvectorof k c hcidhcid thatcorrespondstothekthlargesteigenvalueofcwherehcid h h isthecentralized n h fromthespectraltheoremwehave p arg max pcp k psn ppiik letj spanbethesetofvectorswhoseeveryentryhasthesamevalueletjbethesubspace inrnthatisorthogonaltojforasubspacekofrnlet rn rnbetheprojectionoperator k ontok wehavethat p arg max pcp psn cid cid cid cid cid cid arg max p hh p psn n n arg max cid phh pcid j j psn arg max cid phhpcid psn pj whichagainfromspectraltheoremistheeigenvectorofthesecondlargesteigenvalueofhh whichisu z usinganinductionandthesamereasoningitfollowsthatforanyk swe havep z thisprovestheproposition k k preprint c additional result c detailedlayerwisevisualizationofrepresentations infigureandfigureweprovideadditionalvisualizationsperlayerforeachofourmodelsand eachofourdatageneratingprocesses layer layer layer layer layer layer layer layer layer layer layer layer layer layer layer layer figure weplotdpcaprojectionsfromeveryotherlayerinllamabdubeyetal giventhegridtraversaltask indeeperlayerswecanseeaclearvisualizationofthegrid preprint layer layer layer layer layer layer layer layer layer layer layer layer layer layer layer layer figure weplotdpcaprojectionsfromeveryotherlayerinllamabdubeyetal forthehexagonalgridtask preprint c pcadirichletenergyandaccuracyresultsonothermodels hereweprovideresultsfromotherlanguagemodelsiellamabdubeyetalllama binstruct gemmab gemma team gemmab figure plot pca projection last layer various model various data generating process figure plot normalized dirichlet energy curve accuracy various language modelsonvarioustasks acrossallmodelsandtasksweseeresultssimilartothemainpaper llamab grid llamabinstruct grid gemmab grid gemmab grid llamab ring llamabinstruct ring gemmab ring gemmab ring llamab hex llamabinstruct hex gemmab hex gemmab hex figure weplotdpcaprojectionsfromthelastlayerofvariouslanguagemodelsgivenvarious datageneratingprocesses forthegridandhexagonalgraphsweapplypcaonthelastlayers ring visualize layer respectively interestingly llamab findtheringrepresentationinthendandrdprincipalcomponents c standardizeddirichletenergy fig report dirichlet energy value computed standardization representation ie aftermeancenteringthemandnormalizingbythestandarddeviation thisrendersthetrivial solution dirichlet energy minimization infeasible since assigning constant representation node yield infinite energy due zero variance seen result plot arequalitativelysimilartothenonstandardizedenergyresultsfigbutmorenoisyespecially ring graph expected since standardization exacerbate influence noise yieldingfluctuationsintheenergycalculation preprint context length ygrene telhcirid dezilamron layer layer accuracy llamab grid context length ygrene telhcirid dezilamron layer layer accuracy llamab grid context length ygrene telhcirid dezilamron layer layer accuracy llamabinstruct grid context length ygrene telhcirid dezilamron layer layer accuracy gemmab grid context length ygrene telhcirid dezilamron layer layer accuracy gemmab grid context length ygrene telhcirid dezilamron llamab ring context length ygrene telhcirid dezilamron llamab ring context length ygrene telhcirid dezilamron llamabinstruct ring context length ygrene telhcirid dezilamron gemmab ring context length ygrene telhcirid dezilamron gemmab ring context length ygrene telhcirid dezilamron llamab hex context length ygrene telhcirid dezilamron llamab hex context length ygrene telhcirid dezilamron llamabinstruct hex context length ygrene telhcirid dezilamron gemmab hex context length ygrene telhcirid dezilamron gemmab hex ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca figure accuracy versus normalized dirichlet energy curve various language model various task every model task see energy minimized accuracy starting improve context length ygrene telhcirid dezilamron layer layer accuracy llamab grid context length ygrene telhcirid dezilamron layer layer accuracy llamab grid context length ygrene telhcirid dezilamron layer layer accuracy llamabinstruct grid context length ygrene telhcirid dezilamron layer layer accuracy gemmab grid context length ygrene telhcirid dezilamron layer layer accuracy gemmab grid context length ygrene telhcirid dezilamron llamab ring context length ygrene telhcirid dezilamron llamab ring context length ygrene telhcirid dezilamron llamabinstruct ring context length ygrene telhcirid dezilamron gemmab ring context length ygrene telhcirid dezilamron gemmab ring context length ygrene telhcirid dezilamron llamab hex context length ygrene telhcirid dezilamron llamab hex context length ygrene telhcirid dezilamron llamabinstruct hex context length ygrene telhcirid dezilamron gemmab hex context length ygrene telhcirid dezilamron gemmab hex ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca ycarucca figure accuracy versus zero mean centered normalized dirichlet energy curve various languagemodelsonvarioustasks zeromeancenteringensuresthatgraphrepresentationsarenot using trivial solution energy minimization ie assigning representation every node preprint c causalanalysisofrepresentations inthissectionwereportpreliminarycausalanalysesofourgraphrepresentationswhilefullyunder standing mechanism behind formation representation well relationship said representation model output interesting future direction focusofourworkandthusweonlyranproofofconceptexperiments withthatsaidweask dotheprincipalcomponentsthatencodeourgraphrepresentationshaveany causalroleinthemodelspredictions totestthisweattempttomovethelocationoftheactivationsforonenodeofthegraphtoanother bysimplyrescalingitsprincipalcomponents namelyassumeactivationhcorrespondingtonode layer say wish move activation different target node j first compute mean representation node j using activation corresponding node j within recent n timesteps notated h assuming first two principal component encode w j thecoordinatesofthenode wesimplyrescaletheprincipalcomponentsofh tomatchthatof h j weviewthisapproachasratherrudimentary namelytherearelikelymoreinformativevectorsthat encodericherinformationsuchasinformationaboutneighboringnodes howeverwedofindthat thefirsttwoprincipalcomponentshavesomecausalroleinthemodelspredictions wetestourrescalinginterventiononrandomlygeneratedcontexts foreachcontextassum ingourunderlyinggraphhasnnodeswetestmovingtheactivationsofthelasttokenitoalln otherlocationsinthegraph wethenreporttheaveragedmetricacrosstheresultingn testcases wereportmetricsaccuracyhithitandaccumulatedprobabilitymassonvalidtokens hit hit report percentage time top top predicted token valid neighbor target node j accumulated probability mass simply sum probabilitymassallocatedtoallneighborsievalidpredictionsofthetargetnodej tablereportsourresultsforourringandgridtasks weincluderesultsforrescalingwithor principalcomponentsaswellasnullinterventionsandinterventionswitharandomvector overall wefindthattheprincipalcomponentshavesomecausaleffectonthemodelsoutputpredictionsbut doesnotprovideafullexplanation ring grid hex hit hit prob hit hit prob hit hit prob interv n interv n nullinterv randominterv table interventionresultsforourringandgridtasks wedemonstratethatoftentimessimply rescalingtheprincipalcomponentforeachtokenrepresentationcanmovethetokentoadifferent position graph however note simple rescaling approach perfectly captureacausalrelationshipbetweenprincipalcomponentsandmodelpredictions c empiricalsimilarityofprincipalcomponentsandspectralembeddings theorempredictsthatifthemodelrepresentationsareminimizingthedirichletenergythefirst twoprincipalcomponentswillbeequivalenttothespectralembeddingszz empirically measure whether first two principal component indeed equivalent thespectralembeddings intablewemeasurethecosinesimilarityscoresbetweentheprincipal componentsandspectralembeddings c accuracyofincontexttaskswithaconflictingsemanticprior whatwouldhappenwhenanincontexttaskwhichcontradictsasemanticpriorisgiventoamodel namelyengelsetalshowthatwordslikedaysoftheweekhaveacircularrepresentation preprint cospcz cospcz grid ring hex table absolute value cosine distance principal component model activation spectralembeddings weempiricallyobservethatinpracticethesecoordinatesendupbeingvery similar grid hexagon use principal component last layer ringweuseanearlierlayerlayerinwhichtheringisobserved experiment randomly shuffle token day week ie token mon tue wed thu fri sat sun define new ring give random neighboring pair thenewlydefinedringasourincontexttask figuredemonstratestheaccuracywhengivenanincontexttaskthatiscontradictorytoasemantic prior interestingly wefirstobservethemodelmakepredictionsthatreflectstheoriginalsemantic prior pink accuracy drop quickly model capture semantic rule beingfollowed withmoreexemplarsweseeaslowdecayoftheremainingsemanticaccuracyand transition model behavior begin make prediction reflect newly defined orderingofourringblue number example ycarucca shuffled semantic figure incontextstructureoverridessemanticprior givenanincontexttaskthatcontra dictsamodelssemanticpriorweobservethemodeltransitionfrommakingpredictionsthatadhere tothesemanticpriorpinktopredictionsthatreflectthenewlydefinedincontexttask furthermore fig quantify dirichlet energy computed certain pc dimen sion wefindthatenergyminimizationhappensinthedimensionscorrespondingtotheincontext structure context length ygrene total pca pca figure energyminimizationhappensintheincontextcomponentdimensions weshow dirichlet energy depending context given taking semantic pca incontext pca dimension show energy minimization happens pca corre spondingtotheincontextdimensions preprint c additionalempiricalverificationsoftransitionpredictions hereweprovideadditionaldetailsforempiricallyverifyingourpredictionsformodeltransitions figuresanddemonstratedetailedaccuracycurvesforawiderangeofgraphsizes figureemergentbehaviorforvaryingtaskcomplexitygraphsizeforthehexagonaltask plot accuracy varying level complexity graph size hexagonal incontext task interestinglyregardlessofgraphsizeweseeanabruptdiscontinuouschangeinthemodels performance figuredemonstratesthatwecanpredictwhensuchabruptchangecanbeexpected asafunctionoftaskcomplexity example index robhgienp graph size llama icl raw llama icl smoothed piecewise linear fit transition point example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size figure emergentbehaviorforvaryingtaskcomplexitygraphsizeforthegridtask plottheaccuracyforvaryinglevelsofcomplexitygraphsizeforthegridincontexttask interest inglyregardlessofgraphsizeweseeanabruptdiscontinuouschangeinthemodelsperformance figuredemonstratesthatwecanpredictwhensuchabruptchangescanbeexpectedasafunction oftaskcomplexity preprint example index robhgienp graph size llama icl raw llama icl smoothed piecewise linear fit transition point example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size example index robhgienp graph size figure emergent behavior varying task complexity graph size ring task plot accuracy varying level complexity graph size ring incontext task interestinglyregardlessofgraphsizeweagainseeanabruptdiscontinuouschangeinthemodels performance node b figure incontextemergenceinahexagonalgraphtracingtask weanalyzetheincontext accuracycurvesasafunctionofcontextsizeinputtedtothemodel thegraphusedinthisexper iment mm grid varying value rule following accuracy graph tracing task accuracy show two phase ascent fit piecewise linear function ob servedascenttoextractthetransitionpointwhichmovesrightwardswithincreasinggraphsize b interestinglythetransitionpointscalesasapowerlawinmiethenumberofnodesinthegraph node b node c node node figure hexagonal graph tracing accuracy compared memorization solution rulefollowingaccuraciesonthehexagonalgraphcomparedtothememorizationmodelinsec hexagonalgraphwithabcdnodes generallywefindthatthehexagonalgraph trackingaccuracyfromllamabdubeyetalislowerthantheshotmemorization modelindicatingthattheremightbeadifferentunderlyingprocess"]}