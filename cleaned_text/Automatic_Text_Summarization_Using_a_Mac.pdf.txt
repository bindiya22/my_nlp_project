automatic text summarization using machine learning approach joel larocca neto alex freitas celso kaestner pontifical catholic university parana pucpr rua imaculada conceicao curitiba pr brazil joel alex kaestnerppgiapucprbr httpwwwppgiapucprbralex abstract paper address automatic summarization task recent research work extractivesummary generation employ heuristic work indicate select relevant feature present summarization procedure based application trainable machine learning algorithm employ set feature extracted directly original text feature two kind statistical based frequency element text linguistic extracted simplified argumentative structure text also present computational result obtained application summarizer well known text database compare result baseline summarization procedure introduction automatic text processing research field currently extremely active one important task field automatic summarization consists reducing size text preserving information content summarizer system produce condensed representation input user consumption summary construction general complex task ideally would involve deep natural language processing capacity order simplify problem current research focused extractivesummary generation extractive summary simply subset sentence original text summary guarantee good narrative coherence conveniently represent approximate content text relevance judgement summary employed indicative way pointer part original document informative way cover relevant information text case important advantage using summary reduced reading time summary generation automatic procedure also advantage size summary controlled ii content determinist iii link text element summary position original text easily establishedin work deal automatic trainable summarization procedure based application machine learning technique project involving extractive summary generation shown success task depends strongly use heuristic unfortunately indicative given choose relevant feature task employ statistical linguistic feature extracted directly automatically original text rest paper organized follows section present brief review text summarization task section describe detail proposal discussing employed set feature general framework trainable summarizer section relate computational result obtained application proposal reference document collection finally section present conclusion outline envisaged research work review text summarization automatic summarization process divided three step preprocessing step structured representation original text obtained processing step algorithm must transform text structure summary structure generation step final summary obtained summary structure method summarization classified term level linguistic space two broad group shallow approach restricted syntactic level representation try extract salient part text convenient way b deeper approach assume semantics level representation original text involve linguistic processing level first approach aim preprocessing step reduce dimensionality representation space normally includes stopword elimination common word semantics aggregate relevant information task eg eliminated ii case folding consists converting character kind letter case either upper case lower case iii stemming syntacticallysimilar word plural verbal variation etc considered similar purpose procedure obtain stem radix word emphasize semantics frequently employed text model vectorial model preprocessing step text element sentence case text summarization considered ndimensional vector possible use metric space measure similarity text element employed metric cosine measure defined co xy x vector x indicates scalar product x indicates module x therefore maximum similarity corresponds co whereas co indicates total discrepancy text element evaluation quality generated summary key point summarization research detailed evaluation summarizers made tipster text summarization evaluation conference summac part effort standardize summarization test procedure case reference summary collection provided human judge allowing direct comparison performance system participated conference human effort toelaborate summary however huge another reported problem even case human judge low concordance according mitra importantly summary produced human judge different date agreement idea reference summary important consider existence objectively evaluate performance automatic summary generation procedure using classical information retrieval ir precision recall measure case sentence called correct belongs reference summary usual precision ratio number selected correct sentence total number selected sentence recall ratio number selected correct sentence total number correct sentence case fixedlength summary two measure identical since size reference automatically obtained extractive summary identical mani bloedorn proposed automatic procedure generate reference summary original text contains authorprovided summary corresponding sizek reference extractive summary consists k similar sentence authorprovided summary according cosine measure using approach easy obtain reference summary even big document collection machine learning ml approach envisaged collection document corresponding reference extractive summary trainable summarizer obtained application classical trainable machine learning algorithm collection document summary case sentence document modeled vector feature extracted text summarization task seen twoclass classification problem sentence labeled correct belongs extractive reference summary incorrect otherwise trainable summarizer expected learn pattern lead summary identifying relevant feature value correlated class correct incorrect new document given system learned pattern used classify sentence document either correct incorrect sentence producing extractive summary crucial issue framework obtain relevant set feature next section treat point detail trainable summarizer using ml approach concentrate presentation two main point set employed feature framework defined trainable summarizer including employed classifier large variety feature found textsummarization literature proposal employ following set feature meantfisf since seminal work luhn text processing task frequently use feature based ir measure context ir important measure term frequency tf term frequency inverse document frequency tfidf text summarization employ idea case single document select set relevant sentence included extractive summary sentence hencethe notion collection document ir replaced notion single document text summarization analogously notion document element collection document ir corresponds notion sentence element document summarization new measure called term frequency inverse sentence frequency denoted tfisfws final used feature calculated mean value tfisf measure word sentence b sentence length feature employed penalize sentence short since sentence expected belong summary use normalized length sentence ratio number word occurring sentence number word occurring longest sentence document c sentence position feature involve several item position sentence document whole position section paragraph etc presented good result several research project use percentile sentence position document proposed nevillmanning final value normalized take value similarity title according vectorial model feature obtained using title document query sentence document similarity document title sentence computed cosine similarity measure e similarity keywords feature obtained analogously previous one considering similarity set keywords document sentence compose document according cosine similarity next two feature employ concept text cohesion basic principle sentence higher degree cohesion relevant selected included summary f sentencetosentence cohesion feature obtained follows sentence first compute similarity sentence document add similarity value obtaining raw value feature process repeated sentence normalized value range feature sentence obtained computing ratio raw feature value largest raw feature value among sentence document value closer indicate sentence larger cohesion g sentencetocentroid cohesion feature obtained sentence follows first compute vector representing centroid document arithmetic average corresponding coordinate value sentence document compute similarity centroid sentence obtaining raw value feature sentence normalized value range obtained computing ratio raw feature value largest raw feature value among sentence document sentence feature value closer larger degree cohesion respect centroid document supposed better represent basic idea document next feature approximate argumentative structure text employed consensus generation analysis complete rethorical structure text would impossible current state art text processing spite method based surface structure text usedto obtain goodquality summary obtain approximate structure first apply text agglomerative clustering algorithm basic idea procedure similar sentence must grouped together bottomup fashion based lexical similarity result hierarchical tree produced whose root represents entire document tree binary since step two cluster grouped five feature extracted tree follows h depth tree feature sentence depth tree referring position given level tree position first identify path form root tree node containing first four depth level depth level feature assigned according direction taken order follow path root since argumentative tree binary possible value position left right none latter indicates tree node depth lower four j indicator main concept binary feature indicating whether sentence capture main concept document main concept obtained assuming relevant word noun hence sentence identify noun using partofspeech software noun compute number sentence occurs fifteen noun largest occurrence selected main concept text finally sentence value feature considered true sentence contains least one noun false otherwise k occurrence proper name motivation feature occurrence proper name referring people place clue sentence relevant summary considered binary feature indicating whether sentence contains value true least one proper name value false proper name detected partofspeech tagger l occurrence anaphor consider anaphor indicate presence non essential information text sentence contains anaphor information content covered related sentence detection anaphor performed way similar one proposed strzalkowski determine whether certain word characterize anaphor occur first six word sentence also binary feature taking value true sentence contains least one anaphor false otherwise occurrence nonessential information consider word indicator nonessential information word speech marker furthermore additionally typically occur beginning sentence also binary feature taking value true sentence contains least one discourse marker false otherwise mlbased trainable summarization framework consists following step apply standard preprocessing information retrieval method document namely stopword removal case folding stemming employed stemming algorithm proposed porter sentence converted vectorial representation compute set feature described previous subsection continuous feature discretized adopt simple classblind method consists separating original value equalwidth interval experiment different discretization method surprisingly selected method although simple produced better result experiment ml trainable algorithm employed employ two classical algorithm namely c naive bayes usual ml literature employ algorithm trained training set evaluated separate test set framework assumes course document collection reference extractive summary correct sentence belonging automatically produced extractive summary labeled positive classificationdata mining terminology whereas remaining sentence labeled negative experiment extractive summary document automatically obtained using authorprovided nonextractive summary explained section computational result previously mentioned used two wellknown ml classification algorithm namely naive bayes c former bayesian classifier assumes feature independent despite unrealistic assumption method present good result many case successfully used many text mining project c decisiontree algorithm frequently employed comparison purpose classification algorithm particularly data mining ml community two series experiment first one employed automatically produced extractive summary second one manuallyproduced summary employed experiment used document collection available tipster document base collection consists text published several magazine computer hardware software etc size varying kbytes kbytes due framework used document authorprovided summary set keywords whole tipster document base contained document characteristic subset document randomly selected experiment reported section first experiment using automaticallygenerated reference extractive summary employed four textsummarization method follows proposal feature described section using c classifier b proposal using naive bayes classifier c first sentence used baseline summarizer method selects first n sentence document n determined desired compression rate defined ratio summary length source length although simple procedure provides relatively strong baseline performance textsummarization method word summarizer w microsofts w text summarizer part microsoft word used comparison summarization method several author method us nondocumented technique perform almost extractive summary text summary size specified user w characteristic different previous method specified summary size refers number character extracted sentence modified w experiment due characteristic adirect comparison w method completely fair summary generated w contain less sentence summary produced method ii case possible compute exact match sentence selected w original sentence case ignore corresponding sentence important note proposal based ml trainable summarizer two remaining method trainable used mainly baseline result comparison document collection used experiment consisted document partitioned disjoints training test set document training set contained document kbytes document kbytes document kbytes document kbytes average number sentence per document since total sentence training set test set contained document kbytes document kbytes document kbytes document kbytes average number sentence per document since total sentence test set table report result obtained four summarizers consider compression rate performance expressed term precision recall value expressed percentage corresponding standard deviation indicated symbol best obtained result shown boldface table result training test set composed automaticallyproduced summary summarizer compression rate compression rate precision recall precision recall trainable c trainable bayes first sentence word summarizer draw following conclusion experiment value precision recall method significantly higher rate compression rate expected result since larger compression rate larger number sentence selected summary larger probability sentence selected summarizer match sentence belonging extractive summary best result obtained trainable summarizer naive bayes classifier compression rate using feature c classifier obtained result poor result similar firstsentences word summarizer baseline latter result offer u interesting lesson research project trainable summarizers focus proposal new feature classification trying produce elaborate statisticsbased linguisticsbased feature butthey usually employ single classifier experiment normally conventional classifier used result indicate researcher concentrate attention study elaborate classifier tailored textsummarization task least evaluate select best classifier among conventional one already available second experiment employ test step summary manually produced human judge emphasize training phase proposal used database automaticallygenerated summary employed previous experiment test database composed document selected random original document base manual reference summary produced human judge professional english teacher many year experience specially hired task compression rate four summarizers first experiment compared obtained result presented table table result training set composed automaticallyproduced summary test set composed manuallyproduced summary summarizer compression rate compression rate precision recall precision recall trainable c trainable bayes first sentence word summarizer best result obtained proposal using naive bayes algorithm classifier similar previous experiment result compression superior result produced compression order verify consistency two experiment compared manuallyproduced summary automaticallyproduced one considered manuallyproduced summary reference calculated precision recall automatically produced summary document obtained result presented table result consistent one presented mitra indicate degree dissimilarity manuallyproduced summary automaticallyproduced summary experiment comparable dissimilarity two summary produced different human judge table comparison automaticallyproduced manuallyproduced summary precision recall compression rate compression rate conclusion future research work explored framework using ml approach produce trainable text summarizers way proposed year ago kupiec chosen research direction allows u measure result text summarization algorithm objective way similar standard evaluation classification algorithm found ml literature avoids problem subjective evaluation quality summary central issue text summarization research performed extensive investigation framework proposal employ trainable summarizer us large variety feature employing statisticsoriented procedure others using linguisticsoriented one classification task used two different well known classification algorithm namely naive bayes algorithm c decision tree algorithm hence possible analyze performance two different text summarization procedure performance procedure compared performance two nontrainable baseline method basically two kind experiment first one considered automaticallyproduced summary training test phase second experiment used automaticallyproduced summary training manually produced summary testing general trainable method using naive bayes classifier significantly outperformed baseline method interesting finding experiment choice classifier naive bayes versus c strongly influenced performance trainable summarizer intend focus mainly development new extended classification algorithm tailored text summarization future research work reference barzilay r elhadad using lexical chain text summarization mani maybury ed proceeding acleacl workshop intelligent scalable text summarization association computional linguistics brandow r mitze k rau l automatic condensation electronic publication sentence selection information processing management brill e simple rulebased partofspeech tagger proceeding third conference applied comp linguistics assoc computational linguistics carbonell j g goldstein j use mmr diversitybased reranking reordering document producing summary proceeding sigir edmundson h p new method automatic extracting journal association computing machinery harman data preparation merchant r ed proceeding tipster text program phase morgan kaufmann publishing co kupiec j pedersen j chen f trainable document summarizer proceeding th acmsigir conference association computing machinery larocca neto j santos kaestner ca freitas aa document clustering text summarization proc th int conf practical application knowledge discovery data mining padd london practical application company luhn h automatic creation literature abstract ibm journal research development mani house klein g hirschman l obrsl l firmin chrzanowski sundheim b tipster summac text summarization evaluation mitre technical report mtr w mitre corporation mani bloedorn e machine learning generic userfocused summarization proceeding fifteenth national conference ai aaai mani automatic summarization jbenjamins publ co amsterdam philadelphia marcu discourse tree good indicator importance text mani maybury ed adv automatic text summarization mit press mitchell machine learning mcgrawhill mitra singhal buckley c automatic text summarization paragraph extraction proceeding acleacl workshop intelligent scalable text summarization madrid nevillmanning c g witten h paynter g w et al kea practical automatic keyphrase extraction acm dl porter mf algorithm suffix stripping program reprinted sparckjones k willet p ed reading information retrieval morgan kaufmann quinlan j c program machine learning morgan kaufmann sao mateo california rath g j resnick savvage r formation abstract selection sentence american documentation salton g buckley c termweighting approach automatic text retrieval information processing management reprinted sparckjones k willet p ed reading iretrieval morgan kaufmann sparckjones k automatic summarizing factor direction mani maybury advance automatic text summarization mit press strzalkowski stein g wang j wise b robust practical text summarizer mani maybury ed adv autom text summarization mit press teufel moens argumentative classification extracted sentence first step towards flexible abstracting mani maybury ed advance automatic text summarization mit press yaari segmentation expository text hierarchical agglomerative clustering technical report barilan university israel