align attention head merging effective way converting mha gqa qingyunjin xiaohuisong fengzhou zengchangqin beihanguniversitybeijingchina oppoaicenterbeijingchina jinqingyun zcqinbuaaeducn songxiaohui zhoufengoppocom abstract large language model shown performwellonavarietyofnaturallanguage mlp layer processingproblems howeverasthemodel sizeandtheinputsequenceslengthincrease rapid increase kv cache significantly slowsdowninferencespeed thereforegqa model alternative mha model widely introduced llm workweproposealowcostmethodforprun ingmhamodelsintogqamodelswithany compression ratio keyvalue head methodisbasedonl maskstograduallyre move redundant parameter addition applyorthogonaltransformationstoattention headswithoutchangingthemodeltoincrease similaritybetweenattentionheadsbeforeprun ingtraininginordertofurtherimproveperfor manceofthemodel ourmethodcanbecom patiblewithrotarypositionembeddingrope mean model training fullyadaptedtothemainstreamstandardgqa framework experimentsdemonstratethatour strategycancompressuptoofkeyvalue headsofthellamabmodelwithouttoo muchperformancedegradationjustachieved throughsupervisedfinetuning introduction recently large language model llm rad fordetalbrownetalouyangetal showremarkableperformanceonavarietyof naturallanguageprocessingtasks howeversince mostllmsarebasedontransformerarchitecture vaswani expansion sequence lengthduringinferenceresultsinalinearincrease inthememoryfootprintofkvcachesignificantly slowingdownmodelinference thereforereduc ingthesizeofkvcacheisakeyissueforllms multiquery attention mqa shazeer andgroupedqueryattentiongqaainslieetal correspondingauthor new value new key logits distillation lm head lm head mlp layer value transfer n n attention layer key g query embedding embedding origin model pruned model figureanillustrationofourpruningtrainingprocess thetraininglossconsistsoftwoparts distillationloss betweenteachermodelandpruningmodelandpruning losstoimposekeyvalueprojectionmatricesgradually transferred new one pruning original key valueprojectionmatriceswillbediscardedthenweget astandardgqamodel reducekvcachebyallowingmultipleatten tionheadstoshareasinglekeyvalueheadwhich issimpleandeffective sincegqahasbetterinfer encestabilityandperformanceithasbeenwidely usedinllamatouvronetalllama dubeyetalqwenyangetal mistral jiang et al llm liu etalzhangetal amethodforconvertingmhamodeltogqa model also proposed ainslie et al theyconstructeachgroupkeyandvalueheadby meanpooling original head within group uptrain model restore modelperformance howeverthecomputational resourcesrequiredforuptrainingareunaffordable inmostconditions work use l mask louizos et al ced lcsc vvixratotransferoriginalkeyvalueprojectionma alinearrateduringtheprocessofpruningtraining trice new one figure show concept incofixiaetalthel methodisapplied inadditionbasedontheideaofcomputationalin directly llm introducing pruning mask varianceinthemodelashkboosetalwe withdifferentgranularities theyprunethehidden apply orthogonal transformation matrix dimension intermediate dimension num ofattentionheadswithoutchangingthemodelbe berofattentionheadsandevenanentiremhaor fore pruning training measure similarity ffnlayer thesubsequentworkshearedllama betweenkvcachesofdifferentattentionheadsaf xia et al incorporates previous method terorthogonaltransformationthendivideattention andspecifiesthetargetstructuresothatthepruned headswithhighsimilarityofkvcachesintothe model directly adapted standard llm samegroup thesimilarityamongthekvcaches framework derivedfromtheattentionheadswithinthesame group maximized orthogonal transfermhatogqa transformation experimentsshowthatthismethod ainslie et al proposes gqa first significantly improve performance time mha converted gqa using prunedmodelafterthetransformationofattention meanpoolinginitialization howeverthismethod head yuetalisthemostrelevantworkto requiresuptrainingtorestoreperformanceandin ourshowevertheirmethodisnotfullycompatible cur significant computational cost yu et al withropesuetal ourcontributionsare keep corresponding parameter based asfollows principal component collected kv cachesthenuseslorahuetaltofine propose general lowcost method tunethemodeltorestoreperformance chenetal forconvertingmhastructuretogqawhich proposes regroup attention head based cancompressthekeyandvalueheadstoany criterion cosine similarity allows percentageandbasicallyrestoreperformance varying group size however none aftersupervisedfinetuning aforementionedimprovementmethodscanbefully adaptedtothestandardgqamodel ourstudyprovidesanewperspectiveforeval uatingthesimilaritybetweenattentionheads compressingmodelbasedonthe present new insight future principalcomponentsoffeatures searchrelatedtocompressingkvcache somepreviousworksliuetalyuandwu weconductexperimentsonpruningtraining havepointedoutthatthefeaturesofllms thellamabmodeltouvronetal aregenerallylowrank thereforeidentifyingand intogqagqaandgqaseparately deletingthelowrankcomponentsofthemodelis themodelperformancedoesnotdecreasesig aneffectivemethodformodelcompression nificantly compared fullsize lowrankbertnoachandgoldberg model reducesthenumberofparametersandincreasesin ferencespeedbydecomposingtheweightmatrices relatedworks intotwolowrankmatrices slicegptashkboos l regularization etalintroducestheideaofcomputational invarianceintransformerarchitectureandremoves l regularizationlouizosetalisastruc columnsorrowsofthetransformedweightmatri turedpruningapproachthattransformsapruning cestoreducemodelsize yuetalapplies problemintoanoptimizationproblemundercon orthogonaltransformationstokeyvalueprojection straints thepruningprocessisperformedsimul matricesbyanalyzingthelowrankfeaturesofkv taneously model optimization introduc cache ingtrainablemasks withthewideapplicationof llmsthismethodhasbeenappliedtocompress method ingllms intheworkofwangetalthe l methodisappliedbasedonlowrankpruningto section specifically describe achievefurtherimprovementsineffectivenessand method ourmethodconsistsoftwopartstransfor theyproposetograduallyincreasethetargetsizeat mationofthenetworkandpruningtraining trans block index ytiralimis enisoc cosine similarity kv cache block value key tnuoc figure ineachblockofllamabtheaveragecosinesimilarityiscalculatedbetweeneverytwokeyandvalue cache forconveniencetheaveragesimilaritiesshowninthisfigurearetheirabsolutevalues itcanbeseenthat mostpairsofkvcachesarealmostorthogonal thismayexplainwhydirectlymergingkeyvalueheadscauses significantloss formation network represents applying n cid simvori cosv nv n thogonal transformation projection matri ij n j cesinordertoincreasethesimilaritybetweenat n tention head group ij two attention head increase efficiency model optimization sameblocknrepresentsthenthtokeninthiscache pruningtrainingprocesscombinespruningusing asshowninfigurewenoticethatwhileafew l mask louizos et al knowledge pair kv cache share high cosine similarity distillationgouetal vast majority al orthogonal reason directly motivation meanpoolingprojectionmatricesresultsinsignif toanalyzethecharacteristicsofkvcachewefol icant loss uptraining needed restore lowapriorcalibrationmethodforllmsfrantar performance andalistarhsunetalinordertoob however according yu et al kv taincalibrationdata samplesequencesfrom cachesarelowrank giventhatthesecachesoc c raffel et al training set cupyonlyaportionofspatialdimensionsapplying sequenceistokenslongtokensinto appropriateorthogonaltransformationstothepro tal thenperformmodelinferenceonllamab jectionmatricestoalignkeyandvaluecachescan andcollectkvcachesie reducethedifficultyofmodeloptimization fortu natelythisapproachisfeasible k k k v v v h h preliminary wherekv rdn arekvcachescorresponding given two set vector shape toeachblockwhichcanbedividedintok v rdhnn isthenumberoftokensdisembedding x x x x n rmn rmn find op dimensionandh representsthenumberofheadsin n timalorthogonaltransformationthatalignsthetwo eachmhad issettodhthenwecancalculate h setsofvectors thiskindofproblemsiscalledor theaveragecosinesimilaritybetweeneachoftwo thogonalprocrustesproblemanditsmathematical headsasfollows formulationisasfollows n cid simk io r ji n cosk ink jn minqxy f n qtheoptimalorthogonaltransformationcanbede block output selfattention layer rived svd matrix yxt general canbeseenasthesumofallattentionheads solutionisschnemann performsvdonthecovariancematrixofxand multiheadw w w w q k v yxt cidh w w xsoftmaxcid w kix tw qixcid oi vi h thenobtaintheoptimalorthogonalmatrixq wheretheprojectionmatricesintheattentionheads q arew qiw kiw vi rdhd andw oi rddh x rdlen represents input sequence wecanusethesamewaytoalignanytwokv x rd representsthecurrenttoken forbrevity cachesfromdifferentattentionheadsinthesame ropeisignoredhere thenwecanfusetheorthog block furthermoreifwewanttoalignmorethan onal matrix value projection matrix w vj twosetsofcachesgeneralizedprocrustesanalysis output projection matrix w ensure oj wikipediacontributorsisagoodsolution computationalinvariance thedetaileddescriptionisshowninalgorithm w q w vj vj vj algorithmgeneralizedprocrustesanalysis require matricesx x x h w j w ojqt vj ensure alignedmatricesy h asforw andw duetotheexistenceofrope initializey x foralli q k computemeanshapem cidh orthogonal transformation cannot applied di h rectly however divide ddimension repeat spaceintodsubspacesandapplytheorthogo fori toh compute svdytm naltransformationineverytwodimensionjustlike updatey rope say orthogonal matrix keyprojectionmatrixshouldbeinthisform endfor updatemeanshapem h cidh iy r untilconvergence return h r kj r r transformationofattentionheads wherer isadrotationmatrix thenwefusethe tocalculatetheoptimalorthogonalmatrixforeach orthogonal matrix r query projection pairofkeyandvalueheadswecollectkvcaches kj matrixw andkeyprojectionmatrixw according method mentioned qj kj weusetwocriteriatoperformthecalculations w r w based cosine similarity firstly normalize qj kj qj vector k v roughly reduce influenceofmagnitudeofthevector w kj r kjw kj soweget k k k qtk rd r w x trd r w x kj qj kj kj v v xt sw qt jr kt jr tsr kjw kjx v xtwt rd w x qj t kj thenwecangettheoptimalorthogonalmatrixq v r sw qjx str tw kjx toalignanytwovaluecachestakingv andv j q stk example vvt whereq representsthequeryofthesth position j andk representsthekeyofthetth position q transformationdoesntchangethemodeleither vjfigure thisfigureshowstheaveragecosinesimilarityofkeyandvaluecachesbetweenanytwoheadsbefore andafteranapplyingtransformationinsomeblocksofllamabappropriateorthogonaltransformationscan significantlyimprovethecosinesimilaritybetweenkvcaches inthiswaygivenanytwokeyorvaluecaches p vj wecanusethismethodtocalculatethemaximum n cosinesimilarityachievable simvafter cid v np v n ij n vj j f n simkafter cid cosk nr k n n ij n kj j n next section compare perfor mancesofthetwocriteria n simvafter cid cosv nq v n ij n vj j findbettergroupingmethod n afterobtainingthesimilarityscoresbetweenevery noticingsimvafter isequaltosimvafter sois pair attention head regroup attention ij ji simkafter figureshowsthecosinesimilarity head based score define sim betweenkvcachesbeforeandafterapplyingthe ilarity score group sum similarity transformation scoresbetweeneverypairofattentionheadswithin based euclidean distance similar ap thatgroup andthetotalsimilarityscoreforeach plyingtransformationsbasedoncosinesimilarity grouping result sum similarity score wealsoapplytransformationsbasedoneuclidean allgroups ourobjectiveistodeterminethegroup distancebetweentwokvcaches inthiscasewe ing result highest total score use dontnormalizevectorsandthesimilaritybetween simkafter simvafter grouping criterion twocachescanbedescribedasthenegativevalue highest similarity pair within oftheeuclideandistanceofthemforbrevityonly samegroupdoesnotnecessarilyequatetothelowestcostin somekeyformulasaredisplayedhere termsofconvergingtothesameparametersduringpruning thisstrategyremainsacceptableconsideringcomputationand v iv jt timecostsrespectively inthenextsectionwewillcompare grouping use generalized pro performance two way mathe crustesanalysistoalignattentionheadsinthesame maticalexpressionofthescoreofagroupingresult group isasfollows g adaptationofl regularization score cidg cid simkafter pruning training add new projection key agiagj matriceswhichareinitializedbymeanpoolingall gijd theoriginalheadswithinthatgrouptothemodel ainslieetalhereweusew orw g k kg v kg score cid cid simvafter represent new projection matrix gth value agiagj groupinthekth blockofthemodel gijd g gth group g group ele w cidd w mentsina g aretheserialnumberofanattention k kg k kgdi headandthereared hgheadsinagroup weusesimulatedannealingalgorithmtoget cid best grouping result exchange two random w w v v kg kgdi head different group calculate new scoreacceptingthenewresultifitreachesahigher new matrix trained together score repeat process multiple iteration themodelandreplaceoriginalkeyvalueheadsaf becauseinitializationhasasignificantimpacton terpruning assumethemodelhasn block block thefinalresultwerunthealgorithmmultipletimes h head attention layer introduce thedetailsofthealgorithmareshownbelow l masksz rn blocksh louizosetalto achievethisgoal algorithmsimulatedannealing require maxiterepochsimk orsimv wapply z w z w ensure grouping result highest score k kj kj k kj kj k kg bestg n wapply z w z w setscore best v kj kj v kj kj v kg fori epochdo g j w w orig initializesolutiong randomly k kj v kj n inal projection matrix w w score current calculatescoreg nsimv k kg v kg ifscore current score best thenewlyaddedprojectionmatricesw kap kp jly setscore best score current wapply aretheprojectionmatricesemployeddur v kj setbestg n g n ing pruning following l regularization ap endif proachweparametrizethepruningmaskstohard forj maxiter concrete distribution initially mask set g exchangerandomelementsfromdif z weconstrainthemaskstozeroduringprun n ferentgroupsing n ingxiaetal andtheoriginalprojection score new calculatescoreg nsimv matrixwillbetransferredtothenewmatrixwhen ifscore new score current z unlike traditional l regularization setg n g n aim eliminate original key value head score current score new andjustutilizel maskstograduallytransferthe ifscore new score best original head newly added head mask setscore best score new acrossblocksareconstrainedbyasinglelossfunc setbestg n g n tion endif endif endfor l l cid nblo ckh cid ztcid cid nblo ckh cid ztcid endfor target size equal zero return bestg n sparsitywarmupstepsweusevanillakllossandbildlosslietal grouping key grouping value toencouragealignmentofstudentlogitswith dicate grouping attention head based key teacherlogits valuecachesimilarity cosanddistrepresent transformation based cosine similarity l distill l kll bild euclideandistance tosumuptheoveralltraininglossis mainresults wereporttheexperimentalresultsintable ex l l l distill l cept one set experiment transformed modelsoutperformthebaseline asthesparsityof experiment keyvalueheadincreasestheadvantageofmodel setting transformationbecomesmoreobviousdemonstrat ingtheeffectivenessofaligningtheattentionheads modelconfigurations weapplyourmethodtothe whiletheresultsofthegqadontmeetexpec llamabmodeltouvronetalthrough tationsinotherexperimentsmodelstransformed experiment convert source based value cache similarity using euclidean modeltogqagqaandgqaandcom distanceasthecriterionachievedthebestperfor pare finetuned fullsize model sepa mance rately datasets use following opensource analysisoftheresults datasetsforpruningtrainingandevaluation boolq experimental result indicate group clarketalpiqabisketalhel ingofattentionheadsdoeshaveanimpactonthe laswag zellers et al winogrande sak performanceofprunedmodelsandthatgrouping aguchietalarceasyclarketal attentionheadsbasedonvaluecachesimilaritiesis arcchallengeclark et al siqa sap beneficial model performance et al openbookqa mihaylov et al surpriseasthevaluecachedirectlycontributesto size instruction template theoutputvectoroftheattentionlayer although datasetarelistedinappendixb experiment utilize l mask accelerate implementation detail use nvidia trainingprocessmodeltransformationcanbenefit agputoperformmodeltransformationand anymhatogqaconversionprocess nvidiaagpusforsupervisedfinetuning sfttheteachermodelandpruningtraining randomlyselectsequencesoftokenslong fromthectrainingsetraffeletalascal ibrationdatainmodeltransformation inallexperi mentstheinitiallearningrateiseforthemodel parameter e pruning mask cosineschedulerisemployedtoreducethelearn ingratetobytheendoftraining weperform epochsofsftontheteachermodelepochsof pruningtrainingongqaepochsongqa andepochsongqa hyperparameter setting found block index appendixa ablationstudies test impact different similarity evalua tioncriteriaseesectionandgroupingstrate gy see section result presented table baseline refers pruning di rectlywithoutanytransformationdefaultgroup ing refers merging adjacent attention head ezis laer target size target size target size figurethesharedsparsityofl masksacrossblocks allows different pruning speed different block leadingtoamorestabletrainingprocess addition experiment found thatthemodelwithintroductionofnewkvheads performsmuchbetterthanthemodelretainingorig inalones thatswhywechoosenottoretainany originalkvheadsthissettingalsoallowsdifferentmodel method boolq piqa hellaswag winogrande arcc arce openbookqa siqa avg mha teacher baseline co defaultgrouping dist gqa co groupingbykey dist co groupingbyvalue dist baseline co defaultgrouping dist gqa co groupingbykey dist co groupingbyvalue dist baseline co defaultgrouping dist gqa co groupingbykey dist co groupingbyvalue dist tableperformancesofprompttuningonllamamodelswithvariousmethods thelastcolumnavg average accuracyindicatestheaverageaccuracyofallthesesubdatasets pruningspeedsfordifferentblocks figureshows ing method may optimal one actual average size mask block find reasonable grouping method one differenttargetsizes future research direction moreover methodentirelyreliesonthestatisticalmathemati conclusion calfeaturesofeachattentionheadwithoutconsid eringsemanticinformationofeachattentionhead paper propose general method fact compressing attention head based se pruning mha model gqa model mantic information also promising direction anycompressionratioofkeyvalueheads wefind tangetal applying appropriate orthogonal transforma tions model increase similarity tweenkeyvalueheadswithoutchangingthemodel reference thereby reducing difficulty model pruning joshuaainsliejamesleethorpmichieldejongyury furthermoreweintroducel masksduringprun zemlyanskiyfedericolebrnandsumitsanghai ing training reduce impact directly gqa traininggeneralizedmultiquerytransformer eliminatingparametersonthemodel ourmethod modelsfrommultiheadcheckpoints arxivpreprint arxiv isapplicabletoallkeyvalueheadpruningcondi tions saleh ashkboos maximilian l croci marcelo gen nari nascimento torsten hoefler james hensman slicegpt compresslargelanguagemod limitation el deleting row column arxiv preprint arxiv ourworkhastwomainlimitations firstwedont delveintothegroupingmethodandcurrentgroup yonatanbiskrowanzellersjianfenggaoyejinchoietal piqa reasoningaboutphysicalcommonsense edward j hu yelong shen phillip wallis zeyuan innaturallanguage inproceedingsoftheaaaicon allenzhuyuanzhilisheanwangluwangand ferenceonartificialintelligencevolumepages weizhuchen lora lowrankadaptationoflarge languagemodels arxivpreprintarxiv tom brown benjamin mann nick ryder melanie subbiahjareddkaplanprafulladhariwalarvind albert q jiang alexandre sablayrolles arthur men neelakantanpranavshyamgirishsastryamanda schchrisbamforddevendrasinghchaplotdiego askell sandhini agarwal ariel herbertvoss delascasasflorianbressandgiannalengyelguil gretchen krueger tom henighan rewon child laume lample lucile saulnier et al mistral b adityarameshdanielzieglerjeffreywuclemens arxivpreprintarxiv winter chris hesse mark chen eric sigler teusz litwin scott gray benjamin chess jack minchonglifengzhouandxiaohuisong bild bi clark christopherberner sammccandlish alec directionallogitsdifferencelossforlargelanguage radford ilya sutskever dario amodei lan modeldistillation arxivpreprintarxiv guagemodelsarefewshotlearners inhlarochelle mranzatorhadsellmfbalcanandhlined itors advance neural information processing yixin liu kai zhang yuan li zhiling yan chujie systemsvolumepagescurranasso gao ruoxi chen zhengqing yuan yue huang hanchi sun jianfeng gao et al sora ciatesinc view background technology limitation opportunitiesoflargevisionmodels arxivpreprint tony cai jianqing fan tiefeng jiang distribu arxiv tionsofanglesinrandompackingonspheres journalofmachinelearningresearch zichangliujuewangtridaotianyizhoubinhang yuanzhaosonganshumalishrivastavacezhang yuandongtianchristopherreetal dejavu con yuang chen cheng zhang xitong gao robert textual sparsity efficient llm inference time mullinsgeorgeaconstantinidesandyirenzhao ininternationalconferenceonmachinelearning optimisedgroupedqueryattentionmechanismfor pagespmlr transformer arxiv preprint arxiv shayne longpre le hou tu vu albert webson hyungwonchungyitaydennyzhouquocvle christopher clark kenton lee mingwei chang barret zoph jason wei et al flan collection tom kwiatkowski michael collins kristina designing data method effective instruc toutanova boolq exploring surprising diffi tiontuning ininternationalconferenceonmachine culty natural yesno question arxiv preprint learningpagespmlr arxiv christoslouizosmaxwellinganddiederikpkingma peterclarkisaaccowheyorenetzionitusharkhot learningsparseneuralnetworksthroughlregular ashishsabharwalcarissaschoenickandoyvind ization arxivpreprintarxiv tafjord thinkyouhavesolvedquestionanswering tryarctheaireasoningchallenge arxivpreprint todormihaylovpeterclarktusharkhotandashish arxiv sabharwal canasuitofarmorconductelectricitya newdatasetforopenbookquestionanswering arxiv abhimanyudubeyabhinavjauhriabhinavpandey preprintarxiv abhishekkadianahmadaldahleaieshaletman akhil mathur alan schelten amy yang angela matanbennoachandyoavgoldberg compressing fanetal thellamaherdofmodels arxivpreprint pretrainedlanguagemodelsbymatrixdecomposi arxiv tion proceeding st conference asiapacificchapteroftheassociationforcompu eliasfrantaranddanalistarh sparsegpt massivelan tationallinguisticsandthethinternationaljoint guagemodelscanbeaccuratelyprunedinoneshot conferenceonnaturallanguageprocessingpages ininternationalconferenceonmachinelearning pagespmlr longouyangjeffreywuxujiangdiogoalmeida jianpinggoubaoshengyustephenjmaybankand carrollwainwrightpamelamishkinchongzhang dachengtao knowledgedistillation asurvey sandhiniagarwalkatarinaslamaalexrayetal ternationaljournalofcomputervision traininglanguagemodelstofollowinstructionswith human feedback advance neural information processingsystems danhendryckscollinburnsstevenbasartandyzou mantasmazeikadawnsongandjacobsteinhardt alecradfordkarthiknarasimhantimsalimansand measuringmassivemultitasklanguageunderstand ilyasutskever improvinglanguageunderstanding ing arxivpreprintarxiv bygenerativepretraining colinraffelnoamshazeeradamrobertskatherine zihengwangjeremywohlwendandtaolei struc leesharannarangmichaelmatenayanqizhou tured pruning large language model arxiv weiliandpeterjliu exploringthelimitsoftrans preprintarxiv fer learning unified texttotext transformer journalofmachinelearningresearch wikipedia contributor generalized procrustes analysis wikipedia free encyclopedia url httpsenwikipediaorgw keisuke sakaguchi ronan le bra chandra bhaga indexphptitlegeneralizedprocrustes vatulaandyejinchoi winogrande anadversarial analysisoldid online accessed winograd schema challenge scale communica october tionsoftheacm mengzhouxiazexuanzhonganddanqichen struc v sanh distilbert distilled version bert turedpruninglearnscompactandaccuratemodels smaller faster cheaperandlighter arxivpreprint arxivpreprintarxiv arxiv mengzhouxiatianyugaozhiyuanzenganddanqi maarten sap hannah rashkin derek chen ronan chen shearedllama acceleratinglanguagemodel lebrasandyejinchoi socialiqa commonsense pretrainingviastructuredpruning arxivpreprint reasoningaboutsocialinteractions arxivpreprint arxiv arxiv xu qingfeng sun kai zheng xiubo geng peter h schnemann generalized solution puzhaojiazhanfengchongyangtaoanddaxin orthogonalprocrustesproblem psychometrika jiang wizardlm empoweringlargelanguagemod el follow complex instruction arxiv preprint arxiv noam shazeer fast transformer decoding one writehead need arxiv preprint yang baosong yang binyuan hui bo zheng arxiv bowenyuchangzhouchengpenglichengyuan lidayihengliufeihuangetal qwentechnical richard socher alex perelygin jean wu jason report arxivpreprintarxiv chuangchristopherdmanningandrewyngand christopher potts recursive deep model se hao yu jianxin wu compressing transformer manticcompositionalityoverasentimenttreebank featuresarelowrankbutweightsarenot inpro proceeding conference empiri ceedingsoftheaaaiconferenceonartificialintelli calmethodsinnaturallanguageprocessingpages gencevolumepages hao yu zelan yang shen li yong li jianxin jianlinsu murtadhaahmed yulu shengfengpan wu effectivelycompresskvheadsforllm arxiv wen bo yunfeng liu roformer enhanced preprintarxiv transformerwithrotarypositionembedding neuro computing rowan zellers ari holtzman yonatan bisk ali farhadi yejin choi hellaswag kai sun dian yu dong yu claire cardie chine really finish sentence arxiv preprint vestigatingpriorknowledgeforchallengingchinese arxiv machinereadingcomprehension transactionsofthe association computational linguistics peiyuan zhang guangtao zeng tianduo wang weilu tinyllama anopensourcesmalllanguage model arxivpreprintarxiv mingjiesunzhuangliuannabairandjzicokolter simple effective pruning approach large languagemodels arxivpreprintarxiv hanlintangyanglinjinglinqingsenhanshikuan hong yiwu yao gongyi wang razoratten tionefficientkvcachecompressionthroughretrieval head arxivpreprintarxiv hugo touvron louis martin kevin stone peter al bert amjad almahairi yasmine babaei nikolay bashlykovsoumyabatraprajjwalbhargavashruti bhosaleetal llama openfoundationandfine tunedchatmodels arxivpreprintarxiv vaswani attention need advance neuralinformationprocessingsystemsa hyperparametersettings reduce memory usage employ deepspeed sft pruning training set k forbildlosslietal duringthepruningtrainingprocessthesparsitywarmupstepsaccount forofthetotalstepsduringwhichthetargetsizeofthel masksdecreaseslinearlytozero maximumpruningstepscompriseofthetotalstepsafterwhichthemasktrainingceasesonlythe modelparametersareadjusted somemorehyperparametersettingsforsftteachermodelandpruning trainingareshownintable sftteacher pruningtraining batchsize microbatchsize warmupsteps initiallrofmasks e initiallrofmodel e table somehyperparameterssettingforexperiments b detailsofdatasets thesizesofsubdatasetsareshownintable datasets train test boolq piqa hellaswag winogrande arcc arce openbookqa siqa total table sizesofdifferentdatasets thetemplateofeachdatasetcanbeseenintable dataset template arcc whichcolorshirtwillreflectthemostlightonahotsunnyday arce choice blackblueredwhite openbookqa answer hellaswag pleasechoosethemostappropriatetexttocompletethepassagebelow passage amaleathleteputspowderonhishands choice bend inspects hand damage shake shakingly beforeputtingtheminhismouthmountsahighbeaminthegymthenjumps upanddoesahighjump answer boolq coroner bbc announced march would furtherseries question willtherebeasecondseriesofthecoroner choice truefalse answer winogrande choosethemostsensibletexttoreplacetheinthefollowingsentence natalie waslessreligousthanpatriciathereforeattendedchurchservicesmoreoftenon sunday choice nataliepatricia answer piqa goal howdoyoufloodaroom choose sensible solution achieve goal choice fill objectsfillitwithwater answer siqa sashatookhimtovegasforavacation question howwouldsashafeelafterwards choice saddepressedfulfilled answer table thetemplateofeachdataset