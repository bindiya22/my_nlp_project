Journal of Advances in Information Technology
ISSN 1798-2340
Volume 1, Number 1, February 2010
Contents
EDITORIAL
Welcome Message from the Editor-in-Chief 1
A.C.M. Fong
Introducing the Associate Editor-in-Chiefs 2
A.C.M. Fong
Introduction to the Inaugural Issue 3
A.C.M. Fong
REGULAR PAPERS
A Review of Machine Learning Algorithms for Text-Documents Classification 4
Aurangzeb Khan, Baharum Baharudin, Lam Hong Lee, and Khairullah Khan
Multilingual Context Ontology Rule Enhanced Focused Web Crawler 21
Mukesh Kumar and Renu Vig
Integrated Performance and Visualization Enhancements of OLAP Using Growing Self Organizing 26
Neural Networks
Muhammad Usman, Sohail Asghar, and Simon Fong
Design and Implementation of an Online Social Network with Face Recognition 38
Ray K.C. Lai, Jack C.K. Tang, Angus K.Y. Wong, and Philip I.S. Lei
Dynamic Differential Evolution for Constrained Real-Parameter Optimization 43
Youyun Ao and Hongqin Chi
Fuzzy Logic Based Position-Sensorless Speed Control of Multi Level Inverter Fed PMBLDC Drive 52
T.V. Narmadha and T. Thyagarajan
On Performance of Multicast Delivery with Fixed WiMAX Telemedicine Networks Using Single- 59
Carrier Modulation
Bernard Fong and Guan Yue HongJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 1
Welcome Message from the Editor-in-Chief
Dear Reader,
It is with much joy and anticipation that we celebrate the launch of Journal of Advances in Information Technology
(JAIT) with this inaugural issue. On behalf of the JAIT Editorial Team, I would like to extend a very warm welcome to
the readership of JAIT. I take this opportunity to thank our authors, editors and anonymous reviewers, all of whom have
volunteered to contribute to the success of the journal. I am also grateful to Dr. George Sun and the staff at Academy
Publisher for making JAIT a reality.
JAIT is dedicated to the rapid dissemination of high quality research papers on how advances in IT can help us meet
the challenges of the 21st century, and to capitalize on the promises ahead. We welcome contributions that can
demonstrate near-term practical usefulness, particularly contributions that take a multidisciplinary / convergent
approach because many real world problems are complex in nature.
Barely a decade into the new millennium, we have witnessed significant events such as 9/11, SARS, and the South
Asian Tsunami, to name but a few. There are also the on-going issues, such as demographic changes (population aging,
internal migration and rapid urbanization), management of resources, and environment issues like the current debate
about climate change. As IT practitioners and researchers, we aim to seek ways to harness the power of technology to
meet some of these real world challenges, and to provide substance for making informed judgments on important
matters. For example, modeling and prediction can help manage graying populations, model the spread of diseases or
identify/mitigate future security threats; analysis of satellite imagery coupled with knowledge discovery can help
manage deforestation, resource exploration or prepare future evacuation plans in response to natural disasters; large
scale information processing can aid understanding of how (and to what extent) human activities can impact the
environment and climate. The list of possibilities goes on.
As for the promises that lay ahead, IT has already become an integral part of everyday life. From commerce and
government to scientific discovery, healthcare, education and entertainment, IT is indispensable and will continue to
fuel further advances in all facets of human endeavors. With “e-everything”, we already see that IT has revolutionized
the way we conduct business and learn. Further advances in IT will continue to change the way we live and play. As IT
practitioners and researchers, we are responsible for making all this happen, and to some extent help shape the future.
With technological advances also come social and legal changes. For example, popularization of the internet has led to
a proliferation of virtual online communities, which are often formed on an ad hoc basis and typically characterized by
a share of some common interests. Compared to a generation ago, people today interact frequently online and these
changes inevitably have social implications.
JAIT provides an ideal forum for exchange of information on all of the above topics and more, in various formats:
full length and letter length research papers, survey papers, work-in-progress reports on promising developments, case
studies / best practice articles written by industry experts, and tutorials on up-and-coming technological breakthroughs.
JAIT is published four times a year. To ensure rapid dissemination of information, we aim at completing the review
process of each paper within 3 months of initial submission. We also publish special issues on relevant themes proposed
by guest editors.
Finally, we wish to encourage more contributions from the scientific community and industry practitioners to ensure
a continued success of the journal. Authors, reviewers and guest editors are always welcome. We also welcome
comments and suggestions that could improve the quality of the journal.
Thank you. We hope you will find JAIT informative.
A.C.M. Fong
Editor-in-Chief
February 2010
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.1-12 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
Introducing the Associate Editor-in-Chiefs
I am pleased to introduce the distinguished Associate Editor-in-Chiefs of the JAIT.
A.C.M. Fong
Editor-in-Chief
February 2010
Prof. Dr. Jinan Fiaidhi. Jinan Fiaidhi is Full Professor with tenure of Computer
Science at the Lakehead University. She is also an Adjunct Research Professor with
the University of Western Ontario. She received here graduate degrees in Computer
Science from Essex University (PgD 1983) and Brunel University (PhD, 1986).
During the period (1986-2001), Dr. Fiaidhi served at many academic positions (e.g.
University of Technology (Asso. Prof and Chairperson), Philadelphia University
(Asso. Prof), Applied Science University (Professor), Sultan Qaboos University
(Asso. Prof.). Since late 2001, Dr. Fiaidhi is full Professor and Graduate Coordinator
of Computer Science at Lakehead University, Ontario, Canada. Dr. Fiaidhi research
is focused on mobile and ubiquitous learning utilizing the emerging technologies
(e.g. Cloud Computing, Enterprise Mashups, Semantic Web). Dr. Fiaidhi research is
supported by the major research granting associations in Canada (e.g. NSERC, CFI).
Moreover, Dr. Fiaidhi is a Professional Software Engineer of Ontario (PEng), Senior Member of IEEE, member of the
British Computer Society (MBCS) and member of the Canadian Information Society (CIPS) holding the designate of
ISP. Dr. Fiaidhi has intensive editorial experience (e.g. Editor of IEEE IT-Pro, Associate EiC of the Journal of
Emerging Technologies in Web Intelligence).
Kin-Choong Yow received the BE degree with 1st class honours in electrical
engineering from National University of Singapore, in 1993 and his PhD degree
from Cambridge University, UK in 1998. He is currently an Associate Professor of
Computer Engineering in the College of Engineering, Nanyang Technological
University (NTU), Singapore. His research interests include Computer Vision,
Wireless Communications and Computational Intelligence. He has more than 65
publications in international journals and conference proceedings, and he has served
as reviewer for a number of premier journals and conferences, including the IEEE
Wireless Communications and the IEEE Transactions on Education. He has been
invited to give presentations at various scientific meetings and workshops, such as
the CNET Networks Event 2002 as well as the Microsoft Windows Server 2003
Launch 2003. His pioneering work in Mobile and Interactive Learning won the HP
Philanthropy grant in 2003 for applying Mobile Technologies in a Learning
Environment. Also, in 2003, he was one of the only 2 Singaporeans to be awarded
participation to the ASEAN Technology Program on Multi Robot Cooperation
Development held in KAIST, Korea. He was the winner of the NTU Excellence in Teaching Award 2005, and he won
the Most Popular SCE Year 1 lecturer for 4 consecutive years from 2004-2007. He has led numerous student teams to
National and International victories such as the IEEE Computer Society International Design Competition (CSIDC)
2001, the Microsoft Imagine Cup 2002, 2003 and 2005, and the Wireless Challenge 2003. He is also a member of the
IEEE, ACM, and the Singapore Computer Society (SCS).
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.2-2JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 3
Introduction to the Inaugural Issue
In this inaugural issue, we present a variety of papers to cover, as much as possible, the breadth and depth of the
intended scope of JAIT. We begin with a survey paper by Khan et al. on machine learning techniques for text document
classification. With a proliferation of electronic documents on the web and elsewhere, it is increasingly important to be
able to classify such e-documents for proper management. Their paper presents a timely review on some of the more
prominent theories and methods of document classification and text mining for e-documents.
The second paper by Kumar and Vig presents a focused crawler that is enhanced by ontological rules. So-called
focused crawlers have been developed primarily to seek and process relatively “untouched” web contents, such as some
non-English documents that are not indexed by mainstream crawlers. The proposed focused crawler is intended for
multilingual applications and the authors have applied their proposed crawler to mixed English and Hindi contents.
Online Analytical Processing (OLAP) is an important approach for mining multidimensional data. It has found
widespread business applications, for instance, in decision support. In their paper entitled “Integrated Performance and
Visualization Enhancements of OLAP Using Growing Self Organizing Neural Networks”, the authors present a novel
architecture that reportedly can offer significant improvements over previous methods.
Online social networks have revolutionized the way people interact. In the next paper, Lai et al. present a social
network that is enhanced with a face recognition and tagging feature. They also discuss the issues involved in designing
and implementing such a system. Their paper therefore lays the groundwork for further developments in the drive
towards enhancing the users’ experience of such online networks.
In the paper that follows, the authors introduce a dynamic differential evolution (D-DE) algorithm to solve
constrained optimization problems. Three major improvements over the prior art have been reported, and the authors
have performed experiments using six benchmark functions to substantiate their claim.
The last two papers underscore the wider applications of computing and information technology to the industry. The
paper authored by Narmadha and Thyagarajan presents a multi level inverter fed Permanent Magnet Brushless DC
Motor (PMBLDCM) with a simplified voltage control technique based on fuzzy logic. Sensing is through “indirect
position sensing,” which is justified by the observation that position sensing came indirectly from voltage and current
waveforms. Experiments and simulations conducted by the authors demonstrate the advantages of their approach.
In the final paper, the authors present an analysis of a fixed Worldwide Interoperability for Microwave Access
(WiMAX) system that has the potential to provide a low cost solution for integrated multimedia access networks with a
wide bandwidth. This makes it particularly suitable for telemedicine applications. They have presented results on
comparing the distribution of video data using two modulation schemes, and have estimated the bandwidth utilization
for continuous data transmission in remote patient monitoring applications.
We hope you will enjoy reading the papers published in this inaugural issue and find its contents to be very valuable.
A.C.M. Fong
Editor-in-Chief
February 2010
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.3-34 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
A Review of Machine Learning Algorithms for
Text-Documents Classification
Aurangzeb Khan, Baharum Baharudin, Lam Hong Lee*, Khairullah khan
Department of Computer and Information Science,
Universiti Teknologi PETRONAS, Tronoh, Malaysia.
*Faculty of Science, Engineering and Technology,
Universiti Tunku Abdul Rahman, Perak Campus, Kampar, Malaysia.
(E-mail: aurangzebb_khan@yahoo.com, baharbh@petronas.com.my,leelh@utar.edu.my,khairullah_k@yahoo.com)
(supervised, unsupervised and semi supervised) and
Abstract— With the increasing availability of electronic
summarization. However how these documented can be
documents and the rapid growth of the World Wide Web,
properly annotated, presented and classified. So it con-
the task of automatic categorization of documents became
sists of several challenges, like proper annotation to the
the key method for organizing the information and know-
ledge discovery. Proper classification of e-documents, online documents, appropriate document representation, dimen-
news, blogs, e-mails and digital libraries need text mining, sionality reduction to handle algorithmic issues [1], and
machine learning and natural language processing tech- an appropriate classifier function to obtain good generali-
niques to get meaningful knowledge. The aim of this paper zation and avoid over-fitting. Extraction, Integration and
is to highlight the important techniques and methodologies classification of electronic documents from different
that are employed in text documents classification, while at
sources and knowledge discovery from these documents
the same time making awareness of some of the interesting
are important for the research communities.
challenges that remain to be solved, focused mainly on text
representation and machine learning techniques. This paper Today the web is the main source for the text documents,
provides a review of the theory and methods of document the amount of textual data available to us is consistently
classification and text mining, focusing on the existing litera-
increasing, and approximately 80% of the information of
ture.
an organization is stored in unstructured textual format
Index Terms— Text mining, Web mining, Documents [2], in the form of reports, email, views and news etc. The
classification, Information retrieval. [3] shows that approximately 90% of the world’s data is
held in unstructured formats, so Information intensive
I. INTRODUCTION business processes demand that we transcend from simple
document retrieval to knowledge discovery. The need of
The text mining studies are gaining more importance re-
automatically retrieval of useful knowledge from the
cently because of the availability of the increasing num-
huge amount of textual data in order to assist the human
ber of the electronic documents from a variety of sources.
analysis is fully apparent [4].
The resources of unstructured and semi structured infor-
mation include the word wide web, governmental elec- Market trend based on the content of the online news ar-
tronic repositories, news articles, biological databases, ticles, sentiments, and events is an emerging topic for
chat rooms, digital libraries, online forums, electronic research in data mining and text mining community [5].
mail and blog repositories. Therefore, proper classifica- For these purpose state-of-the-art approaches to text clas-
tion and knowledge discovery from these resources is an sifications are presented in [6], in which three problems
important area for research. were discussed: documents representation, classifier con-
struction and classifier evaluation. So constructing a data
Natural Language Processing (NLP), Data Mining, and
structure that can represent the documents, and construct-
Machine Learning techniques work together to automati-
ing a classifier that can be used to predicate the class la-
cally classify and discover patterns from the electronic
bel of a document with high accuracy, are the key points
documents. The main goal of text mining is to enable
in text classification.
users to extract information from textual resources and
deals with the operations like, retrieval, classification One of the purposes of research is to review the available
and known work, so an attempt is made to collect what’s
———————————————— known about the documents classification and representa-
Aurangzeb khan and Khairullah Khan are PhD Students, Department tion. This paper covers the overview of syntactic and se-
of Computer and Information Science at Universiti Teknologi PETRO- mantic matters, domain ontology, tokenization concern
NAS, Tronoh, Malaysia.
and focused on the different machine learning techniques
Baharum Baharudin is an Assistant Professor at the Department of
Computer and Information Science at Universiti Teknologi PETRO- for text classification using the existing literature. The
NAS, Tronoh, Malaysia. motivated perspective of the related research areas of text
Lam Hong Lee is an Assistant Professor at the Faculty of Science, mining are:
Engineering and Technology of Universiti Tunku Abdul Rahman, Perak
Campus, located in Kampar, Malaysia. Information Extraction (IE) methods is aim to extract
(E-mail:aurangzebb_khan@yahoo.com,baharbh@petronas.com.my
specific information from text documents. This is the first
leelh@utar.edu.my)., Khairullah_k@yahoo.com)
Manuscript received May 28, 2009; revised September 7, 2009.
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.4-20JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 5
approach assumes that text mining essentially corres- more strong and effective. We have tried to get some re-
ponds to information extraction. ports drawn using tables and graphs on the basis of exist-
ing studies.
Information Retrieval (IR) is the finding of documents
which contain answers to questions. In order to achieve The rest of the paper is organized as follows. In Section 2
this goal statistical measures and methods are used for an overview of documents representation approaches,
automatic processing of text data and comparison to the Section 3 presents document classification models, in
given question. Information retrieval in the broader sense Section 4 new and hybrid techniques were presented.
deals with the entire range of information processing, Section 5 consists of comparative study of different me-
from data retrieval to knowledge retrieval [7]. thods and finally in Section 6, some discussions and con-
clusion were made.
Natural Language Processing (NLP) is to achieve a better
understanding of natural language by use of computers
II DOCUMENTS REPRESENTATION
and represent the documents semantically to improve the
classification and informational retrieval process. Seman- The documents representation is one of the pre-
tic analysis is the process of linguistically parsing sen- processing technique that is used to reduce the complexi-
tences and paragraphs into key concepts, verbs and prop- ty of the documents and make them easier to handle, the
er nouns. Using statistics-backed technology, these words document have to be transformed from the full text ver-
are then compared to the taxonomy. sion to a document vector. Text representation is the im-
portant aspect in documents classification, denotes the
Ontology is the explicit and abstract model representation
mapping of a documents into a compact form of its con-
of already defined finite sets of terms and concepts, in-
tents. A text document is typically represented as a vector
volved in knowledge management, knowledge engineer-
of term weights (word features) from a set of terms (dic-
ing, and intelligent information integration [23].
tionary), where each term occurs at least once in a certain
In this paper we have used system literature review minimum number of document. A major characteristic of
process and followed standard steps for searching, screen- the text classification problem is the extremely high di-
ing, data-extraction, and reporting. mensionality of text data. The number of potential fea-
tures often exceeds the number of training documents. A
First of all we tried to search for relevant papers, presen-
definition of a document is that it is made of a joint mem-
tations, research reports and policy documents that were
bership of terms which have various patterns of occur-
broadly concerned with documents classification or text
rence. Text classification is an important component in
mining. We identified appropriate electronic databases
many informational management tasks, however with the
and websites. Potentially relevant papers were identified
explosive growth of the web data, algorithms that can
using the electronic databases and websites, Such as
improve the classification efficiency while maintaining
IEEE Explore, Springer Linker, Science Direct, ACM
accuracy, are highly desired [8].
Portal and Googol Search Engine. For best and consistent
search a systematic search strategy was adopted. Proper Documents pre-processing or dimensionality reduction
keywords, queries, and phrases were derived from the (DR) allows an efficient data manipulation and represen-
desired research question. These keywords were arranged tation. Lot of discussions on the pre-processing and DR
into categories and related keywords were arranged. are there in the current literature and many models and
Some facilities of digital libraries like sort by year etc techniques have been proposed. DR is a very important
were also used. The search keywords were refined to in- step in text classification, because irrelevant and redun-
clude only those words which have produced successful dant features often degrade the performance of classifica-
results. We used boolean logic for efficient searching, for tion algorithms both in speed and classification accuracy
example (Classification OR text OR recommendations). and also its tendency to reduce overfitting.
We also tried combination of words like Text Mining,
DR techniques can classified into Feature Extraction (FE)
Trend and Ontology analysis, Documents classification
[11] and Feature Selection (FS) approaches, as discussed
and Subjectivity Analysis etc.
below.
Each search results were checked and assessed on screen
A. Feature Extraction
to find relevance for inclusion and exclusion with the
criteria that we made two categories of papers i.e. in or The process of pre-processing is to make clear the border
before 2000 and after 2000. The following studies were of each language structure and to eliminate as much as
included: The result statements written in English, The possible the language dependent factors, tokenization,
research is conducted after 1980, Published and/or unpub- stop words removal, and stemming [10]. FE is the fist
lished research, focused on documents classification, Ma- step of pre processing which is used to presents the text
chine Learning and Natural Language Processing (NLP). documents into clear word format. So removing stop
The non English writing and study before 1980 were ex- words and stemming words is the pre-processing tasks
cluded. [12]. The documents in text classification are represented
by a great amount of features and most of them could be
To find evidence and check the quality of papers we car-
irrelevant or noisy [9]. DR is the exclusion of a large
ried out an in-depth study of the results provided from the
number of keywords, base preferably on a statistical
research. In our future work we will try to make this step
© 2010 ACADEMY PUBLISHER6 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
process, to create a low dimension vector [13]. DR tech- the classification accuracy of some learning algorithms as
niques have inward much attention recently because ef- their evaluation function. Since wrappers have to train a
fective dimension reduction make the learning task more classifier for each feature subset to be evaluated, they are
efficient and save more storage space [14]. Commonly usually much more time consuming especially when the
the steeps taken please for the feature extractions (Fig.1) number of features is high. So wrappers are generally not
are: suitable for text classification. As opposed to wrappers,
filters perform FS independently of the learning algo-
Tokenization: A document is treated as a string, and then
rithm that will use the selected features. In order to eva-
partitioned into a list of tokens.
luate a feature, filters use an evaluation metric that meas-
Removing stop words: Stop words such as “the”, “a”, ures the ability of the feature to differentiate each class
“and”… etc are frequently occurring, so the insignificant [17]. In text classification, a text document may partially
words need to be removed. match many categories. We need to find the best match-
ing category for the text document. The term (word) fre-
Stemming word: Applying the stemming algorithm that
quency/inverse document frequency (TF-IDF) approach
converts different word form into similar canonical form.
is commonly used to weight each word in the text docu-
This step is the process of conflating tokens to their root
ment according to how unique it is. In other words, the
form, e.g. connection to connect, computing to compute
TF-IDF approach captures the relevancy among words,
etc.
text documents and particular categories.
Some of the recent literature shows that works are in
Tokenize Text
Read Documents
progress for the efficient feature selection to optimize the
classification process. A novel feature selection method is
presented in [17], in which the degrees of deviation from
Stopwords
poison distribution are utilized to select informative fea-
tures. Based on ant colony optimization a new feature
Feature Selection selection algorithm is presented in [18], to improve the
Stemming
text categorization. Also in [19] the authors introduced a
new weighting method based on statistical estimation of
the importance of a word categorization problem. The
Vector Representation [20] proposed a new feature scaling method, called class–
dependent–feature–weighting (CDFW) using naive Bayes
(NB) classifier.
Learning Algorithm Many feature evaluation metrics have been explored, not-
able among which are information gain (IG), term fre-
Fig. 1 Document Classification Process quency, Chi-square, expected cross entropy, Odds Ratio,
the weight of evidence of text, mutual information, Gini
B. Feature Selection
index. Term frequency and document frequency (TF/DF)
After feature extraction the important step in pre- (Table-1) etc. A good feature selection metric should
processing of text classification, is feature selection to consider problem domain and algorithm characteristics.
construct vector space, which improve the scalability,
The authors in [21] focused on the document representa-
efficiency and accuracy of a text classifier. In general, a
tion techniques and demonstrate that the choice of docu-
good feature selection method should consider domain
ment representation has a profound impact on the quality
and algorithm characteristics [15]. The main idea of FS is
to select subset of features from the original documents.
of the classifier. They used the centroid-based text clas-
sifier, which is a simple and robust text classification
FS is performed by keeping the words with highest score
scheme, and compare four different types of document
according to predetermined measure of the importance of
representations: N-grams, Single terms, phrases and RDR
the word [9].The selected features retains original physi-
which is a logic-based documents representation. The N-
cal meaning and provide a better understanding for the
gram is a string-based representation with no linguistic
data and learning process [11]. For text classification a
processing. The Single term approach is based on words
major problem is the high dimensionality of the feature
with minimum linguistic processing. The phrase approach
space. Almost every text domain has much number of
is based on linguistically formed phrases and single
features, most of these features are not relevant and bene-
words. The RDR is based on linguistic processing and
ficial for text classification task, and even some noise
representing documents as a set of logical predicates. In
features may sharply reduce the classification accuracy
[22] the authors present significantly more efficient in-
[16]. Hence FS is commonly used in text classification to
dexing and classification of large document repositories,
reduce the dimensionality of feature space and improve
e.g. to support information retrieval over all enterprise
the efficiency and accuracy of classifiers.
file servers with frequent file updates.
There are mainly two types of feature selection methods
in machine learning; wrappers and filters. Wrappers use
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 7
TABLE 1. FEATURE SELECTION TECHNIQUES
P (t, c )
∑ ∑ p (t, c ) log
P (t)P (c )
Gain Ration GR (t k , c i ) = c∈{ci,ci} −t∈{tk ∑,tk} P (c ) log P (c )
c∈{ci,ci}
Informational K K K
Gain(IG) IG(w)= −∑ P(c j)log P(c j)+ P(w)∑ P(c j |w)log P(c j |w)+ P(w)∑ P(c j |w)log P(c j |w)
j=1 j=1 j=1
= H(samples )−H(samples |w)
D×(#(c ,f )#(c ,f )−#(c ,f ).#(c ,f ))2
χ2(f c )= j i, j i, j i, j i,
Chi Square i, j (#(#(c ,f )+#(c ,f ))×(#(c ,f )+#(c ,f ))×((c ,f )+#(c ,f ))×(#(c ,f )+#(c ,f ))
j i, j i, j i, j i, j i, j i, j i, j i,
Conditional mu- CMI(C|S)= H(C)−H(C |S ,S ,.....,S )
1 2 n
tual Information
Document Fre- DF(t ) = P(t )
k k
quency(DF)
Term Fre- freq
quency(TF) tf ( f i,d j) = max frei qj
k Kj
Inverse Document D
Frequency(IDF) idf = log # ( f1)
Term s(t) = P (t ∈ y | t ∈ x)
Weighted Ration WOddsRation(w) = P(w)×OddsRatio(w)
Odd Ration P( f |c )(1− P( f | ¬c ))
OddsRatio ( f ,c ) = log i j i j
i j (1− P( f
i
| c j))( P( f
i
| ¬c j)
the concepts of namespace, import, cardinality relation-
C. Semantic and Ontology Base Documents
ship between the classes and enumerated classes. Ontolo-
Representation
gy has been proposed for handling semantically hetero-
This section focused no the semantic, ontology tech- geneity when extracting informational from various text
niques, language and the associated issues for documents sources such as internet [27].
classification. According to [44] the statistical techniques
are not sufficient for the text mining. Better classification Machine learning algorithms automatically builds a clas-
will be performed when consider the semantic under con- sifier by learning the characteristics of the categories
sideration. Ontology is a data model that represents a set from a set of classified documents, and then uses the clas-
of concepts within a domain and the relationships be- sifier to classify documents into predefined categories.
However, these machine learning methods have some
tween those concepts. It is used to reason about the ob-
drawbacks: (1) In order to train classifier, human must
jects within that domain. Ontology is the explicit and
collect large number of training text terms, the process is
abstract model representation of already defined finite
very laborious. If the predefined categories changed,
sets of terms and concepts, involved in knowledge man-
these methods must collect a new set of training text
agement, knowledge engineering, and intelligent informa-
terms. (2) Most of these traditional methods haven't con-
tion integration [23].The characteristics of objects and
sidered the semantic relations between words, so it is dif-
entities (individuals, instances) is a real thing and associa-
ficult to improve the accuracy of these classification me-
tion (relations) with attribute is used for the titles of the
thods [6]. (3) The issue of translatability, between one
two concepts or entities. Ontology is divided into three
natural language into another natural language. These
categories i.e., Natural Language Ontology (NLO), Do-
types of issues identify that machine understanding sys-
main Ontology (DO) and Ontology Instance (OI) [24].
tems are facing problems. Such issues are discussed in the
NLO is the relationship between general lexical tokens of
literature, some of these may be addressed if we have
statements based on natural language, DO is the know-
machine readable ontology [32], and that’s why this is
ledge of a particular domain and OI is the automatically
and important potential area for research.
generated web page behaves like an object. Web Ontolo-
gy Language (OWL) is the ontology support language During the text mining process, ontology can be used to
derived from America DAPRA Agent Markup Language provide expert, background knowledge about a domain.
(DAML) and based on ontology, inference and European Some resent research shows the importance of the domain
Ontology Interchange Language (OIL)[25]. OWL claims ontology in the text classification process, the [27]
to be an extension in Resource Description Framework presents automatic classification of incoming news using
(RDF)[26]. In expressing logical statements because it hierarchical news ontology, based on this classification
not only describe classes and properties but also provides on one hand, and on the users' profiles on the other hand,
© 2010 ACADEMY PUBLISHER8 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
the personalization engine of the system is able to provide TABLE 2. SEMANTIC ISSUES FOR DOCUMENTS CLASSIFICATION
a personalized paper to each user on to her mobile read-
Sentence How we Identifying sentence boundaries in a document.
ing device. A novel ontology-based automatic classifica- Splitting
tion and ranking method is represented in [34] where
Tokenization How the documents are tokenized and tokens are re-
Web documents are characterized by a set of weighted
corded or annotated, by word or phrase. This is impor-
terms, categories are represented by ontology. In [35] the tant because many down stream components need the
authors presented an approach towards mining ontology tokens to be clearly identified for analysis.
from natural language, in which they considered a do-
Part-of- What about the part of speech characteristics and the
main-specific dictionary for telecommunications docu- Speech (pos) data annotation. How such components are assigning a
ments. Tagging pos tag to token pos information.
How to include user context and preferences in the form Stop word How stop word list will be taken, and which words are
list to consider as stop word in which domain.
of an ontology in order to classify unstructured docu-
ments into useful categories and the use of a context- Stemming If we reduce the words to their stems, how it will affect
the meaning of the documents.
based free text interpreter (CFTI) [36], which performs
syntactical analysis and lexical semantic processing of Noisy Data Which steps are required for the document to be clear
sentences, to derive a description of the content of the from noisy data.
unstructured documents, with relevance to the context of
Word Sense How we clarify the meaning of the word in the text,
the user. In [38] the authors presented a novel text catego- ambiguity problem.
rization method based on ontological knowledge that
Collocations What about the compound and technical terms.
does not require a training set. Also an Automatic Docu-
ment Classifier System based on Ontology and the Naïve Syntax How should make a syntactic or grammar analysis.
What about data dependency, anaphoric problems.
Bayes Classifier is proposed in [39].
Text Repre- Which will be more important for representation of the
Ontology’s have shown their usefulness in application sentation documents: Phrases, Word or Concept and Noun or
areas such as knowledge management, bioinformatics, e- adjective? And for this which techniques will be feasible
learning, intelligent information integration [40], infor- to use.
mation brokering [41] and natural-language processing Domain and How to define the area, data availability and its relation
[42]. Now it is the positional and challenging area for text data under- for ontology construction.
classification. standing for
Ontology
Semantic analysis is the process of linguistically parsing
Semantically representation of documents is the challeng-
sentences and paragraphs into key concepts, verbs and
ing area for research in text mining. By proper implanta-
proper nouns. Using statistics-backed technology, these
tion of this will be improve the classification and the in-
words are then compared to taxonomy (categories) and
formation retrieval process.
grouped according to relevance [43]. Better classification
will be performed when consider the semantic under con-
sideration, so the semantically representation of text and III MACHINE LEARNING TECHNIQUES
web document is the key challenge for the documents
The documents can be classified by three ways, un-
classification and knowledge management. Recently
supervised, supervised and semi supervised methods.
many researchers addressed such types of issues.
Many techniques and algorithms are proposed recently
The authors in [45] present the ambiguity issues in natu- for the clustering and classification of electronic docu-
ral language text and present anew technique for resoling ments. This section focused on the supervised classifica-
ambiguity problem in extracting concept/entity from the tion techniques, new developments and highlighted some
text which can improve the document classification of the opportunities and challenges using the existing
process. Multilingual text representation and classifica- literature. The automatic classification of documents into
tion is on of the main and challenging issue in text classi- predefined categories has observed as an active attention,
fication. as the internet usage rate has quickly enlarged. From last
few years , the task of automatic text classification have
In [37] the idea of workflow composition is presented,
been extensively studied and rapid progress seems in this
and addressed the important issues of semantic descrip-
area, including the machine learning approaches such as
tion of such as services for particular text mining task.
Bayesian classifier, Decision Tree, K-nearest neigh-
Moreover, there are other two open problems in text min-
bor(KNN), Support Vector Machines(SVMs), Neural
ing: polysemy, synonymy. Polysemy refers to the fact
Networks, Latent Semantic Analysis, Rocchio’s Algo-
that a word can have multiple meanings. Distinguishing
rithm, Fuzzy Correlation and Genetic Algorithms etc.
between different meanings of a word (called word sense
Normally supervised learning techniques are used for
disambiguation) is not easy, often requiring the context in
automatic text classification, where pre-defined category
which the word appears. Synonymy means that different
labels are assigned to documents based on the likelihood
words can have the same or similar meaning. Some of
suggested by a training set of labelled documents. Some
the natural language issues that should be consider during
of these techniques are described below.
the text mining process shown in overview [46] is listed
below in Table-2.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 9
A. Rocchio’s Algorithm storing the feature vectors and categories of the training
Rocchio’s Algorithm [75] is a vector space method for set. In the classification phase, distances from the new
document routing or filtering in informational retrieval, vector, representing an input document, to all stored vec-
build prototype vector for each class using a training set tors are computed and k closest samples are selected. The
of documents, i.e. the average vector over all training annotated category of a document is predicted based on
document vectors that belong to class ci, and calculate simi- the nearest point which has been assigned to a particular
larity between test document and each of prototype vec- category.
tors, which assign test document to the class with maxi-
mum similarity. k
argmax ∑sim(D |D)*δ(C(D ),i) (2)
i j j
j=1
C
i
=α∗centroid
C i
−β∗centroid
C i (1) Calculate similarity between test document and each
neighbour, and assign test document to the class which
When given a category, the vector of documents belong- contains most of the neighbors. Fig.3.
ing to this category is given a positive weight, and the
vectors of remaining documents are given negative
weight. The positively and negatively weighted vectors,
the prototype vector of this category is obtained.
Relevant
Non Relevant
Relevant
Fig. 3 k-Nearest Neighbor
Optimal
Non Relevant
T plh ei ms em ne t.t h Aod
s
i cs
o
e mff pe ac rt ei v te o, n Ro on
c
cp ha ir oa m ae lgtr oic
ri
ta hn md e mas oy
re
t o
l
oi cm a-
l
Fig .2 Rocchio Optimal query for separating relevant and non relevant characteristics of documents are considered, however the
documents classification time is long and difficult to find optimal
This algorithm [61] is easy to implement, efficient in value of k. i.e., to analyze the k-NN and the Rocchio al-
computation, fast learner and have relevance feedback gorithm, some shortcomings of each are identified in
mechanism but low classification accuracy. Linear com- [56]. A new algorithm is proposed in [67] which incorpo-
bination is too simple for classification and constant α rating the relationship of concept-based thesauri into doc-
and β are empirical. This is a widely used relevance feed- ument categorization using a k-NN classifier, while [60]
back algorithm that operates in the vector space model presents the use of phrases as basic features in the email
[76]. The researchers have used a variation of Rocchio’s classification problem and performed extensive empirical
algorithm in a machine learning context, i.e., for learning evaluation using large email collections and tested with
a user profile from unstructured text [77] [78], the goal in three text classification algorithms, namely, a naive
these applications is to automatically induce a text clas- Bayes classifier and two k-NN classifiers using TF- IDF
sifier that can distinguish between classes of documents. weighting and resemblance respectively. The k-nearest
neighbor classification method is outstanding with its
B. K-nearest neighbor (k-NN) simplicity and is widely used techniques for text classifi-
The k-nearest neighbor algorithm (k-NN) [66] is used to cation. This method performs well even in handling the
test the degree of similarity between documents and k classification tasks with multi-categorized documents.
training data and to store a certain amount of classifica- The major drawback of this method is it uses all features
tion data, thereby determining the category of test docu- in distance computation, and causes the method computa-
ments. This method is an instant-based learning algorithm tionally intensive, especially when the size of training set
that categorized objects based on closest feature space in grows. Besides, the accuracy of k-nearest neighbor classi-
the training set [62]. The training sets are mapped into fication is severely degraded by the presence of noisy or
multi-dimensional feature space. The feature space is irrelevant features.
partitioned into regions based on the category of the train-
ing set. A point in the feature space is assigned to a par- C. Decision Tree
ticular category if it is the most frequent category among
The decision tree rebuilds the manual categorization of
the k nearest training data. Usually Euclidean Distance is
training documents by constructing well-defined
typically used in computing the distance between the vec-
true/false-queries in the form of a tree structure. In a deci-
tors. The key element of this method is the availability of
sion tree structure, leaves represent the corresponding
a similarity measure for identifying neighbors of a partic-
category of documents and branches represent conjunc-
ular document [62]. The training phase consists only of
tions of features that lead to those categories. The well-
© 2010 ACADEMY PUBLISHER10 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
organized decision tree can easily classify a document by size of rules set without affecting the performance of the
putting it in the root node of the tree and let it run through classification. The [49] presents a hybrid method of rule-
the query structure until it reaches a certain leaf, which based processing and back-propagation neural networks
represents the goal for the classification of the document. for spam filtering, Instead of using keywords, this study
utilize the spamming behaviours as features for describ-
ing emails.
The main advantage of the implementation of decision
rules method for classification tasks is the construction of
local dictionary for each individual category during the
feature extraction phase [64]. Local dictionaries are able
to distinguish the meaning of a particular word for differ-
ent categories. However, the drawback of the decision
rule method is the impossibility to assign a document to a
Fig. 4 Decision Tree category exclusively due to the rules from different rule
sets is applicable to each other. Besides, the learning and
The decision tree classification method is outstanding
updating of decision rule methods need extensive in-
from other decision support tools with several advantag-
volvement of human experts to construct or update the
es. The main advantage of decision tree is its simplicity in
rule sets. Like the decision trees classification method,
understanding and interpreting, even for non-expert users.
the decision rules method does not work well when the
Besides, the explanation of a given result can be easily
number of distinguishing features is large.
replicated by using simple mathematics algorithms, and
provide a consolidated view of the classification logic, E. Naïve Bayes Algorithm
which is a useful information of classification.
Naïve Bayes classifier is a simple probabilistic classifier
It can be shown experimentally that text classification based on applying Bayes’ Theorem with strong indepen-
tasks frequently involve a large number of relevant fea- dence assumptions. A more descriptive term for the un-
tures [79]. Therefore, a decision tree’s tendency to base derlying probability model would be independent feature
classifications on as few tests as possible can lead to poor model. These independence assumptions of features make
performance on text classification. However, when there the features order is irrelevant and consequently that the
are a small number of structured attributes, the perfor- present of one feature does not affect other features in
mance, simplicity and understandability of decision trees classification tasks [99]. These assumptions make the
for content-based models are all advantages. The [80] computation of Bayesian classification approach more
describe an application of decision trees for personalizing efficient, but this assumption severely limits its applica-
advertisements on web pages. bility. Depending on the precise nature of the probability
model, the naïve Bayes classifiers can be trained very
The major risk of implementing a decision tree is it over
efficiently by requiring a relatively small amount of train-
fits the training data with the occurrence of an alternative
ing data to estimate the parameters necessary for classifi-
tree that categorizes the training data worse but would
cation. Because independent variables are assumed, only
categorize the documents to be categorized better [63].
the variances of the variables for each class need to be
This is due to the classification algorithm of decision tree
determined and not the entire covariance matrix.
is made to categorize training data effectively, however
neglect the performance of classifying other documents. Due to its apparently over-simplified assumptions, the
Besides, huge and excessively complex structure of tree naïve Bayes classifiers often work much better in many
is built from a dataset with very large number of entries. complex real-world situations than one might expect. The
naïve Bayes classifiers has been reported to perform sur-
D. Decision Rules Classification prisingly well for many real world classification applica-
tions under some specific conditions [100] [101] [102]
Decision rules classification method uses the rule-based
[103] [104].
inference to classify documents to their annotated catego-
ries [64] [65]. The algorithms construct a rule set that An advantage of the naïve Bayes classifier is that it re-
describe the profile for each category. Rules are typically quires a small amount of training data to estimate the pa-
constructed in the format of “IF condition THEN conclu- rameters necessary for classification. Bayesian classifica-
sion”, where the condition portion is filled by features of tion approach arrives at the correct classification as long
the category, and the conclusion portion is represented as the correct category is more probable than the others.
with the category’s name or another rule to be tested. The Category’s probabilities do not have to be estimated very
rule set for a particular category is then constructed by well. In other words, the overall classifier is robust
combining every separate rule from the same category enough to ignore serious deficiencies in its underlying
with logical operator, typically use “and” and “or”. Dur- naïve probability model.
ing the classification tasks, not necessarily every rule in
The main disadvantage of the naïve Bayes classification
the rule set needs to be satisfied. In the case of handling a
approach is its relatively low classification performance
dataset with large number of features for each category,
compare to other discriminative algorithms, such as the
heuristics implementation is recommended to reduce the
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 11
SVM with its outperformed classification effectiveness. architectures [106] [107]. These elements, namely artifi-
Therefore, many active researches have been carried out cial neuron are interconnected into group using a mathe-
to clarify the reasons that the naïve Bayes classifier fails matical model for information processing based on a con-
in classification tasks and enhance the traditional ap- nectionist approach to computation. The neural networks
proaches by implementing some effective and efficient make their neuron sensitive to store item. It can be used
techniques [100] [102] [103] [104] [105]. for distortion tolerant storing of a large number of cases
represented by high dimensional vectors.
P(c )P(D|c ) Different types of neural network approaches have been
(4)
P(c |D)= i i implemented to document classification tasks. Some of
i P(D)
the researches use the single-layer perceptron, which con-
tains only an input layer and an output layer due to its
n simplicity of implementing [108]. Inputs are fed directly
P(D|c )=∏P(d |c ) (3)
to the outputs via a series of weights. In this way it can be
i j i
j=1 considered the simplest kind of feed-forward network.
The multi-layer perceptron which is more sophisticated,
which consists of an input layer, one or more hidden lay-
Where P(Ci)= P ( C = c i ) = N N i ers, and an output layer in its structure, also widely im-
plemented for classification tasks [106].
and P(dj|ci) = P(d j|c i)=
1+ MN
ji
M+∑N
ki
k=1
Naïve Bayes has been one of the popular machine learn-
ing methods for many years. Its simplicity makes the
framework attractive in various tasks and reasonable per-
formances are obtained in the tasks although this learning
is based on an unrealistic independence assumption. For
this reason, there also have been many interesting works Fig. 5 Artificial Neural Network
of investigating naive Bayes. Recently the [83] shows
The main advantage of the implementation of artificial
very good results by selecting Naïve Bayes with SVM for
neural network in classification tasks is the ability in han-
text classification also the authors in [84] prove that
dling documents with high-dimensional features, and
Naive Bayes with SOM give very good results in cluster-
documents with noisy and contradictory data. Further-
ing the documents. The authors in [85] propose a Poisson
more, linear speed up in the matching process with re-
Naive Bayes text classification model with weight-
spect of the large number of computational elements is
enhancing method, and shows that the new model as-
provided by a computing architecture which is inherently
sumes that a document is generated by a multivariate
parallel, where each element can compare its input value
Poisson model. They suggest per-document term fre-
against the value of stored cases independently from oth-
quency normalization to estimate the Poisson parameter,
ers [107].
while the traditional multinomial classifier estimates its
parameters by considering all the training documents as a The drawback of the artificial neural networks is their
unique huge training document. The [86] presented that high computing cost which consumes high CPU and
naive Bayes can perform surprisingly well in the classifi- physical memory usage. Another disadvantage is that the
cation tasks where the probability itself calculated by the artificial neural networks are extremely difficult to under-
naive Bayes is not important. The authors in a review stand for average users. This may negatively influence
[87] described that researcher shows great interest in the acceptance of these methods.
naïve Bayes classifier for spam filtering. So this tech-
In recent years, neural network has been applied in doc-
nique is most widely used in email, web contents, and
ument classification systems to improve efficiency. Text
spam categorization.
categorization models using back-propagation neural
Naive Bayes work well on numeric and textual data, easy network (BPNN) and modified back-propagation neural
to implement and computation comparing with other al- network (MBPNN) are proposed in [54] for documents
gorithms, however conditional independence assumption classification. An efficient feature selection method is
is violated by real-world data and perform very poorly used to reduce the dimensionality as well as improve the
when features are highly correlated and does not consider performance. New Neural network based document clas-
frequency of word occurrences. sification method [68], was presented, which is helpful
for companies to manage patent documents more effec-
F. Artificial Neural Network tively.
Artificial neural networks are constructed from a large The ANN can get Inputs xi arrives through pre-synaptic
number of elements with an input fan order of magni- connections, Synaptic efficacy is modelled using real
tudes larger than in computational elements of traditional
© 2010 ACADEMY PUBLISHER12 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
weights wi and the response of the neuron is a nonlinear netic algorithm. In the experimental analysis, they show
function f of its weighted inputs. that the improved method is feasible and effective for text
classification.
The output from neuron j for pattern p is Opj where
I. Support Vector Machine (SVM)
1
Support vector machines (SVMs) are one of the discri-
O ( net ) =
pj j 1 + e −λ net j (5) minative classification methods which are commonly
recognized to be more accurate. The SVM classification
method is based on the Structural Risk Minimization
and
principle from computational learning theory [109]. The
idea of this principle is to find a hypothesis to guarantee
net = bias *W +∑O W (6) the lowest true error. Besides, the SVM are well-founded
j bias pk jk
k that very open to theoretical understanding and analysis
[110].
Neural network for document classification produce good The SVM need both positive and negative training set
results in complex domains and suitable for both discrete which are uncommon for other classification methods.
and continuous data (especially better for the continuous These positive and negative training set are needed for
domain). Testing is very fast however training is relative- the SVM to seek for the decision surface that best sepa-
ly slow and learned results are difficult for users to interp- rates the positive from the negative data in the n-
ret than learned rules (comparing with Decision tree), dimensional space, so called the hyper plane. The docu-
Empirical Risk Minimization (ERM) makes ANN try to ment representatives which are closest to the decision
minimize training error, may lead to overfitting. surface are called the support vector. The performance of
the SVM classification remains unchanged if documents
G. Fuzzy correlation that do not belong to the support vectors are removed
from the set of training data [99].
Fuzzy correlation can deal with fuzzy information or in-
complete data, and also convert the property value into
fuzzy sets for multiple document classification [69].
In [55] the authors explores the challenges of multi-class
text categorization using one-against-one fuzzy support
vector machine with Reuter’s news as the example data,
and shows better results using one-against-one fuzzy sup-
port vector machine as a new technique when compare
with one-against-one support vector machine. [61] pre-
sented the improvement of decision rule and design a new
algorithm of f-k-NN (fuzzy k-NN) to improve categoriza-
tion performance when the class distribution is uneven,
and show that the new method is more effective. So the
researchers shows great interest recently to use the fuzzy
rules and sets to improve the classification accuracy, by
incorporating the fuzzy correlation or fuzzy logic with the
Fig. 6 Illustration of optimal separating hyper plane, hyper planes and
machine learning algorithm and the feature selection me-
support vectors
thods to improve the classification process.
H. Genetic Algorithm
X
Genetic algorithm [81] aims to find optimum characteris-
tic parameters using the mechanisms of genetic evolution
and survival of the fittest in natural selection. Genetic
algorithms make it possible to remove misleading judg- f
x f
ments in the algorithms and improve the accuracy of doc-
ument classification. This is an adaptive probability glob-
0 f
f
x
al optimization algorithm, which simulated in a natural 0 x f
environment of biological and genetic evolution, and is 0 f f
x f
widely used for their simplicity and strength. Now several 0
researchers used this method for the improvement of the
X F
text classification process. In authors in [82] introduced
the genetic algorithm to text categorization and used to
build and optimize the user template, and also introduced Fig. 7 Mapping non linear input space onto high dimensional space
simulated annealing to improve the shortcomings of ge-
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 13
The SVM classification method is outstanding from the An optimal SVM algorithm via multiple optimal strate-
others with its outstanding classification effectiveness gies is developed in [47], such as a novel importance
[99] [111] [112] [110] [113] [70]. Furthermore, it can weight definition, the feature selection using the entropy
handle documents with high-dimensional input space, and weighting scheme, the optimal parameter settings. The
culls out most of the irrelevant features. However, the SVM is a best technique for the documents classification
major drawback of the SVM is their relatively complex [83].
training and categorizing algorithms and also the high
time and memory consumptions during training stage and IV HYBRID TECHNIQUES
classifying stage. Besides, confusions occur during the
Many new hybrid methods and techniques are proposed
classification tasks due to the documents could be a no-
recently in the area of Machine Learning and text mining.
tated to several categories because of the similarity is
The concept of combining classifiers is proposed as a
typically calculated individually for each category [99].
new direction for the improvement of the performance of
So SVM is supervised learning method for classification individual classifiers. Recently many methods have been
to find out the linear separating hyperplane which maxim- suggested for the creation of ensemble of classifiers. Me-
ize the margin, i.e., the optimal separating hyperplane chanisms that are used to build ensemble of classifi-
(OSH) and maximizes the margin between the two data ers[114] include: i) Using different subset of training data
sets. To calculate the margin, two parallel hyperplanes are with a single learning method, ii) Using different training
constructed, one on each side of the separating hyper- parameters with a single training method (e.g. using dif-
plane, which are "pushed up against" the two data sets. ferent initial weights for each neural network in an en-
Intuitively, a good separation is achieved by the hyper- semble) and iii) Using different learning methods.[88].
plane that has the largest distance to the neighboring data
The benefits of local versus global feature sets and local
points of both classes, since in general the larger the mar-
versus global dictionaries in text categorization have ex-
gin the lower the generalization error of the classifier.
amined in [121]. Local features are class dependent fea-
Maximizing the margin is equivalent to tures while global features are class independent features.
Local dictionaries are class dependent dictionaries while
1 global dictionaries are class independent dictionaries.
∑
minimize
wTw+C( Nζ ) The best text categorization is obtained using local fea-
2 i=1 i tures and local dictionaries [121].
wbζ (9)
i A New hybrid text document classification approach is
,
subject to y(wTx −b)+ζ −1≥0, 1≤i≤N proposed in [83], used naive Bayes method at the front
i i i end for raw text data vectorization, in conjunction with a
ζ ≥0, 1≤i≤N SVM classifier at the back end to classify the documents
to the right category. They shows that the proposed hybr-
i
id approach of the Naive Bayes vectorizer and SVM clas-
Introducing Lagrange multipliers α , β , the Lagran- sifier has improved classification accuracy compared to
gian is: the pure naive Bayes classification approach. The [84]
presents another hybrid method of naïve Bayes with self
1 N
l(w,b,ζi;α,β) = 2wTw+C∑ζi organizing map (SOM). Proposed Bayes classifier used at
N
i=1
N the front end, while SOM performs the indexing steps to
−∑αi[y i(wTx i−b)+ζi−1]−∑µiζi
retrieve the best match cases.
i=1 i=1
=
1 2wTw+∑N
(C−αi−µi)ζi
(10)
So In the context of combining multiple classifiers for
i=1
−⎜ ⎝⎛ ∑ iN =1αiy ix iT⎟ ⎠⎞w−⎜ ⎝⎛ ∑ iN =1αiy i⎟ ⎠⎞b+∑ iN =1αi t te hx at
t
c ca ot meg bo inri iz na gt i do in f,
f
ea
r
en nu
t
m cb lae sr
s
io fif
e
r re
s
s ce aa nrc h imer ps
r
oh va ev e
c
ls ah so siw fin
-
cation accuracy [89]. It is observed from the Comparison
between the best individual classifier and the combined
The authors in [50] implemented and measured the per-
method, that the performance of the combined method is
formance of the leading supervised and unsupervised
superior [90] [91] [92].
approaches for multilingual text categorization; they se-
lected support vector machines (SVM) as representative A hybrid method is proposed in [93] in which the learn-
of supervised techniques as well as latent semantic index- ing phase evaluation back propagation neural network
ing (LSI) and self-organizing maps (SOM) techniques for (LPEBP) to improve the traditional BPNN. And adopt
unsupervised methods for system implementation. In [52] singular value decomposition (SVD) technique to reduce
the authors analyses and compares SVM ensembles with the dimension and construct the latent semantics between
four different ensemble constructing techniques, namely terms, and show that the LPEBP is much faster than the
bagging, AdaBoost, Arc-X4 and a modified AdaBoost. traditional BPNN, which enhances the performance of the
Twenty real-world data sets from the UCI repository are traditional BPNN. The SVD technique cannot only great-
used as benchmarks to evaluate and compare the perfor- ly reduce the high dimensionality but also enhance the
mance of these SVM ensemble classifiers by their classi- performance. So SVD is to further improve the document
fication accuracy. classification systems precisely and efficiently.
© 2010 ACADEMY PUBLISHER14 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
The [94] a new hybrid technique for text classification is In [120] the combination of similarity-based learning
proposed that requires less training data and less compu- algorithms and associated thresholding strategies signifi-
tational time and show that text classification that re- cantly influences the overall performance of text classifi-
quires fewer documents for training instead of using cation. After investigating two similarity-based classifiers
words, word relation i.e. association rules from these (k-NN and Rocchio) and three common thresholding
words is used to derive feature set from pre-classified techniques (RCut, PCut, and SCut), they described a new
text documents. The concept of Naïve Bayes classifier is learning algorithm known as the keyword association
then used on derived features and finally only a single network (KAN) and a new thresholding strategy (RinS-
concept of genetic presented has been added for final Cut) to improve performance over existing techniques,
classification. and shows that the new approaches give better results.
In[55] the authors explores the challenges of multi-class A new machine learning method is proposed for con-
text categorization using one-against-one fuzzy support structing ranking models in document retrieval [57]. The
vector machine with reuter’s news as the example data, method, aims to use the advantages of both the traditional
and shows better results using one-against-one fuzzy sup- Information Retrieval (IR) methods and the supervised
port vector machine as a new technique when compare learning methods for IR proposed recently.
with one-against-one support vector machine.
The main concern of authors in [58] is to investigate the
A hybrid algorithm is proposed in [56], based on variable effectiveness of using multi-words for text representation
precision rough set to combine the strength of both k-NN on the performances of text classification. Firstly, a prac-
and Rocchio techniques to improve the text classification tical method is proposed to implement the multi-word
accuracy and overcome the weaknesses of Rocchio algo- extraction from documents based on the syntactical struc-
rithm. ture. Secondly, two strategies as general concept repre-
sentation and subtopic representation are presented to
The authors in [95] suggest a new hybrid approach to web
represent the documents using the extracted multi-words.
document classification built upon both, graph and vector
The proposed method launches in [59] for text classifica-
representations. K-NN algorithm shows that the proposed
tion tasks with only unlabeled documents and the title
graph and vector approaches performing better in terms
word of each category for learning, and then it automati-
of classification accuracy along with a significant reduc-
cally learns text classifier by using bootstrapping and
tion in classification time.
feature projection techniques.
The [96] proposed two methods to modify the standard
BPNN and adopt the semantic feature space (SFS) me- V COMPARATIVE STUDY
thod to reduce the number of dimensions as well as con-
The growing phenomenon of the textual data needs text
struct latent semantics between terms, and show that the
mining, machine learning and natural language
modified methods enhanced the performance of the stan-
processing techniques and methodologies to organize and
dard BPNN and were more efficient than the standard
extract pattern and knowledge from the documents. This
BPNN. The SFS method cannot only greatly reduce the
review focused on the existing literature and explored the
dimensionality, but also enhances performance and can
documents representation and classification techniques.
therefore be used to further improve text classification
Text representation is a crucial issue. Most of the litera-
systems precisely and efficiently.
ture gives the statistical of syntactic solution for the text
The [97] presents a semi-supervised learning method representation. However the representation model depend
SSRANK for classification task. It leverages the uses of on the informational that we require. Concept base or
both labelled data and unlabeled data, utilizes views from semantically representations of documents require more
both traditional IR and supervised learning to conduct attention.
data labelling, and relies on a criterion to control the
The performance of a classification algorithm in data
process of data labelling.
mining is greatly affected by the quality of data source.
A new algorithm of f-k-NN (fuzzy k-NN) proposed in Irrelevant and redundant features of data not only in-
[98] for the improvement of decision rule and design to crease the cost of mining process, but also degrade the
improve classification performance when the class distri- quality of the result in some cases [71]. Each algorithm
bution is uneven, and show that the new method is more has its own advantages and disadvantages as described in
effective. The approach of [48] is a nontrivial extension section II and III.
of document classification methodology from a fixed set
However, in [6] the author compare the different text
of classes to a knowledge hierarchy like Gene Ontology.
classification techniques and have to bear in mind that
In [51], the authors proposed a new approach to automat- comparisons are reliable only when based on experiments
ic discovery of implicit rhetorical information from texts performed by the same author under carefully controlled
based on evolutionary computation methods. In order to conditions. They are instead more problematic when they
guide the search for rhetorical connections from natural- involve different experiments performed by different au-
language texts. And in [53], the authors present a seg- thors. In this case various “background conditions,” often
mentation methodology of handwritten documents in extraneous to the learning algorithm itself may influence
their distinct entities, namely, text lines and words. the results. These may include, among others, different
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 15
choices in pre-processing (stemming, etc.), indexing, di- In [118] compression on four machine learning algo-
mensionality reduction and classifier parameter values rithms, which are Naive Bayesian (NB), neural network
etc. (NN), support vector machine (SVM) and relevance vec-
tor machine (RVM), are proposed for spam classification.
A performance compression in [115] presented a con-
An empirical evaluation for them on the benchmark spam
trolled study on a large number of filter feature selection
filtering corpora is presented. The experiments are per-
methods for text classification. Over 100 variants of five
formed based on different training set size and extracted
major feature selection criteria were examined using four
feature size. Experimental results show that NN classifier
well-known classification algorithms: Naive Bayesian
is unsuitable for using alone as a spam rejection tool.
(NB) approach, Rocchio-style classifier, k-NN method
Generally, the performances of SVM and RVM classifi-
and SVM system. Two benchmark collections were cho-
ers are obviously superior to NB classifier. Compared
sen as the testbeds: Reuters-21578 and small portion of
with SVM, RVM is shown to provide the similar classifi-
Reuters Corpus Version 1 (RCV1), making the new re-
cation result with less relevance vectors and much faster
sults comparable to published results. They presents that
testing time despite the slower learning procedure, they
feature selection methods based on χ2 statistics consis-
show that RVM is more suitable than SVM for spam
tently outperformed those based on other criteria (includ-
classification in terms of the applications that require low
ing information gain) for all four classifiers and both data
complexity.
collections, and that a further increase in performance
was obtained by combining uncorrelated and high- In [119] email data was classified using four different
performing feature selection methods. The results they classifiers (Neural Network, SVM classifier, Naïve Baye-
obtained using only 3% of the available features are sian Classifier, and J48 classifier). The experiment was
among the best reported, including results obtained with performed based on different data size and different fea-
the full feature set. The empirical results of their study ture size. The final classification result should be ‘1’ if it
suggest that using filter methods which include the χ2 is finally spam, otherwise, it should be ‘0’. This paper
statistic, combining them with DF or IG, and eliminating shows that simple J48 classifier which make a binary tree,
the rare words. Such methods were consistently better. could be efficient for the dataset which could be classi-
fied as binary tree.
In [116] the authors discussed, that some studies com-
pared feature selection techniques or feature space trans- The [120] shows that two main research areas in statistic-
formation whereas some others compared the perfor- al text categorization are: similarity-based learning algo-
mance of different algorithms. Recently the rising interest rithms and associated thresholding strategies. The combi-
towards the Support Vector Machine, various studies nation of these techniques significantly influences the
showed that SVM outperforms then other classification overall performance of text categorization. After investi-
algorithms. So should we just not problem about other gating two similarity-based classifiers (k-NN and Roc-
classification algorithms and opt always for SVM? They chio) and three common thresholding techniques (RCut,
have decided to investigate this issue and compared SVM PCut, and SCut), they described a new learning algorithm
to k-NN and naive Bayes on binary classification tasks. known as the keyword association network (KAN) and a
An important issue is to compare optimized versions of new thresholding strategy (RinSCut) to improve perfor-
these algorithms; from their results it shows all the clas- mance over existing techniques. Extensive experiments
sifiers achieved comparable performance on most prob- have been conducted on the Reuters-21578 and 20-
lems. One surprising result is that SVM was not a clear Newsgroups data sets, and shows that the new approaches
winner, despite quite good overall performance. If a suit- give better results.
able pre-processing is used with k-NN, this algorithm
Comparing with ANN, SVM capture the inherent charac-
continues to achieve very good results and scales up well
teristics of the data better and embedding the Structural
with the number of documents, which is not the case for
Risk Minimization (SRM) principle which minimizes the
SVM. As for Naive Bayes, it also achieved good perfor-
upper bound on the generalization error (better than the
mance.
Empirical Risk Minimization principle) also ability to
The [117] deals with the performance of different classi- learn can be independent of the dimensionality of the
fication algorithms and the impact of feature selection feature space and global minima vs. local minima, How-
algorithm on Logistic Regression Classifier, How it con- ever there are some difficulties in parameter tuning and
trols False Discovery Rate (FDR) and thus improves the kernel selection.
efficiency of Logistic Regression classifier. As per the
analysis support vector machine has more parameters
VI DISCUSSION AND CONCLUSIONS
than logistics regression and decision tree classifier, SVM
has the highest classification precision most of the time, This paper provides a review of machine learning ap-
however SVM is very time consuming because of more proaches and documents representation techniques. An
parameters, demands more computation time. Compared analysis of feature selection methods and classification
to SVM, logistic regression is computationally efficient. algorithms were presented. It was verified from the study
Its results usually have static meaning. However it does that information Gain and Chi square statistics are the
not perform well when data set exhibits explicit data most commonly used and well performed methods for
structures. feature selection, however many other FS methods are
© 2010 ACADEMY PUBLISHER16 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
proposed as single or hybrid technique recently, shown not the case for SVM [122] [123]. As for naive Bayes, it
good results, and needs more exploration for efficient also achieved good performance with suitable pre-
classification process. Several algorithms or combination processing. k-NN algorithm performed well as more local
of algorithms as hybrid approaches was proposed for the characteristic of documents are considered, however the
automatic classification of documents, among these algo- classification time is long and difficult to find optimal
rithms, SVM, NB and kNN classifiers are shown most value of k.
appropriate in the existing literature.
More works are required for the performance improve-
Most researchers in text classification assume the docu- ment and accuracy of the documents classification
ments representation as a Bag of Word (BOG), although process. New methods and solutions are required for use-
according to [44] the statistical techniques are not suffi- ful knowledge from the increasing volume of electronics
cient for the text mining. Text representation is a crucial documents. The following are the some of opportunities
issue. Most of the literature gives the statistical of syntac- of the unstructured data classification and knowledge
tic solution for the text representation. However the re- discovery.
presentation model depend on the informational that we
require. Concept base or semantically representation of • To improve and explore the feature selection methods
for better classification process.
documents requires more research. Better classification
will be performed when consider the semantic under con- • To reduce the training and testing time of classifier and
siderations, semantically and ontology base documents improve the classification accuracy, precision and re-
representation opportunities were discussed in this paper. call.
With the addition of the ontology and semantic to
represent the documents will be more improve accuracy • For Spam filtering and e-mail categorization the user
and the classification process. So the identification of may have folders like electronic bills, e-mail from
features that capture semantic content is one of the impor- family, friends and so on, and may want a classifier to
tant areas for research. The general multiple learning is- classify each incoming e-mail that’s automatically
sues in the presence of noise is a tremendously challeng- move it to the appropriate folder. It is easier to find
ing problem that is just now being formulated and will messages in sorted folders in a very large inbox.
likely require more work in order to successfully develop
• Automatic allocation of folders to the downloaded ar-
strategies to find the underlying nature of the manifold.
ticles, documents from text editors and from grid net-
work.
Several algorithms or combination of algorithms as hybr- • The use of semantics and ontology for the documents
id approaches were proposed for the automatics classifi- classification and informational retrieval.
cation of documents. Among these algorithms, SVM, NB
• Mining trend, i.e. marketing, business, and financial
, kNN and their hybrid system with the combination of
trend (stock exchange trend) form e-documents (Online
different other algorithms and feature selection tech-
news, stories, views and events).
niques are shown most appropriate in the existing litera-
ture. However the NB is perform well in spam filtering • Stream text quire some new techniques and methods for
and email categorization, requires a small amount of information management.
training data to estimate the parameters necessary for
classification. Naive Bayes works well on numeric and • Automatic classification and analysis of sentiment,
views and extraction knowledge from it. The senti-
textual data, easy to implement comparing with other
ments and opinion mining is the new active area of text
algorithms, however conditional independence assump-
mining.
tion is violated by real-world data and perform very poor-
ly when features are highly correlated and does not con- • Classification and clustering of semi-structured docu-
sider frequency of word occurrences. ments have some challenges and new opportunities.
SVM classifier has been recognized as one of the most
• An implementation of sense-based text classification
effective text classification method in the comparisons of
procedure is needed for recovering the senses from the
supervised machine learning algorithms [74]. SVM cap-
words used in a specific context.
ture the inherent characteristics of the data better and em-
bedding the Structural Risk Minimization (SRM) prin- • Informational extraction of useful knowledge from e-
ciple which minimizes the upper bound on the generaliza- documents and Web pages, such as products and search
tion error (better than the Empirical Risk Minimization results to get meaning full patterns.
principle) also ability to learn can be independent of the
• To identify or match semantically similar data from the
dimensionality of the feature space and global minima vs.
web (that contain huge amount of data and each web-
local minima, however, the SVM has been found some
site represents similar information differently) is an im-
difficulties in parameter tuning and kernel selection.
portant problem with many practical applications. So
If a suitable pre-processing is used with k-NN, then this web information, integration and schema matching
algorithm continues to achieve very good results and needs more exploration.
scales up well with the number of documents, which is
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 17
REFERENCES [20] E. Youn, M. K. Jeong , “Class dependent feature scaling
method using naive Bayes classifier for text datamining”
[1] A. Dasgupta, “Feature selection methods for text classifica-
Pattern Recognition Letters , 2009.
tion.”, In Proceedings of the 13th ACMSIGKDD interna-
[21] G. Forman, E. Kirshenbaum, “Extremely Fast Text Feature
tional conference on Knowledge discovery and data min-
Extraction for Classification and Indexing”, Napa Valley
ing, pp. 230 -239, 2007.
California, USA. CIKM’08, October 26–30, 2008
[2] Raghavan, P., S. Amer-Yahia and L. Gravano eds., “Struc-
[22] Mostafa Keikha, Ahmad Khonsari, Farhad Oroumchian, “
ture in Text: Extraction and Exploitation.” In. Proceeding
Rich document representation and classification: An analy-
of the 7th international Workshop on the Web and Data-
sis” , Knowledge-Based Systems 22 , pp.67–71, 2009.
bases(WebDB), ACM SIGMOD/PODS 2004, ACM Press,
[23] D.Fensel, “Ontologies: Silver Bullet for Knowledge Man-
Vol 67, 2004.
agement and e-Commerce”, Springer Verlag, Berlin, 2000.
[3] Oracle corporation, WWW,oracle.com, 2008.
[24] B. Omelayenko., “learning og ontologies for the Web: the
[4] Merrill lynch, Nov.,2000. e-Business Analytics: Depth
analysis of existent approaches”, in the proceeding of the
Report. 2000.
International Workshop on Web Dynamics, 2001.
[5] Pegah Falinouss “Stock Trend Prediction using News Ar-
[25] OWL Web Ontology Language, viewed March 2008
ticle’s: a text mining approach” Master thesis -2007.
http://www.w3.org/TR/owl-features.
[6] Sebastiani, F., “Machine learning in automated text catego-
[26] Sean B. Palmer, “The Semantic Web, an introduction”,
rization” ACM Computing Surveys (CSUR) 34, pp.1 – 47,
2007.
2002.
[27] Lena Tenenboim, Bracha Shapira, Peretz Shoval “Ontolo-
[7] Andreas Hotho “A Brief Survey of Text Mining” 2005.
gy-Based Classification Of News In An Electronic News-
[8] Shang, W., Huang, H., Zhu, H., Lin, Y., Qu, Y., and Wang
paper” International Conference "Intelligent Information
Z., “ A Noval Feature Selection Algorithm for text catogo-
and Engineering Systems" INFOS 2008, Varna, Bulgaria,
rization.” Elsevier, science Direct Expert system with ap-
June-July 2008.
plication -2006, 33(1), pp.1-5, 2006.
[28] Lewis, D.D., “Naive (Bayes) at forty The independence
[9] Montanes,E., Ferandez, J., Diaz, I., Combarro, E.F and
assumption in information retrieval”, ECML-98, 10th Eu-
Ranilla, J., “ Measures of Rule Quality for Feature Selec-
ropean Conference on Machine Learning, Chemnitz, DE -
tion in Text Categorization”, 5th international Symposium
1998.
on Intelligent data analysis , Germeny-2003, Springer-
[29] A. Rafael Calvo, Jae-Moon Lee, Xiabo Li, ‘Managin con-
Verlag 2003, Vol2810, pp.589-598, 2003.
tent with automatic document classification”, Journal of
[10] Wang, Y., and Wang X.J., “ A New Approach to feature
Digital Information, 5(2) , Article No.282,2004.
selection in Text Classification”, Proceedings of 4th Inter-
[30] S. Chakrabarti, S. Roy, M. Soundalgekar, “Fast and accu-
national Conference on Machine Learning and Cybernet-
rate text classification via multiple linear discriminant pro-
ics, IEEE- 2005, Vol.6, pp. 3814-3819, 2005.
jections”, International Journal on Very Large Data Bases
[11] Liu, H. and Motoda, ., “Feature Extraction, constraction
12 (2), pp.170–185, 2003.
and selection: A Data Mining Perpective.”, Boston, Massa-
[31] Deerwester, S., Dumais, S., Furnas, G.W., Landauer, T.K.,
chusetts(MA): Kluwer Academic Publishers.
Harshman, R; “Indexing by Latent Semantic Analysis”.
[12] Lee, L.W., and Chen, S.M., “New Methods for Text Cate-
Journal of the Society for Information Science -1990 41 pp.
gorizationBased on a New Feature Selection Method a and
391-407, 1990.
New Similarity Measure Between Documents”, IEA/AEI,
[32] Mu-Hee Song, Soo-Yeon Lim, Dong-Jin Kang, and Sang-
France 2006.
Jo Lee, "Automatic Classification of Web pages based on
[13] Manomaisupat, P., and Abmad k., “ Feature Selection for
the Concept of Domain Ontology", Proc. of the 12th Asia-
text Categorization Using Self Orgnizing Map”, 2nd Inter-
Pacific Software Engineering Conference, 2005.
national Conference on Neural Network and Brain,
[33] Guiyi Wei, Jun Yu, Yun Ling, and Jun Liu, "Design and
2005,IEEE press Vol 3, pp.1875-1880, 2005.
Implementation of an Ontology Algorithm for Web Docu-
[14] Yan, J., Liu, N., Zhang, B., Yan, S., Chen, Z., Cheng, Q.,
ments Classification", ICCSA 2006, LNCS 3983, pp. 649-
Fan, W., and Ma, W., “OCFS: Optimal Orthogonal centro-
658. 2006.
id Feature selection for Text Categorization.” 28 Annual
[34] Jun Fang, Lei Guo, XiaoDong Wang and Ning Yang “On-
International conference on Reserch and Informational re-
tology-Based Automatic Classification and Ranking for
terival, ACM SIGIR, Barizal, , pp.122-129, 2005.
Web Documents” Fourth International Conference on
[15] Zi-Qiang Wang, Xia Sun, De-Xian Zhang, Xin Li “An
Fuzzy Systems and Knowledge Discovery -FSKD -2007.
Optimal Svm-Based Text Classification Algorithm” Fifth
[35] Alexander Maedche and Ste_en Staab “Mining Ontologies
International Conference on Machine Learning and Cyber-
from Text” LNAI 1937, pp. 189-202, 2000. Springer-
netics, Dalian,pp. 13-16 , 2006.
Verlag Berlin Heidelberg, 2000.
[16] Jingnian Chen a,b,, Houkuan Huang a, Shengfeng Tian a,
[36] Ching Kang Cheng, Xiao Shan Pan, Franz Kurfess “Ontol-
Youli Qua Feature selection for text classification with
ogy-based Semantic Classification of Unstructured Docu-
Naïve Bayes” Expert Systems with Applications 36, pp.
ments”, 2000.
5432–5435, 2009.
[37] M. Sarnovský, M. Parali “Text Mining Workflows Con-
[17] Hiroshi Ogura, Hiromi Amano, Masato Kondo “Feature
struction with Support of Ontologies” 6th International
selection with a measure of deviations from Poisson in text
Symposium on Applied Machine Intelligence and Infor-
categorization” Expert Systems with Applications 36, -pp
matics- SAMI 2008.
6826–6832, 2009.
[38] Maciej Janik and Krys Kochut “Training-less Ontology-
[18] Mehdi Hosseinzadeh Aghdam, Nasser Ghasem-Aghaee,
based Text Categorization” , 2007.
Mohammad Ehsan Basiri “Text feature selection using ant
[39] Yi-Hsing Chang, Hsiu-Yi Huang “An Automatic Docu-
colony optimization”, Expert Systems with Applications 36
ment Classifier System Based On Naïve Bayes Classifier
pp.6843–6853, 2009.
And Ontology” Seventh International Conference on Ma-
[19] P. Sccuy, G.W.Mineanu “Beyoned TFIDF weighting for
chine Learning and Cybernetics, Kunming, July 2008.
text Categorization in the Vector Space Model”, 2003.
© 2010 ACADEMY PUBLISHER18 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
[40] G. Wiederhold and M. Genesereth, “The conceptual basis chine” , Knowledge-Based Systems 21 -pp. 879–886,
for mediation services”,IEEE Expert / Intelligent Systems, 2008
12(5):38-47, 1997. [59] Youngjoong Ko a, Jungyun Seo, “Text classification from
[41] S. Staab, J. Angele, S. Decker, M. Erdmann, A. Hotho, A. unlabeled documents with bootstrapping and feature pro-
Maedche, H.-P. Schnurr, R. Studer, and Y. Sure. “Seman- jection techniques”, Information Processing and Manage-
tic community web portals”, In Proceedings of the 9th In- ment 45 -,pp. 70–83, 2009
ternational World Wide Web Conference, Amsterdam, The [60] Matthew Changa, Chung Keung Poon_, “Using Phrases as
Netherlands, May, 15-19, 2000. Elsevier, 2000. Features in Email Classification”, The Journal of Systems
[42] S. Staab, C. Braun, I. Bruder, A. D¨usterh¨oft, A. Heuer, and Software ,doi: 10.1016/j.jss, 2009.
M. Klettke, G. Neumann, B. Prager, J. Pretzel, H.-P. [61] Willian W. Cohen and Yoram Singer, “Context-sensitive
Schnurr, R. Studer, H. Uszkoreit, and B. Wrenger. Getess, learning method for text categorization”, SIGIR’ 96, 19th
“Searching the web exploiting german texts”, In Proceed- International Conference on Research and Develeoement
ings of the 3rd international Workshop on Cooperating In- in Informational Retrieval, pp-307-315, 1996.
formation Agents. Upsala, Sweden, 1999, LNAI 1652, pp. [62] Eui-Hong (Sam) Han, George Karypis, Vipin Kumar;
113-124. Springer, 1999. “Text Categorization Using Weighted Adjusted k-Nearest
[43] http://www.nstein.com/en/tme_intro.php- 2008. Neighbor Classification”, Department of Computer Science
[44] Yah, A.s., Hirschman, L., and Morgan, A.A. “Evaluation and Engineering. Army HPC Research Centre, University
of text data mining for databasecuration: lessons learned of Minnesota, Minneapolis, USA. 1999.
from the KDD challenge cup.” Bioinformatics 19-(supp.1), [63] Russell Greiner, Jonathan Schaffer; AIxploratorium - Deci-
pp.i331-i339, 2003. sion Trees, Department of Computing Science, University
[45] H.M.Al Fawareh, S.Jusoh, W.R.S.Osman, “Ambiguity in of Alberta,Edmonton,ABT6G2H1, Canada.2001.
Text Mining”, IEEE-2008. URL :http://www.cs.ualberta.ca/ ~aixplore/ learning/ Deci-
[46] A.Stavrianou, P. Andritsos, N. Nicoloyannis “Overview sionTrees
and semantic issues of text mining”, SIGMOD Record, [64] Chidanand Apte, Fred Damerau, Sholom M. Weiss.; “To-
2007, Vol.36,N03, 2007. wards Language Independent Automated Learning of
[47] Zi-Qiang Wang, Xia Sun, De-Xian Zhang, Xin Li “An Text Categorization Models”, In Proceedings of the 17th
Optimal Svm-Based Text Classification Algorithm” Fifth Annual International ACM-SIGIR Conference on Research
International Conference on Machine Learning and Cyber- and Development in Information Retrieval, pp. 23-30.
netics, Dalian, 2006. 1994.
[48] H.Kim, and S.S. Chen, “Associative Naïve Bayes Classifi- [65] Chidanand Apte, Fred Damerau, Sholom M. Weiss; “Au-
er: Automated Linking Of Gene Ontology To Medline tomated Learning of Decision Rules for Text Cate-
Documents” Pattern Recognition doi:10.1016/j.patcog. gorization”, ACM Transactions on Information Systems
2009 (TOIS), Vol. 12 , Issue 3, pp. 233 – 251. 1994.
[49] Chih-Hung Wu , “Behavior-based spam detection using a [66] Tam, V., Santoso, A., & Setiono, R. , “A comparative
hybrid method of rule-based techniques and neural net- study of centroid-based, neighborhood-based and statistical
works”, Expert Systems with Applications, pp. 4321– approaches for effective document categorization”, Pro-
4330, 2009 ceedings of the 16th International Conference on Pattern
[50] Chung-Hong Lee a,, Hsin-Chang Yang , “Construction of Recognition, pp.235–238, 2002.
supervised and unsupervised learning systems for multilin- [67] Bang, S. L., Yang, J. D., & Yang, H. J. , “Hierarchical
gual text categorization” , Expert Systems with Applica- document categorization with k-NN and concept-based
tions, pp. 2400–2410, 2009. thesauri. Information Processing and Management”, pp.
[51] John Atkinson a,, Anita Ferreira b, Elvis Aravena , “Disco- 397–406, 2006.
vering implicit intention-level knowledge from natural- [68] Trappey, A. J. C., Hsu, F.-C., Trappey, C. V., & Lin, C.-I.,
language texts”, Knowledge-Based Systems -2009. “Development of a patent document classification and
[52] Shi-jin Wang, Avin Mathew, Yan Chen , Li-feng Xi , Lin search platform using a back-propagation network”, Expert
Ma, Jay Lee, “Empirical analysis of support vector ma- Systems with Applications, pp. 755–765, 2006 .
chine ensemble classifiers”, Expert Systems with Applica- [69] Que, H. -E. “Applications of fuzzy correlation on multiple
tions, pp. 6466–6476, 2009. document classification.Unpublished master thesis”, In-
[53] G. Louloudis, B. Gatos, I. Pratikakis2, C. Halatsis , “Text formation Engineering epartment, Tamkang University,
Line and Word Segmentation of Handwritten Documents”, Taipei, Taiwan-2000.
Pattern Recognition doi:10.1016/j.patcog.2008.12.016 [70] YiMing Yang, Xin Liu; “A Re-examination of Text Cate-
,2009 gorization Methods, School of Computer Science”, Carne-
[54] Bo Yu, Zong-ben Xu, Cheng-hua Li ,“Latent semantic gie Mellon University. 1999.
analysis for text categorization using neural network”, [71] Wu W, Gao Q, Wang M “An efficient feature selectionme-
Knowledge-Based Systems 21- pp. 900–904, 2008. thod for classification data mining” WSEAS Transactions
[55] Tai-Yue Wang and Huei-Min Chiang “One-Against-One on Information Science and Applications,3: pp 2034-2040.
Fuzzy Support Vector Machine Classifier: An Approach to 2006.
Text Categorization”, Expert Systems with Applications, [72] Y.Yang; “An evaluation of statistical approaches to text
doi: 10.1016/j.eswa.2009. categorization”, Information Retrieval, Vol.1, No.1, pp. 69-
[56] [Duoqian Miao , Qiguo Duan, Hongyun Zhang, Na Jiao, 90, 1999.
“Rough set based hybrid algorithm for text classification”, [73] T.H.Ng, W.B.Goh, and K.L.Low, “Feature selection, per-
Expert Systems with Applications -2009 . ception learning and a usability case study for text catego-
[57] Ming Li, Hang Li , Zhi-Hua Zhou “Semi-supervised doc- rization”, Proceedings of the 20th Annual International
ument retrieval” Information Processing and Management - ACM SIGIR Conference on Research and Development in
2008 . Information Retrieval, Philadelphia, pp.67-73, 1997.
[58] Wen Zhang a, Taketoshi Yoshida a, Xijin Tang “Text clas- [74] Y.Yang,and X.Liu, “An re-examination of text categoriza-
sification based on multi-word with support vector ma- tion”, Proceedings of the 22nd Annual International ACM
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 19
SIGIR Conference on Research and Development in In- neural network and singular value decomposition” Expert
formation Retrieval, Berkeley, pp.42-49,August 1999. Systems with Applications 36 ,pp- 3208–3215, 2009.
[75] Rocchio, J; “Relevance Feedback in Information Retriev- [94] S. M. Kamruzzaman and Farhana Haider; “Hybrid Learn-
al”, In G. Salton (ed.). The SMART System: pp.67-88. ing Algorithm For Text Classification”, 3rd International
[76] Ittner, D., Lewis, D., Ahn, D; “Text Categorization of Low Conference on Electrical & Computer Engineering ICECE
Quality Images”, In: Symposium on Document Analysis 2004, 28-30 December 2004, Dhaka, Bangladesh.
and Information Retrieval, Las Vegas, NV .pp. 301-315, [95] Alex Markov and Mark Last, “A Simple, Structure-
1995 Sensitive Approach for Web Document Classification”,
[77] Balabanovic, M., Shoham Y.: FAB; “Content-based, Col- Springer, AWIC 2005, LNAI 3528, pp. 293–298,
laborative Recommendation”, Communications of the As- 2005.
sociation for Computing Machinery 40(3) pp. 66-72, 1997. [96] Cheng Hua Li, Soon Cheol Park , “Combination of mod-
[78] Pazzani M., Billsus, D; “ Learning and Revising User Pro- ified BPNN algorithms and an efficient feature selection
files”, The Identification of Interesting Web Sites. Machine method for text categorization.”, Information Processing
Learning 27(3) pp. 313-331, 1997. and Management 45, 329–340, 20009.
[79] Joachims, T; “Text Categorization With Support Vector [97] Ming Li , Hang Li , Zhi-Hua Zhou ,“Semi-supervised doc-
Machines: Learning with Many Relevant Features”, In: Eu- ument retrieval”, Information Processing and Management
ropean Conference on Machine Learning, Chemnitz, Ger- 45, pp, 341–355 -2009.
many 1998, pp.137-142 , 1998. [98] Wenqian Shang, Houkuan Huang, Haibin Zhu, Yongmin
[80] Kim, J., Lee, B., Shaw, M., Chang, H., Nelson, W; “Appli- Lin Youli Qu, and Hongbin Dong “An Adaptive Fuzzy
cation of Decision-Tree Induction Techniques to Persona- kNN Text Classifier”, Springer, ICCS 2006, Part III,
lized Advertisements on Internet Storefronts”, International LNCS 3993, pp. 216 – 223, 2006.
Journal of Electronic Commerce 5(3) pp.45-62, 2001. [99] Heide Brücher, Gerhard Knolmayer, Marc-André Mitter-
[81] Wang Xiaoping, Li-Ming Cao. Genetic Algorithm Theory, mayer; “Document Classification Methods for Organizing
Application and Software[M].XI'AN:Xi'an Jiaotong Uni- Explicit Knowledge”, Research Group Information Engi-
versity Press, 2002. neering, Institute of Information Systems, University of
[82] ZHU Zhen-fang, LIU Pei-yu, Lu Ran, “Research of text Bern, Engehaldenstrasse 8, CH - 3012 Bern, Switzerland.
classification technology based on genetic annealing algo- 2002.
rithm” IEEE,, 978-0-7695-3311-7/08, 2008. [100] Andrew McCallum, Kamal Nigam; “A Comparison of
[83] Dino Isa, Lam Hong lee, V. P Kallimani, R. RajKumar, “ Event Models for Naïve Bayes Text Classification”, Jour-
Text Documents Preprocessing with the Bahes Formula for nal of Machine Learning Research 3, pp. 1265-1287. 2003.
Classification using the Support vector machine”, IEEE, [101] Irina Rish; “An Empirical Study of the Naïve Bayes
Traction of Knowledge and Data Engineering, Vol-20, N0- Classifier”, In Proceedings of the IJCAI-01 Workshop on
9 pp-1264-1272, 2008. Empirical Methods in Artificial Intelligence. 2001.
[84] Dino Isa,, V. P Kallimani Lam Hong lee, “Using Self Or- [102] Irina Rish, Joseph Hellerstein, Jayram Thathachar;
ganizing Map for Clustering of Text Documents”, ”, El- “An Analysia of Data Characteristics that affect Naïve
sever , Expert System with Applications-2008. Bayes Performance”, IBM T.J. Watson Research Center 30
[85] Sang-Bum Kim, Kyoung-Soo Han, Hae-Chang Rim, and Saw Mill River Road, Hawthorne, NY 10532, USA.
Sung Hyon Myaeng, “Some Effective Techniques for 2001.
Naive Bayes Text Classification”, IEEE Transactions On [103] Pedro Domingos, Michael Pazzani; “On the Optimali-
Knowledge And Data Engineering, Vol. 18, No. 11, , Pp- ty of the Simple Bayesian Classifier under Zero-One Loss,
1457- 1466 ,November 2006. Machine Learning”, Vol. 29, No. 2-3, pp.103-130. 1997.
[86] P. Domingos and M. J. Pazzani, “On the Optimality of the [104] Sang-Bum Kim, Hue-Chang Rim, Dong-Suk Yook,
Simple Bayesian Classifier under Zero-One Loss,” Ma- Huei-Seok Lim; “Effective Methods for Improving Naïve
chine Learning, vol. 29, nos. 2/3, pp. 103-130, 1997. Bayes Text Classification”, 7th Pacific Rim International
[87] Thiago S.Guzella, Walimir M. Caminhas “ A Review of Conference on Artificial Intelligence, Vol. 2417. 2002.
machine Learning Approches to Spam Filtering”, Elsever , [105] Susana Eyheramendy, Alexander Genkin, Wen-Hua
Expert System with Applications-2009. Ju, David D. Lewis, and David Madigan; “Sparce Bayesian
[88] M. Ikonomakis, S. Kotsiantis, V. Tampakas, “Text Classi- Classifiers for Text Categorization” , Department of Statis-
fication Using Machine Learning Techniques”, Wseas tics,RutgersUniversity.2003.
Transactions on Computers, issue 8, volume 4, pp. 966- [106] Miguel E. Ruiz, Padmini Srinivasan; “Automatic Text
974, 2005. Categorization Using Neural Network”,In Proceedings of
[89] Bao Y. and Ishii N., “Combining Multiple kNN Classifiers the 8th ASIS SIG/CR Workshop on Classification Re-
for Text Categorization by Reducts”, LNCS search, pp. 59-72. 1998.
2534, , pp. 340- 347, 2002. [107] Petri Myllymaki, Henry Tirri; “Bayesian Case-Based
[90] Bi Y., Bell D., Wang H., Guo G., Greer K., ”Combining Reasoning with Neural Network”, In Proceeding of the
Multiple Classifiers Using Dempster's Rule of IEEE International Conference on Neural Network’93,
Combination for Text Categorization”, MDAI, 2004, 127- Vol. 1, pp. 422-427. 1993.
138, 2004. [108] Hwee-Tou Ng, Wei-Boon Goh, Kok-Leong Low;
[91] Sung-Bae Cho, Jee-Haeng Lee, “Learning Neural Network “Feature Selection, Perceptron Learning, and a Usabili-
Ensemble for Practical TextClassification”, Lecture Notes ty Case Study for Text Categorization, In Proceedings of
in Computer Science, Volume 2690, Pages 1032– 1036, the 20th Annual International ACM-SIGIR Conference on
2003. Research and Development in Information Retrieval, pp.
[92] Nardiello P., Sebastiani F., Sperduti A., “Discretizing Con- 67-73. 1997.
tinuous Attributes in AdaBoost for Text Categorization”, [109] Vladimir N. Vapnik, “The Nature of Statistical Learn-
LNCS, Volume 2633, , pp. 320-334, 2003 ing Theory” , Springer, NewYork. 1995.
[93] “Cheng Hua Li , Soon Choel Park, “An efficient document [110] Thorsten Joachims, “Text Categorization with Sup-
classification model using an improved back propagation port Vector Machines: Learning with Many Relevant Fea-
© 2010 ACADEMY PUBLISHER20 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
tures” ECML-98, 10th European Conference on Machine mining, sentiment analysis and text classification through AI
Learning, pp. 137-142. 1998. techniques.
[111] Saurav Sahay, “Support Vector Machines and Docu-
ment Classification”URL:http://www-static.cc.gatech.edu/
Baharum Baharudin received his
~ssahay/sauravsahay7001-2.pdf
Masters Degree from Central Michigan
[112] Soumen Chakrabarti, Shourya Roy, Mahesh V. Soun-
University, USA and his PhD degree
dalgekar;, “ Fast and Accurate Text Classification
via Multiple Linear Discriminant Projection” , The Interna- from University of Bradford, UK. He is
tional Journal on Very Large Data Bases (VLDB), pp. currently a Senior Lecturer at the De-
170-185. 2003. partment of Computer and Information
[113] Yi Lin, “Support Vector Machines and the Bayes Rule Sciences, Universiti Teknologi PE-
in Classification”, Technical Report No.1014, Department TRONAS Malaysia. His research interests lies in Image
of Statistics, University of Wiscousin, Madison. 1999.
Processing, Data Mining and Knowledge Management.
[114] Wikipedia Ensembles of classifiers, http://
en.wikipedia.org/ wiki/ Ensembles_of_ classifiers, 2008.
Lam Hong Lee received the bachelor’s
[115] Monica Rogati , Yiming Yang “High-Performing
degree in computer science from Universi-
Feature Selection for Text ClassificationMonica Rogati,
Monica Rogati”,CIKM’02, November 4–9, 2002, McLean, ty Putra, Malaysia, in 2004 and his PhD
Virginia, USA., 2002. from the Faculty of Engineering and Com-
[116] Fabrice Colas and Pavel Brazdil,“Comparison of puter Science, University of Nottingham,
SVM and Some OlderClassification algorithms in Text Malaysia Campus. He is an Assistant Pro-
Classification Tasks” ,“IFIP International Federation for fessor at the Faculty of Science, Engineer-
Information Processing”, Springer Boston Volume 217,
ing and Technology of Universiti Tunku Abdul Rahman, Perak
Artificial Intelligence in Theory and Practice, pp. 169-178,
Campus, located in Kampar, Malaysia. His current research
2006.
interest lies in improving text categorization using AI tech-
[117] Hanuman Thota , Raghava Naidu Miriyala , Siva
niques.
Prasad Akula, .Mrithyunjaya Rao , Chandra Sekhar
Vellanki ,Allam Appa Rao, Srinubabu Gedela , “Perfor-
mance Comparative in Classification Algorithms Using Khairullah khan received BS-Degree in
Real Datasets”, JCSB/Vol.2 February 2009 Computer Science form Gomal University
[118] Bo Yu a,, Zong-ben Xu b , “A comparative study for DIKhan, Pakistan and Master Degree in
content-based dynamic spam classification using four ma- Computer Science from University of
chine learning algorithms”, 2008 Elsevier , Knowledge-
Science and Techniligy Bannu, Pakistan.
Based Systems 21 ,pp. 355–362,2008.
and is currently a PhD student at the De-
[119] Youn and Dennis McLeod, “ A Comparative Study
partment of Computer and Information
for Email Classification, Seongwook Los Angeles” , CA
Sciences, Universiti Teknologi PETRO-
90089, USA, 2006.
[120] Kang Hyuk Lee, Judy Kay, Byeong Ho Kang, and NAS, Malaysia. He is Senior Lecturer at University of Science
Uwe Rosebrock, “A Comparative Study on Statistical Ma- and Technology Bannu (USTB) Pakistan. His current research
chine Learning Algorithms and Thresholding Strategies for interests include data mining, opinion mining and text classifi-
Automatic Text Categorization”, pp. 444-453, 2002. cation through AI techniques
Springer-Verlag Berlin Heidelberg 2002.
[121] How, B. C. and Kiong, W. T. (2005). An examination
of feature selection frameworks in text categorization. In
AIRS. 558–564.
[122] Pingpeng Yuan, Yuqin Chen, Hai Jin, Li Huang
“MSVM-kNN: Combining SVM and k-NN for Multi-Class
Text Classification”978-0-7695-3316-2/08,2008,IEEE-
DOI 10.1109/WSCS.2008
[123] Fabrice Colas and Pavel Brazdil, “Comparison of
svm and some older classification algorithms in text
classi_cation tasks”, Artificial Intelligence in Theory
and Practice (2006), pp. 169-178, 2006.
Aurangzeb khan received BS-Degree in
Computer Science from Gomal University
DIKhan, Pakistan and Master Degree in
Information Technology From University
of Peshawar, Pakistan and is currently a
PhD student at the Department of Com-
puter and Information Sciences, Universiti
Teknologi PETRONAS, Malaysia. He is an assistant professor
at University of Science and Technology Bannu (USTB) Pakis-
tan. (on study leave). His current research interests include data
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 21
Multilingual Context Ontology Rule Enhanced
Focused Web Crawler
Mukesh Kumar and Renu Vig
{mukesh_rai9@yahoo.com,renuvig@hotmail.com}
University Institute of Engineering and Technology, Panjab University, Chandigarh ,INDIA
Abstract— Rapidly growing size and increasing number of relevancy score Figure: 1 show a Web page tree in which
Non-English resources on World-Wide-Web poses the black nodes are calculated from context ontologies,
unprecedented challenges for general purpose crawlers and and adaptive classification rule score.
Search Engines. It is impossible for any search engine to
index the complete Web. Focused crawler cope with the
growing size by selectively seeking out pages that are English
relevant to a predefined set of topics and avoiding irrelevant
regions of the Web. Rather than collecting and indexing all
accessible Web documents, focused crawler analyses its
crawl boundary to find the links likely to be the most
relevant for the crawl. This paper presents a focused English Hindi English English
crawler whose crawl strategy is based upon the scores
calculated from context ontologies and adaptive
classification rules, and which is capable to deal with
intermediate multilinguity situations (the situations in which
the query language is same as that of target language but English Relevant English Relevant
the intermediate path may pass through some pages which Page Page
are written in mixed, in query and some other language,
way). It enhances the quality of pages retrieved, because it
Figure:1
may be possible that the English meaning of the other
language word sequence may itself or point to some pages
The nodes which are not to be traversed further by any
which are most relevant to the query, and hence should be
of the existing crawler [2,3,4,5,6,8,9,10,11]. Though the
included in the results, which, yet, are left untouched by all
page which is written in Hindi and English, do not
the existing crawlers.
contain any relevancy in terms of English text, yet there
Index Terms— Focused Crawler, Search Engines, may be some text written in Hindi whose English
Information Retrieval, Ontology, Adaptive Rules transcription makes the page relevant to the user query
and that further point to a relevant page written in
English. This situation is termed as intermediate
I. INTRODUCTION
multilinguity.
The World Wide Web, having more than 350 million The proposed crawler is able to deal with intermediate
pages, continues to grow rapidly at a million pages per multilinguity situations, the situations in which the query
day [7]. About 600 MB of text changes every month. language is same as that of target language but the
Such growth and flux poses basic limits of scale for intermediate path may pass through some pages which
today’s generic crawlers [6] and search engines. It is not are written in mixed, in query and some other language,
possible for any search engines to index the whole Web way by making the use of Bilingual dictionary approach.
and to keep track upon the huge consistency
management. The only way out of this problem is II. RELATED WORK
Focused Crawling [12]. A focused crawler tries to fetch
only relevant region of the Web and avoiding irrelevant The area of multilingual information retrieval has been
ones. Since initiation of World Wide Web most of the well explored in the past few decades. The task was
access is in English language which is the most approached in two thoughts. One was a translation of the
dominating and preferred one. In recent times there is query followed by retrieval in monolingual domain,
rapid growth in popularity of internet in semi -English where as the second was translating the documents into
speaking countries like India. Currently the 52% of the query language and performing retrieval [4]. Broadly, it
whole Web is English and rest is non-English and semi- can be said that the task has been seen as a translation
English. A large fraction of this increasing semi-English followed by retrieval approach. Bilingual dictionaries
population read and writes in mixed way, page written in derived from Corpus are used for the translation.
English and other language (like Hindi). Web crawling was simulated by a group of fish
In this paper focused crawler architecture is presented. migrating on the Web [11]. In the so called fish search,
The proposed crawler deals with problem of growing size each URL corresponds to a fish whose survivability is
of the Web by focusing its crawl on two parameters: dependant on visited page relevance and remote server
Manuscript received May 8, 2009; revised August 11 2009.
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.21-2522 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
speed. Page relevance is estimated using a binary No work has yet been done to tackle the
classification by using a simple keyword or regular intermediate multilinguity situations (Fig.1), which
expression match. Only when fish traverse a specified can considerably affect the harvest ratio of the
amount of irrelevant pages they die off. The fish crawler
consequently migrate in the general direction of relevant
pages which are then presented as results
III. PROPOSED CRAWLER
[5] Propose calculating the Page Rank [8] score on the
graph induced by pages downloaded so far and then using
Fig.2 depicts architecture of the proposed focused Web
this score as a priority of URLs extracted from a page.
crawler.It crawler works as per the following code
They show some improvement over the standard breadth-
segment:
first algorithm. The improvement however is not large.
1. Repeat until Maximum Crawler Limit is reached.
This may be due to the fact that the Page Rank score is
2. Take the user query and initialize the seeds along
calculated on a very small, non-random subset of the web
with their priority as according to the SeedInitializer
and also that the Page Rank algorithm is too general for
algorithm discussed later. Set minimum ontology
use in topic-driven tasks.
relevance Min_OntRel, and minimum look ahead
[9] Considers an ontology-based algorithm for page
relevance constant Min_LaRel to some constant
relevance computation. After preprocessing, entities
values.
(words occurring in the ontology) are extracted from the
3. Download the seed pages as according to their
page and counted. Relevance of the page with regard to
priority and for each seed page go to Step 4.
user selected entities of interest is then computed by
4. For each link in page go to Step 5.
using several measures on ontology graph (e.g. direct
5. Generate the Context Link with the help of Context
match, taxonomic and more complex relationships).
Link Extractor and then perform transcription for
A critical look at the available literature indicates that,
the intermediate multilinguity with the help of
the existing crawling approaches have following to be
Multilingual Transcriptor and go to Step 6.
said:
6. Calculate the OBRS and LARS (as discussed later)
1. None of them make use of efficient relevance score
and put them in the priority queue as according to
and tunneling (process of reaching to relevant pages
its OBRS.
from the irrelevant pages with in the current page)
7. Retrieve the link with highest OBRS. If its
in combination to retrieve the Web documents.
OBRS>Min_OntRel then download the Web page
.2. Lot of work has been done in general information
and go to Step 4 Else go to Step 8.
retrieval, also in multilingual information retrieval,
8. If it’s LARS>Min_LaRel then download the Web
where user enters complete query in one language
page and go to Step 4.
and results are in some other language, but the
Functioning of the main components is discussed below:
crawling is done through the single language only.
User Query
Ontology Repository
Relevant Ontology Extractor Seed Initializer
Ontology Based Relevant Score
(OBRS) Generator
Repository containing URL with
LARS Generator with the help of Adaptive Rule respective (LARS, OBRS)
with Class Definition and Classifier
Crawler Manager
Multilingul Transcriptor
DICTIO NARY URL P r i o r i t y Q u eue
Context Link Extractor
URL
Fetched
Document Repository
Crawler
Server Status
Busy Server Queue INTERNET
URL Queue
HTTP Module
Fig 2: Proposed Crawler Architecture
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 23
A. SEED INITIALIZER Ontology Based Crawling [2, 9] is one of the backbone
SeedInitializer algorithm work as per the seed detector features of our crawler. OBRS generator makes use of the
discussed in [10]. It retrieves the seed URLs from the relevant ontology extracted by the Relevant Ontology
three most popular search engines Goggle, Yahoo and Extractor (ROE) and context link passed by the Context
MSN for the specific keyword. It prioritizes the seeds in Link Extractor (CLE). It uses an Importance Table that
the following three classes: importance of each term occurring in the relevant
High: URLs occurring across Search Engines more than ontology passed from the ROE. A more relevant term to
once. the query will have the more importance and the terms
Medium: URLs repeatedly occurring within the Search which are common to more than one domain have less
Engine, not across the Search Engines. importance. Importance Table for a given ontology is
Low: Other URLs occurring only once within the Search given in Table 1.
Engine. The following code segment is used for calculation of
OBRS for a Context Hyper Link passed from the CLE
B. CRAWLER MANAGER with the help of Importance Table
Crawler manager fetches the URLs from the URL Table 1: Importance Table
repository and add them to the priority queue. It generates
the crawler instances that download the document. Ontology terms Importance
Comp. Sc. And Engg. 1.0
C. CRAWLER
Comp. Engg. 0.9
Crawler is a multi-threaded program [10] that is
capable of downloading the Web pages from the Web and Comp. Sc. 0.8
storing the documents in the document repository. Each
Information Tech. 0.5
crawler has its own queue, which holds the list of URLs
Computer 0.4
to be crawled. The crawler fetches the URLs from the
queue. The same or different crawlers would have sent a Engineering 0.3
request to the same server. Busy Server Queue is
maintained to have the list of URLs to which the crawlers
OBRS Generator Algorithm
have sent the request and awaiting for the response. Once
INPUT: A Context Link (CL) corresponding to a Web page,
the server is connected all the requests are fulfilled. Also
an Importance Table.
instead of disconnecting, it keeps connected for a fixed
OUTPUT: The relevance score (OBRS) for each Context
time interval for the future requests. Link (CL).
Step1: Initialize the relevance score of the Context Link
D. CONTEXT LINK EXTRACTOR (CL) to 0 i.e. OBRS=0.
Context Link Extractor [10] fetches the document from Step2: Select first term (T) and corresponding Importance
the Document Repository and extracts the URLs. It then (IMP) from the Importance Table.
Step3: Calculate how many times the term (T) occurs in the
checks for the URLs extracted in the URL Fetched. If not
Context Link (CL). Let the number of occurrence is calculated
found, the surrounding text which include a fixed number
in COUNT.
of letters preceding and succeeding the hyperlink, the
Step4: Multiply the number of occurrence calculated in step
heading or sub heading under which the hyperlink
3 with the Importance IMP. Let call this TERM_IMP. And
appears is extracted from the document. The extracted TERM_IMP=COUNT * IMP
link with the context information is passed to Multingual
Step5: Add this term importance to OBRS. So new OBRS
Transcriptor and URL Repository. will be, OBRS = OBRS + TERM_IMP.
Step6: Select the next term and weight from Importance
E. MULTILINGUAL TRANSCRIPTOR table and go to step3, until all the terms in the weight table are
This is the component that deals with the intermediate visited.
multilinguity situations. It makes use of a bilingual Step7: End.
dictionary (e.g. Hindi to English) for transcription.It
G. LARS GENERATOR
works as according to the following code segment:
1. If the Context Link is in English language then go Pirkola [1] pointed that for crawling based upon topic
it should make use of historical accesses to that particular
to Step 3 , Else go to Step 2
2. Locate the Hindi context, remove the noise words
domain The proposed crawler makes use of adaptive
and transcript for that in English by making use of Rules [3] derived from the classes and link access to
improve the crawl and to handle the situation where an
the Bilingual Dictionary available, generate the
irrelevant link in a page may further point to relevant
transcripted Context Link, and go to Step 3.
3. Pass the Context Link to the OBRS Generator and page. For doing this crawler’s classifier component is
trained with a class and taxonomy, and a set of example
LARS Generator.
documents for each class and call this as train-0 set. Next
F. OBRS GENERATOR for each class in the train-0 set we gather all Web pages
that the example Web pages in the corresponding class
point to. Now we have a collection of class names train-1
© 2010 ACADEMY PUBLISHERset and a set of fetched pages for each class. We know the Table 3: Adaptive Rules for the distribution in Table 1,
class distribution of pages to which the documents in (the number following each rule is the probability P)
each train-0 set class point. For each class in the set train-
CSE:
0, we count the number of referred classes in
corresponding train-1 set and generate rules of the form CSE → IT(0.8)
C →C (P), means a page of class C can point to a
i j i CSE →COMP(0.1)
page of class C
j
with probabilityP. Pis the ratio of
CSE → ENGG(0.1)
train-1 pages in C to all train-1 pages thatC pages in IT :
i j
train-0 refer to. The proposed crawler while seeking Web
IT →CSE(0.2)
pages of class C attaches priority score Pto the pages
i IT → IT(0.4)
that it encounters. To demonstrate the approach example
is presented. The taxonomy includes four classes: IT → ENGG(0.4)
• Computer Sc. and Engineering (CSE) ENGG:
• Information Technology (IT)
• Engineering (ENGG)
ENGG →CSE(0.3)
• Computer (COMP) ENGG → IT(0.4)
Train-0 set can be constructed from any existing
directory. Next, for each class , we retrieve the pages that ENGG → ENGG(00.3)
this class’s example pages refer to. Assume that we fetch COMP:
10 such pages example pages for each class in the train-0
set and that the class distribution among these newly
COMP→COMP(1.0)
fetched pages i.e train-1 set is as listed in Table 2. Then
we can obtain the rules of Table 3.
Adaptive rules may support the tunneling for longer paths
using simple application of transitivity among the rules. 1.0
Also this mechanism is independent of a page’s
similarity, but rather relies on the probability that a given 0.8
page’s class refer to a target class. This P acts as the Look
Ahead Score for the CORE. It can be further manipulated 0.6
to obtain finer results.
0.4
Table 2: Class distribution into train-1 set for each class
in train-0 set 0.2
CSE IT ENGG COMP
8 URLs for IT 2 URLs for 3 URLs for 10 URLs
CSE CSE for COMP
200 400 600 800 1000
1 URL for 4 URLs for 4 URLs for
Total Number of pages
ENGG IT IT
1 URL for 4 URLs for 3 URLs for
COMP ENGG ENGG
V. CONCLUSION
IV. RESULTS
A multilingual focused web crawler is presented that
The proposed crawler is being simulated with 1000
makes use of context ontology score and adaptive
pages and an ontology repository containing ontologies
classification rules for relevancy calculation, and
related to all engineering branches under a particular
multilingual transcriptor to deal with the intermediate
university. And for each test case 0.2 X number of pages
multilinguity. The proposed crawler can deliver improved
out of total X pages are written in mixed way (i.e in
search results by going through the intermediate
English and Hindi), such that the important terms in these
multilingual documents. Ontologies served as efficient
0.2 X pages appears in Hindi. By taking the values of
concepts representation technique. The proposed crawler
Min_OntRel=5, and Min_LaRel=3 the simulated results
shows improved harvest ratio when tested for retrieving
are shown in Fig 3. It is a plot of number of relevant
information for courses run by particular university. It
pages found against the total number of pages
can be used in digital libraries for retrieving documents
downloaded i.e plot for harvest rate for the baseline
distributed in the form of documents written in a mixed
focused crawler, non-multilingual context ontology rule
way. Further work could be done to replace the
enhanced focused web crawler, and multilingual context
multilingual transcriptor with multilingual translator for
ontology rule enhanced focused web crawler.
oitaR
tsevraH
detcepxE
24 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
Baseline Crawler
Proposed Crawler with Multilinguity
Proposed Crawler without Multilinguity
Fig 3: Comparative Results
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 25
semantic matching of the intermediate text, which can [6] Junghoo Cho, Hetor Gasrcia-Molina, WWW 2002
further enhance the search results. “Parallel Crawlers”.
[7].K.Bharat and A. Broder. A technique for measuring the
relative size and overlap of public web search engines. In
REFERENCES
Proc. of the 7th WWW Conference 1998.
[8]. L. Page, S. Brin, R. Motwani, T. Winograd, "The PageRank
[1]. Ari Pirkola, 2007, “Focused Crawling: AMeans
Citation Ranking: Bringing Order to the Web", Stanford
ToAcquire Biological Data from the Web”, VLDB ’07,
Digital Library Technologies Project.
September 23-28, Vienna, Austria, ACM.
[9]. M.Ehrig, A. Maedche., 2003 “Ontology-focused
[2]. Debajyoti Mukhopadhyay, Arup Biswas, ukanta Sinha,.
Crawling of Web Documents”, In Proceedings of the
2007, “A New Approach to Design Domain Specific
ACM symposium on Applied computing.
Ontology Based Web Crawler”, 10th International
[10] M.Yuvrani, N.Ch.S.N.Iyengar, A.Kanan, 2006
Conference on Information Technology, IEEE Computer
“LSCrawler: a Framework for an Enhanced Focused
Science, 289-291.
Web Crawler based on Link Structures”, in the
[3] Ismail Sengor Altingovde and Ozgur Ulusoy,
proceedings of the IEEE/ACM International Conference
November/December 2004, “Exploiting Interclass Rules
on web Intelligence.
for Focused Crawling”, published in IEEE Intelligent
[11] P.M.E. De Bra and R.D.J.Post, "Information retrieval in
Systems. pp 66-73.
the World-Wide Web: Makingclient-based searching
[4]. Jaime G.Carbonell,Yimming Yang,Robert E.
feasible", Computer Networks and ISDN Systems. vol.
Frederking,Ralf D.Brown, Yibing Geng, and Danny lee.
27, no. 2, pp. 183-192.
Translingual information Retrieval: a comparative
[12] S. Chakrabarti, M. van den Berg, B. Domc,1999,
evaluation. In IJCAI(1), pages 708-715,1997.
“Focused crawling: a new approach to topic-specific Web
[5]. J. Cho, H. Garcia-Molina, L. Page. , April 1998 "Efficient
resource discovery”, Proceedings of the 8th international
Crawling Through URL Ordering” In Proceedings of the
World WildWeb Conference, Toronto, Canada.
7th International WWW Conference, Brisbane, Australia.
© 2010 ACADEMY PUBLISHER26 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
Integrated Performance and Visualization
Enhancements of OLAP Using Growing Self
Organizing Neural Networks
Muhammad Usman Sohail Asghar Simon Fong
Shaheed Zulfikar Ali Bhutto Institute of Mohammad Ali Jinnah University, University of Macau,
Science and Technology, Islamabad, Islamabad, Pakistan Taipa, Macau SAR
Pakistan Email: sohail.asghar@jinnah.edu.pk Email: ccfong@umac.mo
Email: usmanspak@yahoo.com
Abstract—OLAP performance and its data visualization can systems in an interactive way in order to support the
be improved using different types of enhancement decision-making process. The fast growing complexity
techniques. Previous research has taken two separate and volumes of the data to be analyzed impose new
directions in OLAP performance improvement and
requirements on OLAP systems [1]. An OLAP system’s
visualization enhancement respectively. Some recent works
performance and level of data visualization can be
have shown the benefits of combining OLAP and Data
enhanced using different tools and techniques. With the
Mining. Our previous work presents an architecture for the
coupling of these enhancement techniques, OLAP
enhancement of OLAP functionality by integrating OLAP
and Data Mining. In this paper, we proposed a novel functionality can be enhanced [2]. However, OLAP
architecture that not only overcomes the existing limitations, performance improvement and visualization enhancement
but also provides a way for an integrated enhancement of have been taken separately in the past.
performance and visualization using self organizing neural Figure 1 depicts the integration of performance
network. We have developed a prototype and validated the improvement and visualization enhancement practices.
proposed architecture using real-life data sets. Experimental
Another aspect of enhancement is Data Mining, which
results show that cube construction time and its interactive
aims at the extraction of synthesized and previously
data visualization capability can be improved remarkably.
unknown insights from large databases [3]. It can be
By integrating enhanced OLAP with data mining system a
viewed as an automated application of algorithms to
higher degree of enhancement is achieved which makes
significant advancement in the modern OLAP systems. detect patterns and extract knowledge from data that is
not obvious to the user [4]. Some recent work has shown
Index Terms— Clustering, Data Mining, GSOM, Multi- the benefits of combining OLAP and Data Mining.
dimensional Data, OLAP, Performance Enhancement, According to [5], automated techniques of Data Mining
Visualization Techniques can make OLAP more useful and easier to apply in the
overall scheme of decision support systems. Furthermore,
Data mining techniques like Associations [6],
I. INTRODUCTION
Classification [7], Clustering [8] and Trend Analysis [9]
OLAP technology refers to a set of data analysis can be used together with OLAP to discover knowledge
techniques to view the data from all of the transactional from data [10].
Figure 1. Integration of Enhancement Techniques [2]
Manuscript received August 11, 2009; revised October 1, 2009.
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.26-37JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 27
In the quest of OLAP enhancement research, Asghar II. PREVIOUS WORK
[3] proposed a functionality-enhancement technique
This section provides a brief summary of our previous
using self-organizing neural networks. This technique
work about OLAP functionality enhancement, on which
proposed the integration of Data Mining with OLAP by
our proposed solution is built upon. In our previous work,
passing the mined data to the OLAP engine for a more
we extended the capability of the OLAP systems by the
focused analysis and, hence, added intelligence to the
use of a neural network. In addition to the usual
OLAP system. The major limitation of the proposed
visualization capabilities, it provided users with the
enhancement architecture was the deficiency of work in
opportunity to analyze data in clusters at different levels
the enhancement of OLAP performance and
of abstraction. The technique used is basically called
visualization. No visualization enhancement technique
Growing Self-Organizing Map (GSOM) [17]. GSOM has
was used for the expanded view of the OLAP data. Data
been developed as a flexible data mining feature mapping
cube processing time and its physical drive storage were
method over the traditional Self-Organizing Map (SOM)
also not discussed. Users of the proposed architecture had
[18]. The innovation of GSOM is the ability of generating
to formulate queries to manually retrieve the data of their
feature maps of different levels of data abstraction using a
choice. Interactive visual analysis was missing which is a
parameter called ‘spread factor’. This spread factor is
very attractive functionality of OLAP systems. Typical
initially used for generation of hierarchical clusters and
OLAP data do not change, as they are usually historical
analysis technique which is known as dynamic SOM
data. A major concern is often the support of ad-hoc data
Tree.
exploration by an analyst or other users looking for trends
or patterns at v levels of details, perhaps integrated with
decision support applications [10].
In this paper, we extend the previous work to
overcome the existing limitations and provide an
enhanced architecture that can cater for both performance
improvement and visualization enhancement. The newly
proposed architecture has various modules which allow
the integrated performance and visualization
enhancement of the OLAP system.
We developed an OLAP prototype system using C#
language and other software development tools such as
Microsoft SQL server [11], Microsoft Analysis Services
Figure 2. Architecture Proposed in the Previous Work [3].
[12] and Dundas OLAP services for Windows Form [13].
Experiments are done with data from Forest Cover Type
These hierarchical clusters from the dynamic SOM
[14] and Zoo [15] data set. It is observed that using the
Tree are subsequently used to provide the OLAP user
proposed architecture we can enhance the existing OLAP
with the ability to visualize and select data clusters at
systems in terms of performance and visualization and
different levels of abstraction for further detailed
can get a higher degree of overall enhancement by
analysis. Figure 2 depicts the previously proposed
integrating the performance improvement and
architecture for enhancing OLAP’s functionality. This
visualization enhancement techniques. To the best of our
architecture indicates two different approaches to pass the
knowledge, no such architecture which features an
data set to an OLAP engine. The framework we devised
enhancement solution for OLAP systems for improving
and presented in this paper is based on the hierarchical
performance and enhancing visualization capabilities was
clusters generated by GSOM which are then translated
ever proposed. Our experimental results show that the
into manual relational tables. The tables are stored in the
cube construction time can be improved remarkably by
relational database which serves the data source for the
using the clustered data tables as compared to relational
OLAP engine.
tables. Similarly, by implementing various visualization
The architecture in the past work however has a
and enhancement tools and APIs [16] at the OLAP
number of drawbacks. Firstly, the clusters generated from
systems can improve the level of interactive data
GSOM are to be manually translated into relational
visualization through the use of different types of charts,
tables. This means that user involvement is required for
graphs and data grids.
the clusters to be mapped on to a relational schema.
The remaining of this paper is organized as follows:
Secondly, the OLAP user is unable to perform interactive
Section 2 highlights the summary of the past work on
visualization on the clustered data as there is no such
which our solution is based. Section 3 addresses the
facility available at the front-end. Customized queries are
related work. We elaborate the proposed architecture of
required to view the clustered data which requires
enhanced OLAP in section 4. The design is followed by
knowledge of the complete clustered data in advance. It
description of implementation of our prototype in section
also lacks of support of Multidimensional expressions
5. Section 6 is where experimental results are disused and
(MDX) which is a query language for the OLAP
compared with the previous work. A conclusions and
database. Users have to rely on the SQL command,
possible future research directions are drawn at the end.
‘GROUP BY’ clause to perform runtime aggregation of
© 2010 ACADEMY PUBLISHER28 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
data. Cube construction time thus grows unfavorably as multi-dimensional cubes is suggested in [25]. Then a new
the increase of size of the data sets. visual interactive exploration technique for OLAP is
The motivation of this paper establishes from the need presented in [26]. This is similar to our work in terms of
to keep up with the pace of incremental and fast OLAP OLAP user facilitation. This enhanced architecture,
development, and the limitations at the previous allows novice users of OLAP technology to explore and
architecture specifically in terms of its performance and analyze OLAP data cubes without sophisticated queries.
data visualization. Subsequently a framework is proposed in [27] for
querying complex multi-dimensional data and
III. RELATED WORKS transforming irregular hierarchies to make them
navigable in a uniform manner. Lately in 2008,
As far as performance is concerned in OLAP system,
Mansmann [28] introduced a comprehensive visual
the bottleneck usually involves data processing speeds
exploration framework which implements OLAP
over the structures of the data cubes. The authors in [5]
operation as a form of powerful data navigation and
identified the problem that OLAP operations require
allows users to explore data using a variety of interactive
complex queries on underlying data, which can be very
visualization techniques. The Dundas visualization
expensive in terms of computation time. Using parallel
toolkit which we have used in our work to visualize the
computers is one solution. Another approach which is
OLAP data also allows user to view the data using a
similar to our solution proposed here is to improve cube
number of charts. The use of this software makes our
processing time rather than the OLAP query processing
work similar as our choice of visualization of data also
time. For instance, authors in [19] suggested that OLAP
provides a number of visualization views to understand
performance can be improved by using the (MHC) Multi-
and analyze the data in an interactive way.
dimensional Hierarchical Clustering technique.
As observed from our review, research communities
Clustering was introduced as a way to speed up
advocate that associating Data Mining to OLAP is useful
aggregation queries without additional storage cost for
for rich analysis and OLAP tools [21]. By following this
view materialization.
fusion idea, we emphasize the coupling of performance
In contrast, our work is to have used GSOM for
and visualization enhancement of OLAP systems as our
generating hierarchical clusters for a focused analysis
solution. Why then is there a need for OLAP
instead of speeding up OLAP queries. Similarly, authors
enhancement architecture? Though a number of
in [20] achieved Heuristic Optimization of OLAP in
enhancement architectures were proposed in the past,
MHC (multi-dimensionally hierarchically clustered)
none of the work so far was intended towards integrated
databases. They found that commercial relational
enhancement of both OLAP performance and
database management systems use multiple one-
visualization, and hence a strong need exists for it [25].
dimensional indexes to process OLAP queries that restrict
multiple dimensions. They presented architecture for
IV. PROPOSED ARCHITECTURE FOR ENHANCED OLAP
MHC databases based on CSB star schema. In our work,
we adopted this concept of using relational tables that To fulfill the growing demands of OLAP users [3], a
contain clustered hierarchical information, that are standardized architecture are required which can easily be
transformed into typical star schema. Along with this deployed as a complete system, it can support the
concept, researchers in [21] suggested an enhanced integrated enhancement. Figure 3 depicts such
OLAP operator based on the Agglomerative Hierarchical architecture, which enforces integrated enhancement of
Clustering (AHC). The operator is called Operator for OLAP’s performance and visualization.
Aggregation by Clustering (OpAC) that is able to provide We describe the important components of the proposed
significant aggregates of facts referred to complex architecture for the enhancement of OLAP systems. The
objects. Our approach is slightly different that we do not goal of this architecture is to integrate OLAP with Data
use any operator for clustering. Instead, we use a separate Mining and to provide integrated performance and
analysis server to construct a cube using the star schema visualization enhancement. To achieve this objective, we
source residing in the database server. deployed separate servers; one is for the database and the
There are other innovated techniques for cube other one for the OLAP data. This architecture indicates
enhancements, such as Cube Presentation Model (CPM) two channels to pass data to the OLAP engine. The first
recommended in [22]. CPM can be naturally mapped path is the conventional or non-clustered method where a
with an advanced visualization technique called Table data set is loaded directly into the database server through
Lens. A visual interface for exploring OLAP data with Extract, Transform and Load (ETL) process. The data are
coordinated-dimension hierarchies is introduced in [10]. stored in the form of a relational database. From the
In literature, a lot of works were devoted to relational database a star schema is designed using
visualization enhancement techniques. Just to name a standard SQL queries and data is loaded into the star
few, an advanced tool (CommonGIS) for highly schema. The OLAP server takes this star schema as a
interactive visual exploration of spatial data is in [23]. source to construct OLAP cubes. It also provides storage
Followed by that, authors in [24] extended a tool for and management mechanism for the cube data. At the
Spatial OLAP, called SOVAT (Spatial OLAP front-end a visualization tool captures the cubes
Visualization and Analysis Tool). A hierarchy-driven generated by the OLAP server and displays the data in
compression technique for the advanced visualization of form of charts, reports and tables.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 29
Figure 3. Enhanced OLAP Architecture Proposed.
Figure 4. Hierarchical Cluster Decomposition of Forest data set.
data. As mentioned previously the star schema becomes
A. Data Processing
the data source. Using this source, cubes of the clustered
One unique feature about the architecture we have data are constructed. These clustered cubes become the
devised to enhance OLAP is the hierarchical data clusters source of data to be visualized using the prototype in the
generated by GSOM. GSOM is based on the design of an same manner. From the visualized clusters, user can
unsupervised neural network [29]. The use of GSOM is select the clusters of choice and perform analysis on
mainly to produce the hierarchal clusters. particular clusters instead of the complete set of data.
Data set is first fed to GSOM tool which produces the
B. Visualization
hierarchical clusters using a numerical spread factor. User
can set the spread factor to control the number of A Front-end visualization tool is connected with the
hierarchical clusters generation using GSOM. An OLAP server that generates cubes using star schema as a
example is given in Figure 4 where the spread factor is 3. data OLAP cubes, displaying the cube data in various
Once the clusters are generated the clusters are mapped views such as charts, graphs, reports and tables at the
manually into relational tables. The relational tables are user’s end. Users can perform basic OLAP operations at
stored in a database in the database server. From these will using front-end tool; hence, interactive analysis is
relational tables star schema is created and uploaded with available using drill down, roll up and other standard
© 2010 ACADEMY PUBLISHER30 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
OLAP operations. Visualization capabilities of both
traditional and clustered OLAP data are enhanced. The
proposed architecture achieves its objective as it
integrates OLAP with Data Dining and uses clustered
data for performance improvement in terms of cube
processing time. In addition to this, the Visualization tool
at the front end supports an interactive visual exploration
of OLAP data using drill-down, roll-up charts and tables.
The visualization tool enhances the visualization
capabilities of both traditional and clustered OLAP data.
V. IMPLEMENTATION
This section is presented in two phases: in the first
phase we explain the implementation details of the
proposed architecture; in the second phase we describe
the experiments performed on two different data sets.
Our architecture supports two data loading approaches,
clustered and non-clustered. For the non-clustered
approach which is the ordinary one, the data set stored in
a form of single comma separated text file (depicted in
Figure 5) is uploaded in a database as a table using
Microsoft SQL Server 2000’s Data Transformation
Services (DTS) as shown in Figure 6. Figure 6. Data Transformation Services (DTS) Wizard View.
Figure 7. Star Schema of ‘Forest Cover Type’ Dataset.
Figure 5. Data in text file.
Figure 8 shows the cube wizard of MS Analysis
After the database has been uploaded with the data, Services. Using this wizard user can construct cubes from
using SQL queries a single fact table and three dimension the data by selecting the different measures (facts)
tables are created and uploaded with data to form a star available in the fact table.
schema as shown in Figure 7. This star schema residing
in the database server is a source data for OLAP server. The wizard also facilitates the user to create
In this project we are using Microsoft Analysis Services dimensional hierarchies. Once the cube wizard is finished
as an OLAP data server. The OLAP engine in the with the dimensions and measures selection it prompts
Analysis server uses this data source to construct cubes. the user for the cube storage mode as well. The Storage
Design Wizard of MS Analysis Services allows for three
types of storage modes, namely multidimensional
(MOLAP), Relational (ROLAP) and Hybrid (HOLAP).
Figure 9 shows the selection interface of storage type.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 31
Figure 11 shows the summary of the cube process. MS
Analysis Services have a built-in Cube browser to view
the cube data and perform the basic OLAP operations
such as drill-down and roll-up.
Figure 8. Fact Table Selection Step in the Cube Wizard.
Figure 11. Animal Cube Construction Process Details.
Figure 12 presents a preview of the built-in cube
browser of MS Analysis Services. The cube browser only
shows data in a data grid. In order to enhance the
visualization capabilities of the OLAP systems, we have
developed a prototype using the Dundas OLAP services.
With the aid of it, the prototype takes cubes residing in
the OLAP server as a source and displays the cube data in
the form of a rich and interactive interface. The user can
perform the basic OLAP operations and can also see the
data in a number of chart types and colorful grids. A
Figure 9. Storage Design Wizard – storage type selection step.
number of reports can also be saved in Extensible
Markup Language (XML) format.
The cube performance can be controlled to some
extent by setting the aggregation options of the cube.
Aggregations are pre-calculated summaries of data that
make querying of the cube faster. The wizard allows
users to do setting of the aggregation options as well.
Figure 10 illustrates the aggregate setting option which
can be controlled using both performance and size of the
cube. Finally, the cube along with all the given settings,
are processed and the details are shown to the user.
Figure 12. Animal Cube Data in Cube Browser.
Dundas OLAP Services has the following user
interface controls: [30].
a) OLAP Chart: A central data visualization area to
display the cube data.
b) OLAP Grid: A visualization grid for cube data-
analysis.
c) OLAP Toolbar: Quick access to key control
functions.
d) Cube Selector: A control to select a cube from a
Figure 10. Cube Aggregation Settings –Storage Design Wizard. multi-cubed data source.
© 2010 ACADEMY PUBLISHER32 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
e) Report List: A collection of reports for storage cube data. The prototype has been developed using
purposes. Microsoft visual studio 2005 and C sharp programming
f) Dimension Browser: A control to browse and change language.
the dimensions of the current cube.
Figure 13 shows the user interface of our prototype
which has all the above mentioned controls to work on
Figure 13. User Interface of Developed Prototype.
The second approach towards OLAP enhancement is
the use of GSOM for loading the clustered data into the
OLAP system. Data set is first fed into a GSOM tool
which produces the hierarchical clusters using the spread
factor. User can set the spread factor to control the
number of hierarchical clusters generation using GSOM
as depicted in Figure 4.
Once the clusters are generated the clusters are mapped
manually into relational tables. The relational tables are
stored in a database in the database server (MS SQL
server 2000). From these relational tables star schema is
created and uploaded with data. As mentioned previously
the star schema becomes the data source and using this
source, cubes of the clustered data are constructed. These Figure 14. Chart view of clustered zoo data.
cluster-based cubes become the source of data to be
visualized using the prototype in the same manner.
VI. EXPERIMENTS
Figure 14 shows the cluster-based cube data in the As explained earlier, for the experiments we selected
form of bar chart using chart view of the prototype. Using two different data sets for testing and validation of the
this approach, user can select the clusters of choice and results of our proposed architecture. The first is a limited
perform analysis on particular clusters instead of the dataset called the Zoo data set and the second one is a
complete cube data. larger data set of Forest Cover Type. The idea is to verify
how well our new architecture would perform for data of
various sizes. Especially we want to demonstrate the
integrated enhancements of performance and
visualization. We tested both clustered and non-clustered
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 33
based approaches on the data sets. However, only the [CategoryID] [int] NULL, [Hair] [int] NULL,
clustered based approach is discussed in details here. [Feathers] [int] NULL, [Airborne] [int]
NULL, [Aquatic] [int] NULL, [Predator]
A. Experiment 1 – with Simple Data
[int] NULL, [Toothed] [int] NULL, [Backbone]
In our first experiment we chose a small data set called [int] NULL, [Breathes] [int] NULL, [Venomous]
Zoo Data. Firstly the data set is passed to GSOM to [int] NULL, [Fins] [int] NULL, [Legs] [int]
generate the hierarchical clusters. We performed this NULL, [Tail] [int] NULL,
process using different values of spread factor on the data [Domestic] [int] NULL, [Catsize] [int] NULL
in order to obtain hierarchical clusters at different levels ) ON [PRIMARY] GO
of abstraction. Only three levels of hierarchical clusters
were selected for analysis. • Fact Table Data Insertion
INSERT INTO [Fact_table] (
It was observed that the hierarchical clusters generated [AnimalID],[CategoryID],[Hair],[Feathers],[Eggs],[Mil
by using three different values of spread factor such as k],[Airborne],[Aquatic],[Predator],[Toothed]Backb
0.01, 0.05 and 0.1 identify different groups and one],[Breathes],[Venomous],
subgroups of several animal types. These groups clearly [Fins] ,[Legs],[Tail],[Domestic],[Catsize] )
indicate the relevant grouping of data based on user
interest and these clusters can be further spread out if • Fact Table Data Update
necessary. GSOM provides an unbiased grouping of data
SELECT
in terms of clusters. These clusters were picked at diverse
animalid,CategoryID,[Hair],[Feathers],[Eggs],[Mil
levels of abstraction and stored in relational tables. One
k],[Airborne],[Aquatic],[Predator],[Toothed][Backb
of the main resulting tables is shown in Table 1.
one],[Breathes],[Venomous],[Fins] ,[Legs] ,
[Tail] ,[Domestic],[Catsize] FROM Animal
TABLE I.
a,Category c,Zoo_Data z WHERE
CLUSTER HIERARCHY TABLE FOR DATASET ZOO
a.AnimalName=z.Animal_Name and
Child_
c.CategoryID=z.Category
Parent Child Child
ID Clus_Name _Clus _Clus _Clus
TABLE II.
1 Mammals C1 C1-1 C1-1-1 A FRAGMENT OF FACT TABLE OF CLUSTERED ZOO DATA
2 Mammals C1 C1-2 C1-2-1
fact_id Clus_id no_of_animals
3 Mammals C1 C1-3 C1-3-1
1 1 3
4 Mammals C1 C1-3 C1-3-2
2 2 1
5 Mammals C1 C1-4 C1-4-1
3 3 4
6 Mammals C1 C1-4 C1-4-2
. . .
7 Mammals C1 C1-5 C1-5-1
8 Fish C2 C2-1 C2-1-1 The OLAP server takes this star schema and uses the
9 Fish C2 C2-1 C2-1-2 clustered data to generate and process a cube. We have
10 Fish C2 C2-2 C2-2-1 named the database having clustered zoo data as
11 Bird C3 C3-1 C3-1-1 Clustered_Zoo. We further created an OLAP database
and created a new cube called cluster_zoo_cube. The
12 Bird C3 C3-2 C3-2-1
OLAP database which has stored the cube becomes the
13 Bird C3 C3-2 C3-2-2
source of cube data. In the prototype, we set the
14 Bird C3 C3-2 C3-2-3
connection so that the front-end tool can connect string
15 Crawler C4 C4-1 C4-1-1 with the OLAP server to get cube data and display it in
16 Crawler C4 C4-1 C4-1-2 the forms of charts, graphs, grids and reports. The
17 Crawler C4 C4-2 C4-2-1 connection string used to connect Microsoft’s Windows
Forms with the OLAP server is as follows;
18 Crawler C4 C4-3 C4-3-1
19 Insects C5 C5-1 C5-1-1
Data Source=[server name]; Provider=msolap.2; Initial
20 Insects C5 C5-1 C5-1-2
Catalog=Clustered Zoo
From these cluster relationship tables, we created a fact
This connection string identifies the server name on
table. The two tables output from GSOM, namely Cluster
which the OLAP server is hosted. Initial catalog is the
Name Table and Cluster Hierarchy Table are used as
OLAP database name which has the cubes stored in it.
dimensions to create a star schema. The fact table is
The prototype uses a function to display the cube data on
created and updated using SQL queries as shown below.
the front end interface. The function used to display cube
data on the client interface is as follow.
• Fact Table Creation
CREATE TABLE [Fact_table] ( ZooID int
olapClient1.ShowData();
identity(1,1) Not Null, [AnimalID] [int] NULL ,
© 2010 ACADEMY PUBLISHER34 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
This function is responsible for showing the cube data store cubes data. The star schema has one fact table and
so that the user can be facilitated with graph, charts and two dimension tables. Using this fact table and dimension
reports. Figure 16 shows the cluster_zoo_cube data using tables we created a cube and named it as
the prototype. Clustered_Forest_Cube. This cube was stored in the
By using this interface, the user can drill down and roll OLAP database and that database was the source for the
up on the cube data and can see an instant manipulation prototype. The cube was connected to the prototype and
of the chart. Switching from chart view to grid view is the cube data was shown on the front end. Readers are
easy. Reports can be managed and saved using the report referred to [31] for more details of the process.
viewer. This interface offers an interactive visualization
of charts. The basic operations that can be performed TABLE III.
using this prototype are depicted in figures 17, 18 and 19. CLASS DISTRIBUTION OF FOREST COVER TYPE [3]
The bar charts show the hierarchy of the number of Name No. of Records
animals present in each cluster. Spruce/Fir 211840
Lodgepole Pine 283301
Ponderosa Pine 35754
Cottonwood Willow 2747
Aspen 9493
Douglas-fir 17367
Krummholz 20510
Total 581012
Figure 15. Chart View of The Fact table for Forest Cover type data set is in a
Clustered Forest Data Cube. similar format as in Table 1 in Experiment 1. Using this
fact table and dimension tables we created a cube in the
Figure 16. Drill Down on
same way that was discussed in the previous experiment
Cluster 1 (C1)
and named it as Clustered_Forest_Cube. This cube was
stored in the OLAP database and that database became
the source for the prototype. The cube was connected
with the prototype and the cube data was shown on the
front end. Figure 20 depicts the drill down and roll up
operations performed on the clustered cube data showing
the number of forests present in each cluster.
Figure 18. Drill Down on
Cluster 3 (C3)
Figure 17. Drill Down on
Cluster 2 (C2).
It is obvious from the prototype that the user can see
the information of different clusters and can select any
cluster of relevant interest for analysis. The presence of
this unbiased grouping of data provides the user not only
the selection of interested groups for OLAP operations
but also to simplify processing of queries.
B. Experiment 2 – with Complex Data
Figure 19. Samples of Drill-Down Operation on Clustered Forest Data.
The experiment was conducted over a large data set of
Forest Cover Type [14]. The data set has 581,012 It is clear from the prototype that the user can perform
instances and 54 attributes. The attribute breakdown analysis on the cluster of his/her choice. The visually
shows is has 12 measures with 54 columns of data from enriched interface allows interactive exploration of cube
which 10 are quantitative variables, 4 are binary data hence enhances the power of OLAP system using the
wilderness areas and 40 binary soil type variables. The proposed architecture.
data represent a typical analytic situation of certain
complexity. The main purpose of the experiment is to VII. DISCUSSION
validate the capabilities of our proposed architecture. We
The results of the experiments are discussed pertaining
tested both clustered and non-clustered based approaches
to the Forest Cover Type data set which represents an
on the data sets. The class distribution of the Forest Cover
example of large size and complex data. This section is
Type data set is shown in table 3.
divided into two segments. In the first part we discuss the
Firstly, the dataset is fed into the GSOM to generate
level of performance improvement achieved in terms of
hierarchical clusters. Hierarchical clusters were then
cube construction time. The second segment discusses the
transformed into relational tables. From the relational
degree of visualization enhancement for the cube data
schema we created a Star Schema. An OLAP database
called Clustered_ForestCovertype has been created to
© 2010 ACADEMY PUBLISHERA. Performance Improvement approach is more suitable. For instance, the experiment
We have performed experiments for both clustered and performed clearly shows that if we increase the size of
non-clustered Forest Cover Type dataset. The non- both clustered and non clustered data it affects the
clustered data set had originally 581,012 records. This processing time of cube. Interestingly, the rate of increase
huge amount of data has been loaded in a MS SQL Server of processing time when the data is not clustered is quite
database called ForestCoverType. high. It can be seen in the graph that distance between the
From this database a star schema has been generated two lines keep on increasing as the data increases. For
using SQL queries, some of which are exposed in Figure the clustered data the line remains below 200 units of
21. This schema became the source for the cube processing time but it reaches up to 1000 units for the
construction. From this non-clustered data set we same non clustered data.
generated a cube and named it Forest Cube, having 3 It is evident from the time comparison graph that the
dimensions and 1 fact table. cube processing time can be reduced by using the
clustered data, and the user can perform the targeted and
fast multidimensional analysis on the clustered cube.
Hence, it is shown from our benchmarking experiments
that our proposed architecture yields performance
improvement of OLAP data cube in computational time.
Time Comparison
1200
1000
800
Figure 20. SQL Queries for star schema generation.
600
The Forest Cube process time was calculated and it 400
was observed that it took 1 second to process 60,180 rows
200
of data to construct the cube.
We carried out the same experiment on the clustered 0
data which was first fed into the GSOM and then
1 3 5 7 9 11 13 15 17 19
transformed into the Star Schema. A cube named Volume of Data (00)
Clustered_Forest_Cube was generated using Microsoft
Analysis Services depicted in Figure 22.
Figure 21. Star schema in Cube Editor of MS Analysis Services.
This clustered data cube was almost instantly
processed and its construction time was less than a
second. Figure 23 shows the cube construction time
comparison of both clustered and non-clustered data.
The results of the experiments to calculate the
construction time of a cube shows that the clustered data
cube takes less time as compared to the non-clustered
data cube. The significant thing about the graph is the
rapid variation in the processing time. The processing
time line of the non clustered data is increasing rapidly as
the volume of data increases. In the case of clustered data
the processing time does not increase so sharply. It is
identified that if huge amount of data has to be dealt with
in the construction of OLAP cube then the clustered
emiT
gnissecorP
JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 35
Non cluster Cube Processing time in (00) sec
Cluster Processing time in sec
Figure 22. Cube Processing Time Graph.
B. Visualization Improvement
We have constructed cubes for both Zoo and Forest
Cover Type data sets using Microsoft’s Analysis Services
OLAP engine. With the development of the prototype and
embedding the OLAP data visualization controls by
Dundas Software, we provide OLAP users with a rich
user interface to perform targeted and interactive visual
exploration of the data.
The user can view the same data in a number of charts
and graphs simply by selecting the chart type from the
toolbar in our prototype. For the sake of demonstration,
we show the Clustered Zoo cube data using our
prototype.
Figure 24 shows the outputs of the animal cube in
various visual chart formats from our prototype.
Furthermore, users can perform an interactive analysis
on the grid as well. Users can drill down and roll up to a
level of detail using the “+” and “-” buttons present on
the grid and charts. This interactive visualization is
enhanced since the previous work only used the GROUP
BY clause of SQL queries to retrieve data from the source
system. The comparison of the previous data
representation and the representation using the developed
prototype is shown in Figure 25.
© 2010 ACADEMY PUBLISHER36 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
Figure 23. Preview of Different Chart Types Using Our Developed Prototype.
Figure 24. Comparison of Visual Representation of Data by the Old and New Prototypes.
emphasized the benefits of integrating performance
VIII. CONCLUSION AND FUTURE WORK improvement with visualization enhancement techniques.
Currently we are working on the dynamic generation
In this paper we proposed integrating OLAP
of relational tables from the GSOM data. In addition to
performance and visualization enhancement techniques.
this we are also working on other Data Mining techniques
To support this integration, we devised an architecture for
that can be integrated with OLAP systems to enhance its
the integrated enhancement using GSOM which
analysis capabilities.
generates hierarchical clusters as data input for OLAP.
Furthermore, we are focusing on identifying the other
Hierarchical clusters help to enhance targeted analysis in
limitations of the current OLAP systems. We are
OLAP by views of clusters which is not possible in
exploring how OLAP can be further extended and
relational database.
enhanced to meet the new challenges and to make it a
The proposed architecture along with its components is
more effective, efficient and intelligent OLAP system.
described in details. To demonstrate its advantages, a
prototype has been developed and experiments are
REFERENCES
conducted over two real-life data-sets. It is observed that
our architecture is relatively easy to implement and [1] S. Mansmann and M. Scholl, “Exploring OLAP aggregates
manage. Experimental results show that OLAP with hierarchical visualization techniques,” in Proc. of
performance and visualization can be enhanced ACM Symposium on Applied Computing, 2007, pp. 1067-
1073.
significantly. At the end we have compared the cube
constructing times with those of the previous work, and
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 37
[2] S. Asghar and M. Usman, “Enhancing OLAP performance clustered databases,” in Proc. of 4th ACM Int’l Workshop
and visualization,” in Proc. of 6th Int’l Conf. on on Data Warehousing and OLAP, 2001, pp. 48-55.
Information Science Technology and Management [21] R. B. Messaoud, O. Boussaid and S. Rabaseda, “A new
(CISTM), 2008, pp. 59-70. OLAP aggregation based on the AHC technique,” in Proc.
[3] S. Asghar, D. Alahakoon and A. Hsu, “Enhancing OLAP of 7th ACM Int’l Workshop on Data Warehousing and
functionality using self-organizing neural networks,” OLAP (DOLAP), 2004, pp. 65-72.
Neural, Parallel and Scientific Computations, vol. 12, no. [22] A. S. Maniatis, P. Vassiliadis, S. Skiadopoulos and Y.
1, pp. 1-20, March 2004 Vassiliou, “Advanced visualization for OLAP”, in Proc. of
[4] S. Chaudhuri and U. Dayal, “An overview of data 6th ACM Int’l Workshop on Data Warehousing and OLAP
warehousing and OLAP technology,” ACM SIGMOD (DOLAP), 2003, pp. 9-16.
Record, vol. 26, no. 1, pp. 65-74, March 1997. [23] A. Voss, V. Hernandez, H. Voss and S. Scheider,
[5] S. Goil and A. Choudhary, “High performance OLAP and “Interactive visual exploration of multidimensional data:
data mining on parallel computers,” Data Mining and Requirements for common GIS with OLAP,” in Proc. of
Knowledge Discovery, vol. 1, no. 4, pp. 391-417, Dec. 15th Int’l Workshop on Database and Expert Systems
1997. Applications (DEXA), 2004, pp. 883-887.
[6] J. Hipp, U. Guentzer and G. Nakhaeizadeh, “Algorithms [24] M. Scotch and B. Paramanto, “SOVAT: Spatial OLAP
for association ruling mining – a general survey and visualization and analysis tool,” in Proc. of 38th Annual
comparison,” ACM SIGKDD Explorations Newsletter, Hawaii Int’l Conf. on Systems Sciences (HICSS), 2005, p.
vol. 2, no. 1, pp. 58 – 64, June 2000. 165.
[7] R. C. Holte, “Very simple classification rules perform well [25] A. Cuzzocrea, D. Sacca and P. Serafino, “A hierarchy
on most commonly used datasets,” Machine Learning, vol. driven compression technique for advanced OLAP
11, no. 1, pp. 63-90, April 1993. visualization of multidimensional data cubes”, in Proc. of
[8] A. K. Jain, M. N. Murty and P. J. Flynn, “Data clustering: 8th Int’l Conf. on Data Warehousing and Knowledge
A review,” ACM Computing surveys (CSUR), vol. 31, no. Discovery (DaWak‘06), Springer Verlag 2006, pp. 106-
3, pp. 264-323, Sep. 1999. 119.
[9] M. J. Shaw, C. Subramaniam, G. W. Tan and M. E. Welge, [26] K. Techapichetvanich and A. Datta, “Interactive
“Knowledge management and data mining for marketing,” visualization for OALP,” in Int’l Conf. on Computational
Decision Support Systems, vol. 31, no. 1, pp. 127-137, May Science and its Applications (ICCSA), 2005, pp. 206-214.
2001. [27] S. Mansmann and M.Scholl, “Extending visual OLAP for
[10] M. Sifer, “A visual interface technique for exploring handling irregular dimensional hierarchies,” in Proc. of
OLAP data with coordinated dimension hierarchies,” in 8th Int’l Conf. on Data Warehousing and Knowledge
Proc. of 12th ACM Int’l Conf. on Information and Discovery (DaWaK'06), Springer Verlag 2006, pp. 95-105.
Knowledge Management (CIKM), 2003, pp. 532-535. [28] S. Mansmann and M. Scholl, “Visual OLAP: A new
[11] C. Siedman, Data Mining with Microsoft SQL Server 2000 paradigm for exploring multidimensional aggregates,” in
Technical Reference, Microsoft Press, Redmond, WA, Proc. of IADIS Int’l Conf. on Computer Graphics and
2001. Visualization (CGV), 2008, pp. 59-66.
[12] S. Soni and W. Kurtz, “Analysis services: Optimizing cube [29] B. Fritzke, “Growing cell structures - a self organizing
performance using Microsoft SQL server 2000 analysis network for unsupervised and supervised learning,” Neural
services,” Microsoft SQL Server 2000 Technical Articles Networks, vol. 7, no. 9, pp. 1441-1460, 1994.
March 2001. [30] Dundas Chart for ASP.NET - OLAP Services, Overview of
[13] “Dundas Chart for ASP.NET,” Help Document, [Online], Dundas OLAP Architecture, 2005 - 2009 Dundas Data
http://support.dundas.com/OnlineDocumentation/WebChar Visualization,
t2005. Source:http://support.dundas.com/OnlineDocumentation/
[14] J. A. Blackard, D. J. Dean and C. W. Anderson, Forest WebOLAP/Overview%20of%20Dundas%20OLAP%20Ar
cover type, The UCI KDD Archive [http://kdd.ics.uci.edu]. chitecture.html. [Online].[Accessed: Dec. 2008].
Irvine, CA: University of California, Department of [31] M. Usman, S. Asghar, S. Fong, “An Architecture of
Information and Computer Science (1998). Integrated Enhancement of OLAP Using Growing Self
[15] E. Keogh, C. Blake and C. J. Merz, “UCI repository of Organizing Neural Networks”, International Conference
machine learning database,” 1999. [Online]. http: / on Computer Engineering and Systems, Cairo, Egypt,
/www.ics.uci.edu /~mlearn /MLRepository.html IEEE, submitted.
[16] T. Imielinski, A. Virmani and A. Abdulghani, “DataMine:
Application programming interface and query language for
database mining,” in Proc. of 2nd KDD Conf., 1996, pp.
256-262.
[17] D. Alahakoon, S. K. Halgamuge and B. Srinivasan,
“Dynamic self organising maps with controlled growth for
knowledge discovery,” IEEE Transactions on Neural
Networks, vol. 11, no. 3, pp. 601-614, May 2000.
[18] T. Kohonen, Self-Organising Maps, 3rd ed. Springer-
Verlag, Berlin, 2001.
[19] V. Markl, F. Ramasak and R. Bayer, “Improving OLAP
performance by multi-dimensional hierarchical clustering,”
in Proc. of 1999 Int’l Symposium on Database Engineering
and Applications, 1999, p. 165.
[20] D. Theodoratos and A. Tsois, “Heuristic optimization of
OLAP queries in multidimensionally hierarchically
© 2010 ACADEMY PUBLISHER38 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
Design and Implementation of an Online Social
Network with Face Recognition
Ray K. C. Lai
University of Macau, Av. Padre Tomás Pereira, Taipa, Macao, China
Email: chonchondotcom@gmail.com
Jack C. K. Tang, Angus K.Y. Wong and Philip I.S. Lei
Macao. Polytechnic Institute, Rua de Luis Gonzaga Gomes, Macao, China
Email: { jacktang, kywong, philiplei }@ ipm.edu.mo
Abstract—This paper presents the idea of an online social at least until the 1960s [2]. However, most face
network that makes use of the face recognition technology. recognition algorithms were developed in 1980s and
With the technology, friend relationship can be established 1990s. Two most common of them are Principal
without the need of having the text-based information of a
Component Analysis (PCA) and Independent Component
user, friend recommendation algorithm can be more
Analysis (ICA). Kirby and Sirovich were among the first
accurate, and face tagging can be done automatically. The
to apply PCA to face images, and showed that PCA is an
design and implementation issues of such system will be
optimal compression scheme that minimizes the mean
discussed. The prototype of our proposed social network
will be demonstrated to show the feasibility of adopting the squared error between the original images and their
face recognition technology in online social networks. reconstructions for any given level of compression [3][4].
Turk and Pentland popularized the use of PCA for face
Index Terms—face recognition, face tagging, social networks recognition [5]. PCA were matched the images in the
database by projecting them onto the basis vectors and
I. INTRODUCTION finding the nearest compressed image in the subspace
(called eigenspace). ICA can also be used to create
Social network is a set of people (or organizations or
feature vectors that uniformly distribute data samples in
other social entities) connected by a set of socially-
subspace [6][7]. This conceptually very different use of
meaningful relationships [1]. Online social network has
ICA produces feature vectors that are not spatially
become very popular in recent years. Some popular social
localized. Instead, it produces feature vectors that draw
networking websites have hundreds of million users
fine distinctions between similar images in order to
registered. In this kind of sites, users can update their
spread the samples in subspace.
personal profiles, notify friends about themselves, play
Though the face recognition technology has been well
game and share photos with their friends internationally.
established, there are no online social networks using it.
Before interacting with friends, a user must add them
The technology can provide a number of advantages to
to form a relationship. In the existing social networking
online social networks, including:
web sites, it is found that, to find a friend, the web sites
1. Friends on photos can be searched even without
are commonly based on the text-based information such
text-based information. It is particular useful when we
as email addresses, names of friends, school names of
only took photo with a new friend but forgot to exchange
friends, etc. Though this approach is working, in this
the contact.
paper, we argue that face recognition can be used to
2. The friend recommendation algorithm can be more
improve the friend searching and other services in online
accurate because it can make use both the text-based and
social network.
face information.
Research in automatic face recognition can date back
3. Face tagging can be automatically done with the
face recognition technology.
Manuscript received October 22, 2009; revised December 29, 2009. In the rest of this paper, we will show the design and
Ray K. C. Lai is a research master student at University of Macau, architecture of our proposed online social network with
Av. Padre Tomás Pereira, Taipa, Macao, China (e-mail:
face recognition functions. After that, we will reveal how
chonchondotcom@gmail.com).
Jack C. K. Tang is a research student at Macao Polytechnic Institute, the face recognition technology can be used to design a
Rua de Luis Gonzaga Gomes, Macao (e-mail:jacktang@ipm.edu.mo). new friendship search algorithm, to generate friend
Angus K.Y. Wong is an associate professor at Macao. Polytechnic recommendation list, and to design photo-based search
Institute, Rua de Luis Gonzaga Gomes, Macao (e-mail:
and match functions. Finally, we will discuss the
kywong@ipm.edu.mo).
Philip I.S. Lei is an associate professor at Macao. Polytechnic implementation of the system and demonstrate our
Institute, Rua de Luis Gonzaga Gomes, Macao (e-mail: prototype social network.
philiplei@ipm.edu.mo).
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.38-42JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 39
TABLE I.
COMPARISON OF ONLINE SOCIAL NETWORKS WITH AND WITHOUT FACE
RECOGNITION TECHNOLOGY
Existing social Social networks using face
networks recognition technology
The personal Any face of a friend
Forming a
information of a friend appearing in any pictures
relationship
is needed of the user is sufficient
Searching and
Using text-based Using face-recognition
recommending a
search algorithm algorithm
friend
Tagging a person Manually Automatically
II. SYSTEM ARCHITECTURE
As shown in Figure 1, the system consists of the
following three components:
Figure 2. Common face recognition process
• User Database
• Face recognition Web service The normal procedures include two parts: the reference
• Friendship algorithm part and the identification part. In the reference process,
the face recognition engine will first detect the faces for
A. User Database
the raw image from the photo. It will then convert the
In addition to the basic information of users, the
photo to grayscale so as to eliminate the lighting effect
information related to the face recognition feature is
and make some image process (PCA or ICA) to get a
needed. The formation includes:
lower resolution from the face. After that, it will extract
• Face - the user face identity, type and subject.
the face feature and store the face to the training database.
• Photo - photo information, ID and subject.
In the face identification process, the engine will go
• Photocomment - the information of each user’s
through the same procedures to extract the face feature
comment on each photo.
and make comparison with the training database. But in
• Friendship - the relationship information between our implementation, we will do some normalization
each user. before store to the training database. We will normalize
• Recognition - the relationship information between all face to the same resolution and size that can enhance
the faces of photo and each friend.
• Invite - the relationship information of inviting
people between user and friendship tables, inviting
TABLE II.
CASES CONSIDERED IN THE FRIENDSHIP ALGORITHM
message and status of invitation.
Case Description Unit Weighting
W(cid:358)(cid:355) (cid:324)(cid:365)(cid:362)(cid:358)(cid:367)(cid:373)
People appeared in
My album No. of appearance 10
my albums
(cid:340)(cid:368)(cid:356)(cid:362)(cid:354)(cid:365) N(cid:358)(cid:373)w(cid:368)(cid:371)(cid:364) (cid:330)(cid:367)(cid:359)(cid:371)(cid:354)(cid:372)(cid:373)(cid:371)(cid:374)(cid:356)(cid:373)(cid:374)(cid:371)(cid:358) (cid:297)(cid:330)(cid:367)(cid:359)(cid:368)(cid:371)(cid:366)(cid:354)(cid:373)(cid:362)(cid:368)(cid:367) (cid:329)(cid:354)(cid:367)(cid:357)(cid:365)(cid:362)(cid:367)(cid:360), (cid:330)(cid:366)(cid:354)(cid:360)(cid:358) (cid:329)(cid:354)(cid:367)(cid:357)(cid:365)(cid:362)(cid:367)(cid:360), (cid:358)(cid:373)(cid:356)(cid:298)
Indirect friend Friend of my friend No. of common friend 8
People appeared in
Friend’s album No. of appearance 6
my friend’s albums
(cid:327)(cid:371)(cid:362)(cid:358)(cid:367)(cid:357)(cid:372)(cid:361)(cid:362)(cid:369)
Someone has photo
(cid:327)(cid:354)(cid:356)(cid:358) U(cid:372)(cid:358)(cid:371) (cid:354)(cid:365)(cid:360)(cid:368)(cid:371)(cid:362)(cid:373)(cid:361)(cid:366) Photo with me that includes myself No. of appearance 4
(cid:339)(cid:358)(cid:356)(cid:368)(cid:360)(cid:367)(cid:362)(cid:373)(cid:362)(cid:368)(cid:367)
(cid:326)(cid:367)(cid:360)(cid:362)(cid:367)(cid:358) (cid:325)(cid:354)(cid:373)(cid:354)(cid:355)(cid:354)(cid:372)(cid:358) Co-photo Someone and me in No. of appearance 2
the same photo
Figure 1. System architecture
the speed of the identification face. After the face
B. Face Recognition Web Service
recognition step, it will provide a score for the face
The face recognition web service handles the tasks
identity.
involving face recognition. It implements with face
The face recognition service is provided by the way of
recognition engine that face localization, enrollment and
Web services, which can make the system more
matching using digital image processing algorithm. The
scalability. When the system becomes larger, the face
process of face recognition consists of a number of steps,
recognition process could become the performance
which are shown in Figure 2.
bottleneck. Because the face recognition service is
provided by web services, more face recognition engines
can be added easily.
© 2010 ACADEMY PUBLISHER40 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
C. Friendship Algorithm After the comparison, the engine will give the face
The friendship algorithm is used to provide the identity a score. We choose the score 50 as the basic
recommendation list for the users. The algorithm makes recognition standard. Score 50 means nearly 0.01% of
use the faces recognized in different cases to generate false acceptance rate (FAR). It is a very acceptable figure
recommendation list. The cases used and the for the face recognition. Once the score of matching face
corresponding weighting for each case are shown in is larger than 50, we would expect the matching face is
Table 2. Note that the exact values the weighting can be the same as the reference face.
revised if necessary.
C. Face Recognition Engine
The friendship algorithm considers different case, and
To implement the face recognition engine in our online
in each case there is a weighting associated. From the
social network, we use VeriLook SDK [8]. VeriLook
values of the weighting, it can be seen that priority has
SDK is a software development kit for face detection and
been considered in the friendship algorithm. That is,
face recognition. It supports multiplatform, Windows,
higher priority is given to the owner’s direct relationship
Linux and MacOS. It also can support C/C++, C#, VB,
and lower priority to the indirect relationship.
Java and Delphi as development language. It contains two
As a result, a final score for each face can be generated
major components -- extractor and matcher. It also
by using the following formula:
provides a camera manager library to support
simultaneous capture from multiple cameras. The
Final score = no. of appearance (my album) x 10 + no.
matching speed of the engine is 100,000 faces per second
of common friend (indirect friend) x 8 + no. of
in 1:N identification. It supports live face detection,
appearance (friend’s album) x 6 + no. of appearance
multiple face processing.
(photo with me) x 4 + no. of appearance (co-photo) x 2
Our effort on this part is to make use the VeriLook
SDK and to build a Web service for face recognition. We
The highest score of the face will appear the top of the
design this engine using Web service that can provide
recommend list, and the lower score faces will appear
backend service with one or more servers. The advantage
below the highest recommend friend according to the
of making it a backend service is that if the face
score.
recognition technology changes in the future, the backend
Based on these final scores, the friendship algorithm
service can be replaced without the need to modify the
will recommend the face to the user when the score
other components.
higher than a threshold.
Actually this prototype will only employ some simple
procedures to detect and recognize faces. So the remote
III. IMPLEMENTATION
service is limited to five methods, DetectFaces,
CompareFace, CompareMultipleFaces,
A. Information Handling
CompareMultipleScore and
The information handling process is the core CompareMulitpleToMultipleFaces as shown in Figure 3.
development of the social network service, as shown in They covered all the necessary functions for face
Figure 1. We used the PHP script language to develop detection and recognition for our web service.
this part. The functions are handling the user accounts,
photo albums, searching and messaging. We use MySQL
to implement our user database.
B. Face recognition process
During the face detection process, we make some
optimizations:
• Convert image to grayscale.
• Extract the image with face detection.
• Resize the face to a suitable size.
• Store the face with base64-encoded string to face
database.
The above optimizations can reduce the memory usage
of each face and improve the performance of face
matching. It can also reduce the process of storing the
raw faces as the faces have been downsized. The
based64-encoded string will be used in the
implementation of the face reorganization web service.
Each user can store multiple face identities -- one main
identity and other supplementary identities. So the user
can store the current face as the main identity and some Figure 3. Web service definition
old faces as supplementary identities. These faces will
store in the trained database for face matching uses.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 41
D. System Operation • Click in the photo to examine the face tagging in
album.
After testing the correct operations of the system, we
have invited about 70 new users to use the system for
testing user interface and performance. In the above
testing stage, we have set up one machine for handling
social network infrastructure and user database to serve
web client, and set up another machine for handling face
recognition engine to provide Web service. The
configurations of the machines are:
• Core 2 Duo 3.0 GHz
• 1GB DDR2 Memory
Figure 4. Establishing a friendship with the face recognition process
• 250GB SATA II Hard Drive
Figure 4 illustrate the system operation of how a user
(UserB) meets another (UserA) using the face recognition IV. DEMONSTRATIONS
technology.
In this section, we will demonstrate how our online
1. Firstly, when UserB registers to the system, besides his social network provides the features of recommendation
basic personal information, he needs to submit a photo list from photos, face identity database and face tagging.
containing his face clearly.
A. Testing
2. When the system receives his face, it will look up the
Figure 5 shows the first page after the user logged in
face in the database about UserB. If the face does not
the system. The recommend list is shown on the lower
exist, it will add the face to the database.
left hand side. By clicking on the “Recognize my faces”,
3. Suppose UserA has taken a photo with UserB in a
the user can upload photo to the face identity database.
social event, and after the event, UserA uploaded the
User may upload photo with the user face only or with
photo to his album in the system.
other friends. The system will detect the faces and let the
4. The system will try to detect the faces on the photos
user choose the right face for the database. After this
UserA just uploaded.
process, the user will have the face identity for the system.
5. Since the system has the faces of UserA and UserB, it
The user may have more than one face identity in the
will put their relationship to the friendship algorithm to
system.
rank and make the recommendation lists for both UserA
and UserB. When the user clicks the “edit” link beside the
6. As a result, UserA can find UserB in his “Recognize my faces”, the corresponding face database
recommendation list, and vice versa. After the friend will appear, as shown in Figure 6, so that the user can
invitation by either UserA or UserB, they can become delete or choose the right head for the profile. This face
friend. database is actually a trained database of face recognition
process. It stores all the possible face identities for face
E. Testing
matching process in the friendship algorithm processing.
In the final stage of the development, a set of sample
data was used for testing the functionality, acceptance B. Face Tagging
and accuracy of the system. The sample data will include Face tagging is a very useful feature in the system. The
some text input, photo upload and comparison test. Some existing social network web sites commonly require a
sample accounts and qualified photos are required for this user to manually select and type name in the appropriate
test. We have collected the sample data of 30 users from area of a picture. As our system employs the face
existing social network application. The size of photo will recognition technology, it can recognize faces
be adjusted to around 30k for faster web page response automatically without user interaction. If the user forgot
and lower network latency. But the quality of the photos the name of his friend, he can use the feature to list the
and faces must be maintained to an acceptable ratio. name of his friend from the name tagging.
The following testing procedures were run to ensure
the correctness of the system:
• Create multiple user accounts.
• Upload the user photo that the face will be used as a
face identity.
• Create albums with multiple accounts.
• Upload photos to multiple albums.
• Run the friendship algorithm scripts to create
recommend lists.
• Make friends with or without the recommend lists.
© 2010 ACADEMY PUBLISHER42 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
experience and user friendly than the existing social
networks. The features include recommendation list by
photos, face identity database and face tagging.
ACKNOWLEDGMENT
The work described in this paper is supported by
Macao Polytechnic Institute Research Grant (No.:
RP/ESAP-1/2009).
REFERENCES
[1] Barry, W. (1996). For a social network analysis of computer
networks: a sociological perspective on collaborative work
and virtual community.
[2] W.W. Bledsoe, The model method in facial recognition,
Panoramic Research, Inc., Palo Alto, CA PRI:15, August
1966.
[3] M. Kirby, L. Sirovich, Application of the Karhunen-Loeve
procedure for the characterization of human faces, IEEE
Transactions on Pattern Analysis and Machine Intelligence
Figure 5. Web interface - my desktop 12 (1990) 103–107.
[4] L. Sirovich, M. Kirby, A low-dimensional procedure for the
characterization of human faces, Journal of the Optical
Society of America 4 (1987) 519–524.
[5] M. Turk, A. Pentland, Eigenfaces for recognition, Journal of
Cognitive Neuroscience 3 (1991) 71–86.
[6] M.S. Bartlett, H.M. Lades, T.J. Sejnowski, Independent
component representations for face recognition, presented
at SPIE Symposium on Electronic Imaging: Science and
Technology, Conference on Human Vision and Electronic
Imaging III, San Jose, CA, 1998.
[7] M.S. Bartlett, J.R. Movellan, T.J. Sejnowski, Face
recognition by independent component analysis, IEEE
Transaction on Neural Networks 13 (2002) 1450–1464.
[8] VeriLook SDK: http://www.neurotechnology.com/.
Ray K. C. Lai received the B.Sc degree in Computer Studies
from Macau Polytechnic Institute, and is currently working
toward the M.Sc degree in Software Engineering from
University of Macau.
Jack C. K. Tang received the B.Sc degree in Computer
Studies from Macau Polytechnic Institute. His research interests
include online social networks and computer networks.
Figure 6. Web interface – face database
Angus K.Y. Wong is an associate professor at Macao
Polytechnic Institute. His research interests include Internet
systems, network infrastructure security, and human–computer
V. CONCLUSION
interactions. Wong has a BSc and a PhD in information
technology from the City University of Hong Kong.
We implement a prototype to demonstrate the basic
function of user registration, create and edit albums,
Philip I.S. Lei is an associate professor at Macau Polytechnic
upload and delete photos, edit some text fields, and make Institute and a PhD candidate in computer science at Sun Yat-
friends with/without recommend list. Sen University. His research interests include user interface
design, social network analysis, and data mining.
Our online social network with face recognition
technology provides features that improve user
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 43
Dynamic Differential Evolution for Constrained
Real-Parameter Optimization
Youyun Ao1, Hongqin Chi2
1School of Computer and Information, Anqing Teachers College, Anqing, China
Email: youyun.ao@gmail.com
2Department of Computer, Shanghai Normal University, Shanghai, China
Email: chihq@shnu.edu.cn
Abstract—Differential evolution (DE) has been shown to be of the parameter space S (obviously, F ⊆S ) which
a simple and effective evolutionary algorithm for global
satisfies the equality and inequality constraints.
optimization both in benchmark test functions and many
Population-based evolutionary algorithm, mainly due
real-world applications. This paper introduces a dynamic
to its ease to implement and use, and its less
differential evolution (D-DE) algorithm to solve constrained
susceptibleness to the characteristics of the function to be
optimization problems. In D-DE, a novel mutation operator
is firstly designed to prevent premature. Secondly, the scale optimized, has become a very popular option to solve
factor F and the crossover probability CR are dynamic and constrained optimization problems in benchmark test
adaptive to be beneficial for adjusting control parameters functions and real-world applications [5]. Muñoz Zavala
during the evolutionary process, especially, when done et al. [6] proposed a new constrained optimization
without any user interaction. Thirdly, D-DE uses orthogonal algorithm based on improved particle swarm optimization
design method to generate initial population and reinitialize
(COPSO). In order to keep diversity, COPSO introduces
some solutions to replace some worse solutions during the
a hybrid approach based on a modified ring neighborhood
search process. Finally, D-DE is validated on 6 benchmark
structure with two new perturbation operators for
test functions provided by the CEC 2006 special session on
perturbing the particle swarm optimization (PSO)
constrained real-parameter optimization. The experimental
results obtained by D-DE are explained and discussed, and memory. In addition, COPSO adopts a new and special
some conclusions are also drawn. handling technique for equality constraints where a
dynamic tolerance value is adjusted to allow the survival
Index Terms—constrained optimization, mutation scheme, of unfeasible particles. Furthermore, COPSO is applied to
differential evolution, evolutionary algorithm, constraint the solution of state-of-the-art benchmark test functions
handling
and various engineering design problems. Liang and
Suganthan [7] proposed a dynamic multi-swarm particle
I. INTRODUCTION
swarm optimizer with a novel constraint-handling
Many real-world optimization problems in science and mechanism (DMS-PSO). DMS-PSO adopts a novel
engineering involve a number of constraints which the constraint-handling mechanism based on multi-swarm.
optimal solution must satisfy. These problems are also Different from the existing constraints handling methods,
called constrained optimization problems or nonlinear sub-swarms are adaptively assigned to explore different
programming problems. We are most interested in the constraints during the search process. Additionally,
general constrained optimization problems, which are all DMS-PSO introduces Sequential Quadratic Programming
transformed into the following format [1], [2], [3], [4]: (SQP) method to improve local search ability. Finally,
r r DMS-PSO is applied to the solution of constrained real-
Minimize f(x) r, x=[x 1,x 2,...,x n]∈ℜn
parameter optimization. Mezura-Montes et al. [8]
Subject tog j(x)≤0, j=1,2,...,q (1) p r o p o s e d a m o d i f i e d d i f f e rential evolution for constrained
r
h j(x)=0, j=q+1,q+2,...,m optimization (MDE). In order to increase the probability
of each parent to generate a better offspring, MDE allows
WhereL
i
≤x
i
≤U i,i=1,2,...,D
each solution to generate more than one offspring but
Here n is the number of the decision or parameter using a different mutation operator which combines
r
variables (that is, xis a vector of sizeD), the ithvariable information of the current parent to find new search
r
x varies in the range[L,U ]. The function f(x) is the directions. Besides, MDE employs three selection criteria
i i i
r based on feasibility to deal with the constraints and
objective function, g (x) is the jthinequality constraint
j adopts a diversity mechanism to maintain infeasible
r
and h (x) is the jthequality constraint. The decision or solutions located in promising areas of the search space.
j
Takahama and Sakai [9] proposed a novel constrained
D
search spaceS is written asS =∏ [L i,U i], and the optimization algorithm by theεconstrained differential
i=1
r r evolution with gradient-based mutation and feasible elites
feasible space F
r
, expressed as F ={x∈S|g j(x)≤0,
(εDE). Firstly, εDEapplies theεconstrained method to
j=1,2,...,q; h j(x)=0, j=q+1,q+2,...,m, is one subset differential evolution. Secondly, to solve problems with
Manuscript received August 24, 2009; revised October 21, 2009.
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.43-5144 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
gm p rra aon dby il ee ne m tq -su
b
a al sfi o ety dr c mo nn u us m tt ar e ta iri oin c nt a s l af na os dpt e t fr im e, aw i sz ih a bi t lc i eoh n ea , lr ie ε
t
eDv se Er py rp ed r si o ef pf ri voc isu nel gst xr it+1 =  u xrr i it t+ 1 ,, i of t( hf e( rur wit i+ s1 e)< f(xr it)) (4)
strategy. Finally,εDE is tested on 24 benchmark test Where the function f is the objective function and the
f cu on nc st ti ro an ins
e
dp rov reid ae l-d
p
ab ry
a
mth ee
te
rC E oC
p
t2 im00 i6
za
s tip oe nc .i al Ds ie fs fs ei ro en
n
to ian
l
condition f(ur it+1)< f(xr it) means the individual ur it+1is
r
evolution (DE) [10], [11], a relatively new evolutionary better thanxt.
i
technique, has been shown to be simple and powerful and 1: Generate initial populationP0.
has been widely applied to both benchmark test functions 2: EvaluateP0and let generation countert=0.
and real-world applications [12]. After analyzing the
3: While (the stopping criterion is not satisfied) do{
existing evolutionary algorithms, this paper introduces a 4: For each individualxr t, its offspringxr t+1is generated
i i
new dynamic differential evolution (D-DE) algorithm for
5: by mutation, crossover and selection operators.
constrained real-parameter optimization efficiently. 6: EvaluatePt+1and let t=t+1}
The remainder of this paper is organized as follows.
Figure 1. The general framework of DE.
Section II briefly introduces the basic idea of DE. Section
III describes in detail the D-DE algorithm. Section IV
presents 6 benchmark test functions. Section V presents
III. THE PROPOSED ALGORITHM DDE
experimental settings adopted by D-DE, conventional DE
and GA, respectively. Section VI provides an analysis of
A. Orthogonal Initial Population
the results obtained from our empirical study. Finally,
r r r
some conclusions and some possible paths for future Generally, the initial population P0 ={x 10,x 20,...,x N0} of
research are provided in Section VII.
evolutionary algorithm is randomly generated as follows:
II. THE BASIC IDEA OF CONVENTIONAL DE
∀i≤N,∀j≤D:x i0
,j
=L
j
+r j×(U
j
−L j) (5)
Where N is the population size, D is the number of
r
Let us assume that x it =[x it ,1,x it ,2,...,x it ,D] are solutions at variables,r jis a random number between 0 and 1, the jth
r r r
generation t,Pt ={x 1t,x 2t,...,x Nt } are population, where D variable of xr 0, written as x0 , is initialized in the range
i i,j
denotes the dimension of solution space, N is population
[L ,U ]. In order to improve the search efficiency, this
size. In conventional DE, the child population Pt+1 is j j
generated through following operators [10], [13]: paper employs orthogonal design method to generate the
initial population, which can make some points closer to
A. Mutation Operator the global optimal point and improve the diversity of
r r
For eachxtin parent population, the mutant vectorvt+1is solutions. The orthogonal design method is described as
i i
follows [14]:
generated according to the following equation: r
vr it+1 = xr rt
1
+F×(xr rt
2
−xr rt 3) (2) decF io sir
o
na n vy
a
rg iaiv be len xin vd aiv rii ed su a il
n
x t= he[ x r1 a, nx g2, e. .. [, Lx D ,U] , ]t .h e
H
eit rh
e,
i i i
Where r 1,r 2,r 3∈{1,2,...,N}\i are randomly chosen and
each x is taken as each factor of orthogonal design. Let
i
mutually different, the scaling factor F is used to control
r r us assume that each factor holds Q levels, namely,
amplification of the differential variationx rt
2
−x rt 3.
quantize the domain [L i,U i] into Qlevels α1,α2,...,αQ.
B. Crossover Operator The jth level of the ithfactorαi,jis defined as follows:
r r
For each individualx it, a trial vector u it+1is generated by L
i
, j=1
the following equation: a
i,j
= L
i
+(j−1)(U Qi− −1L i) , 2≤ j≤Q−1 (6)
u it ,+ j1 =  v xit it, ,+ j j1 ,, oif th(r ea rwnd ise≤CR|| j=rand[1,D]) (3)
Thereafter,
  wU ei
c r e a t e t h e o r t h o
, gj o= naQ
l arrayM =(b i,j)
N×D
Where rand is a uniform random number distributed with D factors and Qlevels, whereN is the number of
between 0 and 1,rand[1,D]is a randomly selected index level combinations. The procedure of generating the
from the set {1,2,...,D} , the crossover probability orthogonal arrayM =(b i,j) N×Dis described as follows:
CR∈[0,1] is used to control the diversity of individuals. 1: for (i=1;i≤N;i++)
C. Selection Operator
2: {b i,1=int((i−1)/Q) modQ;b i,2=(i−1)modQ }
r
3: for ( j=3;j≤D;j++)
The child individual x it+1is selected from each pair of 4: for (i=1;i≤N;i++)
xr tand ur t+1by using greedy selection criterion: 5: {b i,j =(b i,1×(j−2)+b i,2)mod Q }
i i
6: Incrementb i,jby one for 1≤i≤N,1≤ j≤D
Figure 2. Generating orthogonal arrayM =(bi j) N×D.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 45
Therefore, the initial population P0 =(x i0 ,j)
N×D
can be d ne ut mer bm erin ain ng
d
t uh se
u
ad le lyg re ise so ef
t
d toep 2en od re n 3c .y
T
o wn
o
t h pe
a
rg ae mn ee tr ea rt sio tn
generated by using the orthogonal array M =(b i,j) N×D,
and T are the current generation number and the maximal
where the jthvariable of individual xr i0 is x i0
,j
=a
j,b
. generation number respectively.
i,j At the early stage, D-DE uses a bigger scale factor
F and a bigger crossover probabilityCR to search the
B. Novel Mutation Scheme
solution space to preserve the diversity of solutions and
According to the different variants of mutation, there are
prevent premature; at the later stage, D-DE employs a
several different DE schemes often used, which are
smaller scale factorFand a smaller crossover probability
formulated as follows [10]:
r r r r CR to search the solution space to enhance the local
"DE/rand/1/bin": v it+1 = x rt +F×(x rt −x rt ) (7) search and prevent the better solutions found from being
1 2 3
"DE/best/1/bin": vr it+1 = xr best +F×(xr rt −xr rt ) (8) destroyed.
1 2
"DE/current to best/2/bin": D. Repair Rule
r r r r r r
v it+1 =x it +F×(x
best
−x it)+F×(x rt
1
−x rt 2) (9) A vef cte tor rcr uro ts +s 1ov are er , i of uo tn sie
d
eo r m tho eir re o bf
o
t uh ne
d
v aa rir eia sb
,
le ths ein vth ioe
l
n ate ew
d
"DE/best/2/bin": i
vr it+1 = xr best +F×(xr rt −xr rt )+F×(xr rt −xr rt ) (10) variable value ur it ,+ j1 is either reflected back from the
1 2 3 4
"DE/rand/2/bin": violated boundary or set to the corresponding boundary
vr it+1 = xr rt +F×(xr rt −xr rt )+F×(xr rt −xr rt ) (11) value using the repair rule as follows [15], [16]:
Where
xr
is
t1
he best
so2 lution3
of the
curr4
ent
p5
opulation,
(L
j
+u it ,+ j1)/2, if(p≤1/3)∧(u it ,+ j1 <L j)
and the
cb oe nst
trol parameter F is usually set to be a constant.
 L j, if(1/3< p≤2/3)∧(u it ,+ j1 <L j)
U ins ci rn eg a sexr be ts ht ec a pn r oim bap br io liv te y th oe f c go en ttv iner gg e sn tuce c ks p ie ne d t hb eu t loal cs ao l u it ,+ j1 =   (2 UL jj +−u ui it t ,, ++ j j11, ) / 2 , ii ff (( pp ≤> 12 // 33 )) ∧∧ (( uu it ,it +,+ j1j1 >< UL jj )) (15)
optimum. In order to overcome the limitations, this paper U j, if(1/3< p≤2/3)∧(u it ,+ j1 >U j)
p for lo lp oo ws se :s a novel variant of mutation, which is defined as  2U
j
−u it ,+ j1, if(p>2/3)∧(u it ,+ j1 >U j)
Where p is a probability and uniformly distributed
r r K/2 r r
v it+1 = x better +F×∑(x rt −x 2t k) (12) random number in the range[0,1].
2k−1
k=1
E. Constraint Handling Mechanism
Where r 1,r 2,...,r
K
∈{1,2,...,N}\i , they are K mutually
In evolutionary algorithms for solving constrained
different and randomly chosen integers. The better
r optimization problems, the most common method to
solution x is a random sample from top N solutions
better a handle constraints is to use penalty functions. Usually
after ranking the current population based on the
equality constraints are transformed into inequalities of
feasibility rule described in the later. The scale factor
the form [4]:
F is a dynamic control parameter and related to the r
generation number, which is defined as follows:
|h j(x)|−ε≤0,j =q+1,q+2,...,m (16)
t Hereεis a tolerance allowed (a very small value) for the
F = F min +(F max −F min)×(1− T)F b (13) equality constraints.
In general, the constraint violation function of one
HereF and F are the bottom and upper boundaries r
min max
individual x is transformed by m equality and inequality
ofF, and usually are set to 0.1 and 0.9 respectively. The
constraints as follows [9], [17], [18]:
exponentF is a shape parameter determining the degree
b r q r m r (17)
of dependency on the generation number and usually is G(x)=∑w jmax(0,g j(x))β+ ∑w jmax(0,|h j(x)|−ε)β
set to 2 or 3. Two parameters t and T are the current j=1 j=q+1
generation number and the maximal generation number Here the exponentβis a positive number and usually set
respectively. to 1 or 2, and the coefficientw is greater than zero. The
j
r
C. Dynamic Control Parameters function value G(x) shows that the degree of constraints
r
In conventional DE, the crossover probabilityCR is a violation of individualx.βis set to 2 andw jis set to 1 in
constant value between 0 and 1. In this study, a dynamic
this study.
crossover probability CRis defined as follows:
In this study, a simple and efficient constraint handling
t
CR=CR
min
+(CR
max
−CR min)×(1− T)CR b (14) t ae lc soh n ai q cu oe
n
o stf
r
afe ina ts i hb ail ni dty l- inb ga s te ed
c
hru nl ie
q
uis
e
i wnt ir to hd ou uc
t
e pd a, rw amhi ec th
er
i ss
.
Here CR and CR are the bottom and upper When two solutions are compared at a time, the following
min max
boundaries of CR , and usually are set to 0.1 and 0.9 criteria are always applied [3]:
1) If one solution is feasible, and the other is infeasible,
respectively. The exponent CR is a shape parameter
b the feasible solution is preferred;
© 2010 ACADEMY PUBLISHER46 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
2) If both solutions are feasible, the one with the better 11865,0.47664475092742,0.47129550835493,0.4662309
objective function value is preferred; 9264167,0.46142004984199,0.45683664767217,0.45245
3) If both solutions are infeasible, the one with smaller 876903267,0.44826762241853,0.44424700958760,0.440
r
constraint violation function value is preferred. 38285956317), the best is f(x*)=−0.80361910412559 ,
F. Algorithm Framework constraint g is close to being active.
1
The general framework of the D-DE algorithm is outlined B. Test functiong04
as follows:
r r r
Minimize f(xr )=5.3578547x 32 +0.8356891x 1x
5
1 2:: G e n ine ir ta iate
li
zo ert h po arg ao mna el
t
ei rn si ,t i aa nl dp o lep tu tla =ti 0on
.
P0 ={x 10,x 20,...,x N0},
r
+37.293239x 1−40792.141
3: EvaluateP0, and rankP0based on feasibility rule.
Subject tog 1(x)=85.334407+0.0056858x 2x
5
4 5:
:
r e fop re a et
a ch individualxr t in the populationPtdo r
+0.0006262x 1x
4
−0.0022053x 3x
5
−92≤0
i g 2(x)=−85.334407−0.0056858x 2x
5
6 7:
:
G e n e tr ha ete
y
K arer a an lsd oo m
m
uin tute ag lle yr s dr
i1
ff, er
2
re,. n.. t,
.
r K∈{1,2,...,N}\i,
r
−0.0006262x 1x
4
+0.0022053x 3x
5
≤0
8: Generate a random integer rj
rand
∈{1,2,...,D}. g 3(x)=80.51249+0.0071317x 2x
5
9: Randomly select a samplex better from topN aindividuals of +0.0029955x 1x 2 +0.0021813x 32 −110≤0
r
10: the current populationPt. g 4(x)=−80.51249−0.0071317x 2x
5
11: for each parameter j∈{1,2,...,K/2} do
12: ur it ,+ j1 =  xr rb e t t e r , j if+ (F ra× n( dxr ≤rt 2k C−1, Rj |− | jxr r =t 2k r,j a) n, d[1,D]) g 5(xr )=9− .30 0.0 00 92 69 19 +55 0.x 01 0x 42 7− 020 6.0 x0 32 x1 58 13x 32 +90≤0


x it ,j, otherwise
r
+0.0012547x 1x
3
+0.0019085x 3x
4
−25≤0
13: end for
r r
g 6(x)=−9.300961−0.0047026x 3x
5
14: Apply repair rule to repairu it ,+ j1if required, and evaluateu it ,+ j1.
−0.0012547x 1x
3
−0.0019085x 3x
4
+20≤0
r r r
15: Replacex itwith the chil rd u it+1in the populationPt+1, if u it+1 Where 78≤x 1≤102, 33≤x
2
≤45 and r27≤x
i
≤45
16 is better, otherwisex itis retained. (i=3,4,5). The optimum solution is x*= (78,33,
17: end for
29.9952560256815985,45,36.7758129057882073),where
18: RankPt+1based on the feasibility rule, then replaceN worse r
b f(x*)=−3.066553867178332e+004 . Two constraints
19: solutions withN orthogonal reinitialized solutions.
b are active (g andg ).
20: RankPt+1based on the feasibility rule, and lett=t+1. 1 6
C. Test functiong06
21:until (the termination condition is achieved)
r
Figure 3. The general framework of the D-DE algorithm. Minimize f(x)=(x 1−10)3 +(x
2
−20)3
r
Subject to g 1(x)=−(x 1−5)2 −(x
2
−5)2 +100≤0
r
IV. TEST FUNCTION SUITE g 2(x)=(x 1−6)2 +(x
2
−5)2 −82.81≤0
In order to validate D-DE, we employ 6 benchmark test Where 13≤x 1 ≤100 and 0≤ x 2 ≤100 . The optimum
r
problemsg04,g06,g08,g11,g12, which are provided solution is x*= (14.09500000000000064,0.84
r
by the CEC 2006 special session on constrained real- 29607892154795668) where f(x*)=-6961.81387558015.
parameter optimization [4], and which are described in Both constraints are active.
the following. D. Test functiong08
A. Test functiong02
Minimize
f(xr )=−∑ in =1cos4(x i ∑)− n2∏
ix
i2in =1cos2(x i) SM ui bn ji em ci tz te
o
gf( 1xr (x)
r
r)= =− xs 12in −3 x( x2 21π 3 +(x x 11 1) ≤+si 0n x( 22 )πx 2)
i=1 g 2(x)=1−x 1+(x
2
−4)2 ≤0
Subject to g
1(xr )=0.75−∏n
x
i
≤0
Where 0≤x
r1
≤10 and 0≤x
2
≤10 . The optimum
i=1
solution is x*= ( r1.22797135260752599,4.24537336612
r n 274885) where f(x*)=-0.0958250 414180359.
g 2(x)=∑x i −7.5n≤0 E. Test functiong11
Where n=20 and
0i <=1
x
i
≤10 (i=1,2,...,n) . The global
Minimize f(
rxr
)=x 12 +(x
2
−1)2
minimum
xr
*= (3.16246061572185,3.12833142812967,
Subject to h(x)= x
2
−x 12 =0
3.09479212988791,3.06145059523469,3.0279291588555 Where −1≤x
1
≤1,−1≤x
2
≤1. The optimum solution is
r
5,2.99382606701730,2.95866871765285,2.92184227312 x*=(-0.707036070037170616,0.500000004333606807)
450,0.49482511456933,0.48835711005490,0.482316427
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 47
r
where f(x*)=0.7499. in Section III. The parameter values employed by the
real-coded GA are set as follows: the population size
F. Test functiong12
N =50, the maximal generation number T =55000, the
Minimize
Sf u( bxr j) ec=
t
− to(1 0 g0 (xr− )( =x 1 (x− 15 −)2 p− )2(x +2 (−
x
25) −2 q− )2(x 3 −5)2)/100 fc P or mo
r
s r=s eo a1v l/ -e cnr
o
p d(r ewo dhb Gea rb Aeil )ni ,t y i ts hP ec t h d= e
is
0 tn r. iu9 bm ua b tn ie od r
n
a o
i
nm f du d et e xa c et ii sso i fn o
o
n rp r cvo rab ora si sb a oi bl vli et ey s
r
+(x
3
−r)2 −0.0625≤0 and mutation operators asηc =20 andηm =20 ; the
Where 0≤ x
i
≤10 (i=1,2,3) and p,q,r =1,2,...,9 . The tolerant value ε=0.0001 . The number of function
feasible region of the search space consists of 93
evaluations (FES) is equal to N×T =275,000 . The
disjointed spheres. A point (x ,x ,x )is feasible if and
obtained solution at the end of N×T FES is used to
1 2 3 measure the performance of the conventional GA. The
only if there exists p,q,rsuch that the above inequality
GA is independently run 30 times on each test function.
r
holds. The optimum solution is x*= (5,5,5) where
r
f(x*)=-1. The solution lies within the feasible region. VI. EXPERIMENTAL RESULTS AND DISCUSSIONS
A. Comparison with Respect to Conventional DE and GA
V. EXPERIMENTAL SETTINGS on 6 Benchmark Test Problems
Using the above experimental settings, the best, mean and
A. Parameter Settings of D-DE worst results obtained by D-DE, DE, and GA are given in
In our experimental study, the parameter values used in Tables I-III. As shown in Table I, D-DE, DE and GA all
D-DE are set as follows: the population sizeN =50, the can find the best solution for each test problemsg08,g11
maximal generation number T =5000, the level number andg12, respectively. For test problemsg04andg06, D-
Q= N, the number of top solutionsN
a
=0.1×N =5, DE and DE can find the best solution and is better than
that of GA. For test problemg02, the best result obtained
the number of replaced solutions N
b
=0.1×N =5 , the
by DE is slightly better than that of D-DE, and obviously
minimal and maximal values of scale factorFare set to
better than that of GA. According to the mean results
F
min
=0.1 and F
max
=0.9 respectively, K =6 , the
given in Table II, the mean result obtained by D-DE is
minimal and maximal values of crossover probability
better than or not worse than that of DE and GA for all
CRare set to CR
min
=0.1 andCR
max
=0.9 respectively,
test problemsg02,g04,g06,g08,g11 andg12, while
the shape parameter valuesF a =3andF b =3 respectively, the mean result obtained by DE is better than or not
the exponentβ=2, the tolerant value ε=0.0001. The worse than that of GA for test problemsg02,g04,g06,
number of function evaluations (FES) is equal to g08andg12except for test problemg11 where the mean
(1+N b)×N×T =275,000. The achieved solution at the result obtained by GA is better than that of DE. Table II
end of (1+N a)×N×T FES is used to measure the shows that the worst result obtained by D-DE is better
than or not worse than that of DE and GA for all test
performance of D-DE. D-DE is independently run 30
problemsg02,g04,g06,g08,g11 andg12, while the
times on each test function.
worst result obtained by DE is better than or not worse
B. Parameter Settings of Conventional DE
than that of GA for test problems g02 , g04 , g06 ,
In our experimental study, the parameter values adopted
g08 and g12 , except for test problem g11 where the
by the conventional DE are set as follows: the population
worst result obtained by GA is better than that of DE.
sizeN =50, the maximal generation numberT =55000,
Additionally, according to Tables I-III, we can find that
the crossover probability CR =0.9 , the scale factor
D-DE can almost find the optimum solution for each test
F =0.6, the tolerant valueε=0.0001. The number of
problemsg02,g04,g06,g08,g11andg12 in one single
function evaluations (FES) is equal to N×T =275,000.
run, and that D-DE is robust and can outperform DE and
The achieved solution at the end ofN×T FES is used to
GA on a set of test problems.
measure the performance of the conventional DE. The
DE employs the repair rule and constraint handling
TABLE I.
mechanism described in Section III and is independently THE BEST RESULTS OBTAINED BY D-DE WITH RESPECT TO THOSE
run 30 times on each test function. OBTAINED BY DE, GA ON 6 BENCHMARK PROBLEMS.
Function Optimal D-DE DE GA
C. Parameter Settings of Conventional GA
g02 -0.803619 -0.80356676 -0.80361902 -0.80254846
As a computing technique and method, population-based g04 -30665.539 -30665.53867 -30665.53867 -30664.86146
genetic algorithm (GA) [20] has been shown to be an g06 -6961.814 -6961.81388 -6961.81388 -6958.24296
effective evolutionary algorithm [21]. In our experimental g08 -0.095825 -0.09582504 -0.09582504 -0.09582504
g11 0.7499 0.749900000 0.749900000 0.749900232
study, the conventional GA uses simulated binary
g12 -1 -1 -1 -1.00000000
crossover (SBX) [22], polynomial mutation operator [23],
tournament selection between the parent and its child, the
repair rule and constraint handling mechanism described
© 2010 ACADEMY PUBLISHER48 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
TABLE II. is called the median optimized function value.
THE MEAN RESULTS OBTAINED BY D-DE WITH RESPECT TO THOSE
According to the summary of statistical results of test
OBTAINED BY DE, GA ON 6 BENCHMARK PROBLEMS.
problemsg04,g06,g08,g11,g12given in Table IV, it
Function Optimal D-DE DE GA
is clearly seen that D-DE, A-DDE [19], COPSO [6] and
g02 -0.803619 -0.80150890 -0.79516035 -0.78941723
g04 -30665.539 -30665.53867 -30665.53867 -30661.64864 SRES [18] all can find the optimum or near-optimum,
g06 -6961.814 -6961.81388 -6961.81388 -6925.57354 when D-DE uses 275,000 FES, A-DDE 180,000 FES,
g08 -0.095825 -0.09582504 -0.09582504 -0.09582504 COPSO 350,000 FES and SRES 500,000 FES,
g11 0.7499 0.749900000 0.897159582 0.750805569
respectively. For test problemg02, the mean, worst and
g12 -1 -1 -1 -1.00000000
standard derivation of values obtained by D-DE are the
TABLE III. best when compared with other algorithms, while the
THE WORST RESULTS OBTAINED BY D-DE WITH RESPECT TO THOSE median value obtained by D-DE is better than that of A-
OBTAINED BY DE, GA ON 6 BENCHMARK PROBLEMS.
DDE and SRES, and is slightly worse than that of
Function Optimal D-DE DE GA COPSO. Besides, for test problemg02, the best value
g02 -0.803619 -0.79253455 -0.77041284 -0.75131833
obtained by D-DE is slightly worse than that of the other
g04 -30665.539 -30665.53867 -30665.53867 -30655.21039
g06 -6961.814 -6961.81388 -6961.81388 -6887.72373 algorithms. As shown in Table V, the best, median, mean,
g08 -0.095825 -0.09582504 -0.09582504 -0.09582504 worst and standard derivation of values obtained by D-
g11 0.7499 0.749900000 1.000000000 0.755040572 DE when set to 550,000 FES are obviously better than
g12 -1 -1 -1 -1.00000000
those when set to 275,000 FES. The best, median values
B. Comparison with Respect to Some State-of-the-art obtained by D-DE when set to 550,000 FES are almost
Approaches on 6 Benchmark Test Problems convergent to the optimum or near-optimum. Therefore,
for test problemg02, D-DE is not still convergent to the
In this section, we present the experimental results in
detail and compare D-DE with respect to state-of-the-art optimum when set to 275,000 FES. In conclusion, the
algorithms. The experimental results are given in Table performance of D-DE is stable and better than or not
IV. The optimized objective function values (of 30 runs) worse than some state-of-the-art evolutionary algorithms
arranged in ascending order and the 15th value in the list on a set of test problems.
TABLE IV.
COMPARISON D-DE WITH RESPECT TO ALGORITHMS A-DDE [19], COPSO [6], SRES [18] ON 6 BENCHMARK TEST FUNCTIONS
Function Optimal Method Best Median Mean Worst Std FES
D-DE -0.80356676178 -0.803457738386 -0.801508902296 -0.79253454688 4.00E-03 275,000
A-DDE -0.803605 -0.777368 -0.771090 -0.609853 3.66E-02 180,000
g02 -0.803619
COPSO -0.803619 -0.803617 -0.801320 -0.786566 4.59E-03 350,000
SRES -0.804 -0.793 -0.788 -0.746 1.3E-02 500,000
D-DE -30665.53867178 -30665.53867178 -30665.53867178 -30665.53867178 1.16E-011 275,000
A-DDE -30665.539 -30665.539 -30665.539 -30665.539 3.20E-13 180,000
g04 -30665.539
COPSO -30665.538672 -30665.538672 -30665.538672 -30665.538672 0 350,000
SRES -30665.539 -30665.539 -30665.539 -30665.539 0.0E+00 500,000
D-DE -6961.81387558 -6961.81387558 -6961.81387558 -6961.81387558 4.63E-012 275,000
A-DDE -6961.814 -6961.814 -6961.814 -6961.814 2.11E-12 180,000
g06 -6961.814
COPSO -6961.813876 -6961.813876 -6961.813876 -6961.813876 0 350,000
SRES -6961.814 -6961.814 -6961.814 -6961.814 1.9E-12 500,000
D-DE -0.095825041418 -0.095825041418 -0.095825041418 -0.095825041418 2.82E-017 275,000
A-DDE -0.095825 -0.095825 -0.095825 -0.095825 9.10E-10 180,000
g08 -0.095825
COPSO -0.095825 -0.095825 -0.095825 -0.095825 0 350,000
SRES -0.096 -0.096 -0.096 -0.096 0.0E+00 500,000
D-DE 0.749900000000 0.749900000000 0.749900000000 0.749900000000 1.13E-016 275,000
A-DDE 0.75 0.75 0.75 0.75 5.35E-15 180,000
g11 0.7499
COPSO 0.749999 0.749999 0.749999 0.749999 0 350,000
SRES 0.750 0.750 0.750 0.750 1.1E-16 500,000
D-DE -1 -1 -1 -1 0 275,000
A-DDE -1.000 -1.000 -1.000 -1.000 4.10E-11 180,000
g12 -1
COPSO -1.000000 -1.000000 -1.000000 -1.000000 0 350,000
SRES -1.000 -1.000 -1.000 -1.000 0.0E+00 500,000
TABLE V.
EXPERIMENTAL RESULTS OBTAINED BY D-DE WHEN FES=275,000 , FES=550,000 FOR TEST FUNCTIONg02OVER 30 RUNS
FES Best Median Mean Worst Std
275,000 -0.80356676178 -0.803457738386 -0.801508902296 -0.79253454688 4.00E-03
550,000 -0.803610090279 -0.8036058890 -0.802935868229 -0.79259834414 2.55E-03
C. Convergence Graphs Obtained by D-DE for 6
Benchmark Test Problems
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 49
In order to provide a more intuitive comprehension, we find the optimum solution for test problemg02within
present the convergence graphs obtained by D-DE for 300,000 FES, that D-DE can find the optimum solution
test problems g02 , g04 , g06 , g08 , g11 and g12 . for each test problemg06,g08,g11,g12within 50,000
Figures 4-9 depict the convergence graphs for test FES, and that D-DE can obtain the optimum solution for
problems g02 , g04 , g06 , g08 , g11 and g12 , test problemg04within 100,000 FES.
respectively. It is clearly seen that D-DE has a trend to
Figure 4. Convergence graph for g02. Figure 7. Convergence graph for g08.
Figure 5. Convergence graph for g04.
Figure 8. Convergence graph for g11.
Figure 6. Convergence graph for g06.
Figure 9. Convergence graph for g12.
© 2010 ACADEMY PUBLISHER50 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
VII. CONCLUSIONS AND FUTURE WORK optimization,” Technical Report, Nanyang Technological
University, Singapore, 2006.
In this study, we present a dynamic differential evolution
[5] R. Landa-Becerra, C. A. Coello Coello, "Cultured
algorithm (D-DE) for solving constrained real-parameter differential evolution for constrained optimization,"
optimization problems. In this model of D-DE, there Computer Methods in Applied Mechanic sand
exist at least three important contributions as follows: Engineering, Vol. 195, No. 33-36, pp. 4303-4322, 2006.
1) The first contribution is the novel mutation scheme, [6] A. E. Muñoz Zavala, A. Hernández Aguirre, E. R. Villa
which can improve the convergence speed, prevent Diharce, and S. Botello Rionda, “Constrained optimization
with an improved particle swarm optimization algorithm,”
premature and preserve the diversity of solutions.
International Journal of Intelligent Computing and
2) The second contribution is two important control
Cybernetics, Vol. 1, No. 3, pp. 425-453, 2008.
parameters (i.e., the scale factor F and the crossover
[7] J. J. Liang, P. N. Suganthan, “Dynamic multi-swarm
probabilityCR ), which are dynamic and beneficial for particle swarm optimizer with a novel constraint-handling
adjusting control parameters during the evolutionary and mechanism,” in 2006 IEEE Congress on Evolutionary
search process, especially, when done without any user Computation (CEC'2006), pp. 316-323, IEEE, Vancouver,
interaction. BC, Canada, July 2006.
3) The third contribution is that D-DE can prevent [8] E. Mezura-Montes, J. Velázquez-Reyes, and C. A. Coello
Coello, “Modified differential evolution for constrained
premature and enhance the search performance mainly
optimization,” in 2006 IEEE Congress on Evolutionary
due to replacing some relatively worse solutions with
Computation (CEC'2006), pp. 332-339, IEEE, Vancouver,
reinitialized solutions during the evolutionary process.
BC, Canada, July 2006.
In addition, D-DE employs orthogonal design method
[9] T. Takahama, and S. Sakai, “Constrained optimization by
to generate initial population to improve the diversity of the ε constrained differential evolution with gradient-
solutions and introduces a constraint handling technique based mutation and feasible elites,” in 2006 IEEE
based on the feasibility rule and the sum of constraints Congress on Evolutionary Computation (CEC'2006), pp.
violation. 308-315, IEEE, Vancouver, BC, Canada, July 2006.
Finally, D-DE is tested on 6 benchmark test functions [10] R. Storn, K. Price, "Differential evolution - a simple and
efficient heuristic for global optimization over continuous
provided by the CEC 2006 special session on constrained
spaces," Journal of Global Optimization, Vol. 11, pp.
real-parameter optimization. Through comparing D-DE
341–359, 1997.
with respect to state-of-the-art evolutionary algorithms,
[11] K. Price, R. Storn, J. Lampinen, Differential Evolution: A
the experimental results show that D-DE is highly
Practical Approach To Global Optimization, Berlin:
competitive and can obtain good results in terms of a test Springer-Verlag, 2005.
set of constrained real-parameter optimization problems. [12] Z. Y. Yang, K. Tang, X. Yao, "Self-adaptive differential
However, in the future, there are still many aspects to do. evolution with neighborhood search," 2008 Congress on
Firstly, in order to further validate D-DE, we are Evolutionary Computation (CEC'2008), pp. 1110-1116,
considering of the possibility of testing more benchmark 2008.
[13] H. A. Abbass, R. Sarker, C. Newton, "PDE: a Pareto-
test functions (especially, highly dimensional problems)
frontier differential evolution approach for multiobjective
and real-world constrained optimization problems.
optimization problems," in Proceedings of IEEE Congress
Secondly, for some test functions, there exists the
on Evolutionary Computation, Vol. 2, pp. 971-978, 2001.
phenomenon of slow evolutionary at the later stage. In
[14] Y. W. Leung, Y. P. Wang, "An orthogonal genetic
order to overcome the limitation, we will incorporate algorithm with quantization for global numerical
some local search techniques into D-DE to improve the optimization," IEEE Transactions on Evolutionary
convergence speed. Additionally, improving constraint Computation, Vol. 5, No. 1, pp. 40-53, 2001.
handling technique is another future work. [15] J. Brest, V. Zumer, and M. S. Maucec, “Self-adaptative
differential evolution algorithm in constrained real-
parameter optimization,” in 2006 IEEE Congress on
REFERENCES
Evolutionary Computation (CEC'2006), pp. 919-926,
[1] V. L. Huang, A. K. Qin, and P. N. Suganthan, “Self- IEEE, Vancouver, BC, Canada, July 2006.
adaptative differential evolution algorithm for constrained [16] Y. Wang, Z.X Cai, "A Hybrid Multi-Swarm Particle
real-parameter optimization,” in 2006 IEEE Congress on Swarm Optimization to Solve Constrained Optimization
Evolutionary Computation (CEC'2006), pp. 324-331, Problems," Frontiers of Computer Science in China, Vol.
IEEE, Vancouver, BC, Canada, July 2006. 3, No. 1, pp. 38-52, 2009.
[2] A. E. Muñoz-Zavala, A. Hernández-Aguirre, E. R. Villa- [17] C. A. Coello Coello, "Theoretical and Numerical
Diharce, and S. Botello-Riond, “PESO+ for Constrained Constraint-Handling Techniques used with Evolutionary
Optimization,” in 2006 IEEE Congress on Evolutionary Algorithms: A Survey of the State of the Art," Computer
Computation (CEC'2006), pp. 935-942, IEEE, Vancouver, Methods in Applied Mechanics and Engineering, Vol. 191,
BC, Canada, July 2006. No. 11-12, pp. 1245-1287, 2002.
[3] K. Deb, "An efficient constraint handling method for [18] T. P. Runarsson, “Approximate evolution strategy using
genetic algorithms," Computer Methods in Applied stochastic ranking,” in 2006 IEEE Congress on
Mechanics and Engineering, Vol. 186, No. 2, pp. 311-338, Evolutionary Computation (CEC'2006), pp. 2760-2767,
2000. IEEE, Vancouver, BC, Canada, July 2006.
[4] J. J. Liang, T. P. Runarsson, E. Mezura-Montes, M. Clerc, [19] E. Mezura-Montes, A. G. Palomeque-Qrtiz, “Parameter
P. N. Suganthan, C. A. Coello Coello, and K. Deb, control in differential evolution for constrained
“Problem definitions and evaluation criteria for the CEC optimization,” in 2009 IEEE Congress on Evolutionary
2006 special session on constrained real-parameter Computation (CEC'2009), pp. 1375-1382, 2009.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 51
[20] Z. Michalewicz, Genetic Algorithms + Data Structures = adaptive search,” Transactions of the ASME: Journal of
Evolution Programs, Springer-Verlag, third edition, 1996. Mechanical Design, Vol. 120, No. 2, pp. 162-164, 1998.
[21] E. Mezura-Montes, C. A. Coello Coello, “A simple
multimembered evolution strategy to solve constrained Youyun Ao was born in 1973. He received his B.S. degree
optimization problems,” IEEE Transactions on in Computer Science from Jiangxi Normal University,
Evolutionary Computation, Vol. 9, No. 1, pp. 1-17, 2005. Nanchang, China in 1999. He received his M.S. degree in
[22] K. Deb and R. B. Agrawal, “Simulated binary crossover Computer Software and Theory from Shanghai Normal
for continuous search space,” Complex Systems, Vol. 9, University, Shanghai, China in 2006. He is currently a lecturer
No. 2, pp. 115-148, 1995. in Computer Science at Anqing Teachers College, Anqing,
[23] K. Deb and M. Goyal, “A robust optimization procedure Anhui, China. His research interests include evolutionary
for mechanical component design based on genetic computation, intelligent information processing, etc.
© 2010 ACADEMY PUBLISHER52 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
Fuzzy Logic Based Position-Sensorless
Speed Control of Multi Level Inverter Fed
PMBLDC Drive
Narmadha T.V.
Research Scholar, Anna University, Chennai, India
nar_velu@yahoo.co.in
Thyagarajan T.
Prof. & Head, Dept. of Instrumentation Engg,
Anna University, Chennai, India
thyagu_vel@yahoo.co.in
Abstract—This paper presents multi level inverter fed controlling PMBLDC motor namely sensor control and
Permanent Magnet Brushless DC Motor (PMBLDCM) with sensorless control. The latter has advantages like cost
a simplified voltage control technique. It is based on the reduction, reliability, elimination of difficulty in
“indirect position sensing,” which was justified by the
maintaining the sensor etc. Sensorless control is highly
observation that position sensing came indirectly from
advantageous when the motor is operated in dusty or oily
voltage and current waveforms. The switching angle for the
environment, where cleaning and maintaining of Hall
pulse is selected in such way to reduce the harmonic
Sensors is required for proper sensing of rotor position.
distortion. This drive system has advantages like reduced
total harmonic distortion and higher torques. PI, Fuzzy and Sensorless method is preferred when the motor is in less
Hybrid ( Fuzzy and PI) controllers are discussed. Closed accessible location. Accommodation of position sensor
loop simulation response is obtained for PI, Fuzzy and in motor used in compact unit such as computer hard disk
Hybrid controller with a disturbance in the input source. may not be possible. Novel direct back emf detection for
The conventional circuit is improved by introducing Hybrid sensorless BLDC motor is given in [7]. Analysis of
Controller. In Industrial application the physical integration
BLDC motor is given in [8]. Modeling of BLDC motor is
of Hybrid controller in the motor body itself is able to make
given in [9]. Feed forward speed control of Brushless DC
them most suitable for low power (0.5hp) blowers and low
motor with input shaping is given in [10]. A PSO-based
power (50W) tube axial fans for cooling the electronic
optimization of PID controller for a Linear BLDC Motor
equipment. The performance of the PMBLDCM system is
simulated and implemented. Simulation results of these is given in [11]. Speed Control of BLDC based on
systems are presented and the performance measures are CMAC & PID controller is given in [12]. A sensorless
compared. The simulation results with Hybrid controller drive system for BLDC using a Digital Phase-Locked
indicate improved performance. The experimental results Loop is given in [13]. Classical control methods can be
are compared with simulation results. implemented in well- defined systems to achieve good
performance of the systems. To control a system, an
Index Terms : Three level inverter, FLC, PI, Hybrid,
accurate mathematical model of the complete system is
trapezoidal back emf, PWM, , Sim Power systems.
required. Systems with non linear behavior cannot be
I. INTRODUCTION
exactly modeled. The fuzzy Logic control has adaptive
characteristics that can achieve robust response to a
system with uncertainty, parameter variations and load
With the rapid development of microelectronics and
disturbance. Fuzzy Logic and Fuzzy set theory was
power switches, most adjustable-speed drives are now
presented by Zadeh [14]. Fuzzy Logic Controllers have
realized with ac machines. Permanent Magnet
been broadly used for ill-defined, non-linear and complex
Synchronous Motor (PMSM) with sinusoidal shape back-
systems [15], [16]. In the area of electrical drives, fuzzy
EMF and brushless DC (BLDC) motor with trapezoidal
logic controllers have been applied to switched reluctance
shape back-EMF drives have been extensively used in
motors [17], [18], induction motors [19] and PMBLDC
many applications, ranging from servo to traction drives
motors [20] successfully. The above literature does not
due to several distinct advantages such as high power
deal with voltage control method to control the speed
density, high efficiency, large torque to inertia ratio, and
using sensorless approach. This paper demonstrates a
better controllability. Brushless DC motor (BLDC) fed by
sensorless technique to drive a three phase brushless DC
two-phase conduction scheme has higher power/weight,
motor with a multi level voltage Inverter system using
torque/current ratios and it is less expensive due to the
voltage control method with Hybrid Fuzzy logic control.
concentrated windings which shorten the end windings
PMBLDC motors drives are used in a wide range of
compared to three-phase permanent magnet synchronous
commercial and residential applications such as domestic
motor (PMSM) [1]- [6]. There are two methods of
appliances, heating, ventilating and air-conditioning
Manuscript received June 24, 2009; revised August 31, 2009.
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.52-58JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 53
equipment due to their highest possible efficiencies. The motor with high current and low voltage conditions. The
speed control ability using Hybrid fuzzy controller is able three level inverter circuit is shown in Fig 1.
to provide operation at their high efficiency.
In Section I Voltage control based PMBLDC motor is
described. In Section II three level inverter is presented
with some basics of mode of conduction. In Section III
three level inverter fed PMBLDC motor with PI
controller is presented. In Section IV three level inverter
fed PMBLDC motor with Fuzzy Logic controller is
presented. Hardware circuit is fabricated and tested.
A. VOLTAGE CONTROL BASED PMBLDC MOTOR
The measurement of armature current in the three
phases is not required because there is no neutral
connection, and hence the third phase current can be
obtained from the other two. The quasi square-wave
armatures current are mainly characterized through their Fig 1. Three level Inverter Circuit
maximum amplitude value, which directly controls the
machine torque. The inverter performance is very much
reliable because there is natural dead time for each A. THREE LEVEL INVERTER FED PMBLDC MOTOR
transistor. Hence, allows designing a circuit for
controlling only a DC component, which represents the Fig. 2 shows the schematic diagram of the Closed loop
maximum amplitude value of the trapezoidal currents, Sensorless Speed Control of PMBLDC motor using PI
I .and reduces the complex circuitry required by other Controller. The MOSFETs are used as switching devices.
MAX
machines, allowing the self-synchronization process for For speed control of motor, the output frequency of the
the operation of the machine. The most popular way to inverter is varied. The applied voltage to the motor is
control BLDCM for traction applications is through varied in linear proportion to the supply frequency to
voltage-source current-controlled inverters. The inverter maintain the flux constant. The MATLAB simulation is
must supply a quasi-square current waveform whose carried out and the simulation results are presented in this
magnitude, I , is proportional to the machine shaft section. The Fig.3 shows the driving pulses applied to the
MAX
torque. Then, by controlling the phase-currents, torque MOSFETs.
and speed can be adjusted. The response of phase voltage
, phase currents, speed, and feedback current with
disturbance at the source are obtained.
The waveforms of the armature currents are quasi-
square. These currents are sensed through current
sensors. These signals are then rectified , and a DC
component, with the value of the ceiling of the currents,
I , is obtained. It is filtered and DC voltage is obtained
max
across resistor. The voltage Vmax is compared with Vref
and from this comparison, an error signal “e(t)’ is
obtained. This error signal is then processed using PI
controller. The output of PI controller is used to vary the
input voltage of three level inverter. The strategy
becomes simple, because the control only needs to be in
command of DC current instead of three alternating
waveforms. The control strategy also allows regenerative
braking, which is very important in applications, like
electric vehicles, where energy can be returned to the
battery pack.
Fig 2.Closed loop Sensorless Speed Control of PMBLDC
II. THREE LEVEL INVERTER motor
In three level inverter modeling,120 degree
conduction mode is employed. The gating signals given to
the MOSFET are sequenced to every 60 degree interval.
Each MOSFET conducts for a duration of 120 degrees.
The MOSFET is used as a switch since it can operate high
switching frequency. This feature is helpful in driving the
© 2010 ACADEMY PUBLISHER54 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
negative medium, NS negative small, ZR zero, PS
positive small, PM positive medium and PB positive Big.
For example, it follows from Table I that the first rule is:
IF e is NB and de is NB then du is NB
The linguistic rules are in the form of IF-THEN rules
and take form:
IF (e is X and de is Y) then (du is Z),
Where X, Y, Z are fuzzy subsets for the universe of
discourse of the error, change of error and change of the
output .For example, X can denote the subset
NEGATIVE BIG of the error etc. On every of these
universes is placed seven triangular membership
functions. It was chosen to set these universes to
Fig3 a,b,c.Driving Pulses to the MOSFETs normalized type for all of inputs and output. The range of
universe is set to -1 to 1.
B.THREE LEVEL INVERTER FED PMBLDC MOTOR
WITH FUZZY LOGIC CONTROLLER
Fuzzy Logic Controller
The block diagram showing the implementation of the
Fuzzy speed controller is illustrated in Figure 4. It
includes four major blocks: knowledge base,
fuzzification, inference mechanism, and defuzzification.
The knowledge base is composed of a data and a rule
base. The data base, consisting of input and output
membership functions. The rule base is made of a set of
linguistic rules relating the fuzzy input variables into the
desired fuzzy control actions.
Fig 5. Membership function for error, derror, controller
output
D. THREE LEVEL INVERTER FED PMBLDC MOTOR
WITH HYBRID CONTROLLER
The fuzzy PI controller is shown in Fig 6. The fuzzy
controller is basically an input/output static nonlinear
mapping, hence the controller action is in the form
Fig 4 . Block Diagram of Fuzzy Logic
K E+K CE=DU (1)
1 2
Controller
Where K and K are nonlinear coefficients or gain
1 2
Initial rule base that can be used in drive systems for a
factors.
fuzzy logic controller consist of 49 linguistic rules, as
shown in Table I, and gives the change of the output of ∫DU=∫K 1Edt+∫K 2CEdt (2)
fuzzy logic controller in terms of two inputs: the error (e)
and change of error (de). The membership functions of U=K ∫Edt+K E (3)
1 2
these variables are given in Fig.5.In Table I, the
Equation (3) is a fuzzy P-I controller with nonlinear gain
following fuzzy sets are used: NB negative Big , NM
factors.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 55
TABLE I : FAM TABLE FOR HYBRID
CONTROLLER The simulation results of stator current are shown in
Fig 7. The currents are quasi square wave with a
NB NM NS ZE PS PM PB displacement of 120°.
NB NB NB NB NM NS NS ZE
NM NB NM NM NM NS ZE PS
NS NB NM NS NS ZE PS PM
ZE NB NM NS ZE PS PM PB
PS NM NS ZE PS PS PM PB
PM NS ZE PS PM PM PM PB
PB ZE PS PS PM PB PB PB
Fig 8.Three level Inverter Output Voltage
The stator voltages are shown in Fig 8. They are also
displaced by 120° .
Fig. 6 Closed loop Sensorless Speed Control of
PMBLDC motor using Hybrid Controller
III. SIMULATION RESULTS
A. RESPONSE OF HYBRID CONTROLLER
With the help of designed circuit parameters the
MATLAB simulation of the above circuit is performed
and the results are presented here.
Fig 9. Trapezoidal shape Phase Back EMF
The stator phase back emf is shown in Fig 9. The phasor
back emf is trapezoidal as shown.
Fig.10 DC Input Voltage to Inverter with
Disturbance at t=4 sec
Fig 7. Three Phase Inverter Stator Current
© 2010 ACADEMY PUBLISHER56 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
Fig 10 shows the DC input voltage to the 3-level IV. EXPERIMENTAL RESULTS
inverter with a disturbance at t=4 Sec. The closed loop
system brings the voltage to the normal value by After the simulation studies, a Hybrid Fuzzy logic
adjusting the input voltage of the inverter. based Three level inverter fed PMBLDC motor is fabri-
cated and tested. The top view of the hardware is
depicted in Fig 13. The hardware consists of power
circuit, control circuit and PMBLDC motor.
Fig. 11 Rotor Speed in rpm
The rotor speed characteristic of 3-level inverter fed
PMBLDC motor using Hybrid controller is shown in Fig
11. The rotor reaches the rated speed in 5.5 Sec.
Fig 13 Top View of Hardware circuit
The regulators 7812 and 7805 in the control circuit
give the DC supply required by the driver and microcon-
troller chips respectively. The driver chip amplifies 5V
pulse to 10V level. DC output from the rectifier is ripple
free due to the filter. The Atmel microcontroller
89C2051 is used to generate the pulses. Port 1 of the
microcontroller is used for generating the gate pulses.
Timer 0 is used for producing the delay required for the
duration T and T . The microcontroller operates at a
ON OFF
clock frequency of 12 MHz.
Fig 12. Rotor Speed in rpm
The rotor speed characteristic of 3-level inverter fed
PMBLDC motor with PI controller is shown in Fig 12.
The rotor reaches rated speed in 6.1 Sec. The time taken
to settle at rated speed is comparatively more in
conventional controller.
TABLE II PERFORMANCE ANALYSIS
Fig 14 Pulse Signal waveform
Controller THD IAE ISE tss
PI Controller 0.5747 42.79 334 6.1 The pulses produced by the microcontroller are
Fuzzy 0.4173 28.39 107 6.00 amplified using the driver IC IR 2110. Three driver ICs
Controller are used to amplify the gate pulses. The oscillogram of
Hybrid 0.3825 10.49 29.35 5.5 pulse signal is given in Fig 14.
Controller
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 57
and tested. The hardware system used in the present work
has obvious advantage of using single phase supply. This
drive can be used for variable speed applications like
Electrical vehicles, Robotics etc., The experimental
results coincide with the simulation results.
This Paper also presents a comparative study of fuzzy
controllers with conventional controller of sensorless
speed control of Permanent magnet Brushless DC Motor.
The simulation results show that the Hybrid controller is
the best performance in all aspects. It can be noticed that
the hybrid controller exhibits fast rise time, no overshoot,
and lesser settling time with lesser THD, IAE and ISE.
Fig 15 Three level inverter voltage
REFERENCE
[1] B.K. Bose, Power Electronics and AC Drives, Prentice
Hall, Englewood Cliffs, NJ: 1986.
The oscillogram of three phase inverter output voltage
[2] L. Hao, H. A. Toliyat, “BLDC motor full-speed operation
is depicted in Fig 15. The Back EMF waveform of Three
using hybrid sliding mode observer,” in Proc. IEEE-APEC
Phase PMBLDC is depicted in Fig 16. The Back emf
Annu. Meeting, Miami, FL, Feb. 9-13, 2003, vol. 1, pp. 286-
waveforms are trapezoidal as shown. 293.
[3].P.Pillay and R. Krishnan, ‘‘Application characteristics of
permanent magnet synchronous and brushless dc motors for
servo drives,’’ IEEE Trans.on Ind. Appl., vol. 27, no. 5, pp.
986--996, Sep./Oct. 1991.
[4].T.J.E. Miller, “ Brushless Permanent-Magnet and
Reluctance Motor Drives,” Oxford, 1989.
[5].R Krishnan, “Electric motor drives- modeling, analysis and
control ”, Prentice Hall of India Private Limited, 2002.
[6].K.Uzuka, H.Uzuhashi, et al., “Microcomputer Control for
Sensorless Brushless Motor ,” IEEE Trans. on Industry
Application ,Vol.IA-21, May-June, 1985
[7].J.Shao, D.Nolan, and T.Hopkins, “A Novel Direct Back
EMF Detection for Sensorless Brushless DC(BLDC)
Motor Drives,” Applied Power Electronic Conference
(APEC 2002), pp33-38.
[8] N.Mastui, “ Sensorless PM Brushless DC Motor Drives,”
IEEE Trans. on Industrial Electronics, Vol. 43, April
1996.
[9].R Krishnan, “Electric motor drives- modeling, analysis
Fig16. Back EMF waveform and control ”, Prentice Hall of India Private Limited,
2002.
[10].Rene Zwahlen,Timothy Chang, “ Feed forward speed
control of Brushless DC motor with input shaping,” The
PMBLDC motors drives are used in a wide range of
33rd Annual Conference of the IEEE Industrial
commercial and residential applications due to their
Electronics Society (IECON), Nov 5-8,2007.
highest possible efficiencies. The speed control ability of
[11].Mehdi Nasri, Hossein Nezamabadi-Pour,
compressors and blowers is able to provide operation at Malihemaghfoori, “A PSO-Based optimization of PID
their high efficiency. The physical integration of controller for a Linear BLDC Motor” Proc. Of World
controller in the motor body itself is able to make them academy of Science Engg & Tech,Vol.20,April 2007.
most suitable for low power (0.5hp) blowers and low [12].Zhiqiang Li & Changliangxia, “ Speed Control of BLDC
power (50W) tube axial fans for cooling the electronic based on CMAC & PID controller” Proc. Of 6th World
congress on Intelligent Control & Automation. China,
equipment.
June 21-23,2006.
[13].Yoko Amano, Toshio Tsuji, Atsushi Takahashi,
V. CONCLUSION Shigaoonchi, “A Sensorless Drive system for BLDC
using a Digital phase-Locked Loop,” Wiley periodicals
The closed loop controlled sensorless PMBLDC drive Inc. vol.142 No.1pp1155-1162, 2003.
[14].L.A.Zadeh, “ Fuzzy sets,” in Information and Control.
is modeled and simulated using the blocks of simulink.
New York: Academic, 1965, vol.8,pp. 338-353.
The simulation results of closed loop system are
[15]."Outline of a new approach to the analysis of complex
presented. The closed loop system is able to maintain
systems and decision processes,” IEEE Trans.
constant speed by maintaining constant voltage. The
Syst.,Man,Cybern.,vol.3, Jan.1973.
simulation results agree with the analytical predictions.
The PMBLDCM drive system is successfully fabricated
© 2010 ACADEMY PUBLISHER58 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
[16].C.Lee,” Fuzzy logic in control systems, fuzzy logic
controller, PartsI and II,” IEEE Trans. Syst.,Man,Cybern.,
vol.20,pp.404-435,1990.
[17].C.Elmas and O.F.Bay, “ Modeling and operation of a
nonlinear switched reluctance motor drive based on fuzzy
logic,” in Proc. Eur. Power Electronics Applications
Conf., Sevilla, Spain, Sep.18-21, 1995, pp.3.592-3.597.
[18].O.F.Bay, C.Elmas, and M.Alci,” Fuzzy Logic based
control of a switched reluctance drive,” in Int. Aegean
Conf. Electrical Machines Power Electronics, Kusadasi,
Turkey, June 5-7, 1995, pp.333-337.
[19].O.F.Bay, "Fuzzy control of a field orientation controlled
induction motor,” J.Polytechnic, vol.2, no.2, pp.1-9, 1999.
[20].C.Elmas and M.A.Akcayol, "Fuzzy logic controller
based speed control of brushless DC motor,”
J.Polytechnic, vol.3, no.3, pp.7-14, 2000.
T.V.Narmadha has obtained her AMIE (EEE) from the
Institution of Engineers (India) and M.E., degree from College
of Engineering, Anna University in the years 1996 and 2000
respectively. She has 11 years of teaching experience. She is
presently a research scholar in Anna University. Her area of
interest is Permanent Magnet Motor Drives.
T.Thyagarajan is the Professor and Head of the
Department of Instrumentation Engineering, Anna University,
Chennai, India. He obtained B.Tech degree from Government
Engineering College, Anantapur; M.E degree from M.S.
University, Baroda and Ph.D from Anna University. He also
pursed Post-Doctoral Research at National Taiwan University,
Taipei. He has received DTE Award for guiding best U.G.
project and SISIR Kumar award for publishing best research
paper. He made technical visits to many countries. He is a
member of IEEE, IE(I), ISTE, SSI and ISO(I). His teaching and
research interests include electrical drives, auto tuning, process
modeling and control using fuzzy logic, neural networks and
genetic algorithm, sensor networking and soft sensors.
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 59
On Performance of Multicast Delivery with Fixed
WiMAX Telemedicine Networks Using Single-
Carrier Modulation
Bernard Fong
Prognostics and Health Management Center, City University of Hong Kong
Email: bfong@ieee.org
Guan Yue Hong
Email: gyh138@gmail.com
Abstract—IEEE 802.16e fixed WiMAX provides a low cost significantly impact their performance. In wireless
solution for integrated multimedia access networks with a communication systems, a base station modem unit
wide bandwidth making it particularly suitable for (BMU) is connected to each base station controller
telemedicine applications. While a wide variety of
(BSC). Its purpose is to convert the digital data to be
modulation schemes are suitable for fixed wireless systems,
carried across the radio interface into a format
single carrier modulations such as QAM offer advantages
appropriate for transmission over wireless channels.
such as reduction in power requirements and system
Different equipment manufacturers employ various
complexity compared to multi-carrier transmission systems.
In this paper, we analyze the performance of QAM schemes modulation schemes for their BMU implementations as
used in a fixed WiMAX system that supports multicast they have different standards. While the primary concern
distribution of .real-time traffic for healthcare services by is making a decision on compromise between cell
evaluation of system performance on a 10 GHz carrier. coverage and data throughput, there is no straightforward
Results are presented by comparing the distribution of video answer as to which modulation scheme is best for a
data using QPSK and 16-QAM and bandwidth utilization is
wireless network. The optimal tradeoff depends mainly
calculated for continuous data transmission in remote
on specific application, for example, systems for life-
patient monitoring.
saving missions would have far more stringent
Index Terms— broadband access, modulation, QAM, requirements than those for general health assessment. In
telemedicine, WiMAX this paper, we investigate the suitability of using a
wireless point-to-multipoint system for distribution of
multimedia traffic based on a QAM scheme that is
I. INTRODUCTION optimized for providing telemedicine services within a
local environment [1].
IEEE 802.16e WiMAX networks are widely used for
QAM is a very robust type of modulation scheme that
providing point-to-multipoint network access for fixed
provides a high capacity. However, QAM systems are
locations within a radius of several kilometers due to its
subject to fast Rayleigh fading and time delay spread.
design for low cost two-way transmission. Its main
Compensation for distortion over Rayleigh fading
advantages include ease of expansion and allowing
channels has been studied [2]. In [3], a discussion about
frequency reuse. Fixed WiMAX provides a means of
the effects of modulated signals transmitted over a fading
wireless data distribution at high data rates over a
channel is presented which also includes an outline of a
reasonably large area compared to alternative solutions
numerical evaluation on such effect [4]. While QAM
such as wireless local loop (WLL) and wireless cable
offers a comparably high number of bits per hertz [5],
(multichannel multipoint distribution system),
various QAM modulation schemes have been used by
transmission of various types of medical data at
different equipment manufacturers according to some
frequencies in excess of 10 GHz requires high resolution
tradeoff between bits per Hertz and bits per unit area
planning since data packets are vulnerable to burst errors.
coverage. High modulation schemes such as a 128-
Wireless communication systems are affected by
QAM offers more bits per hertz at the expense of cell size
channel-induced phenomena such as fading that can
reduction. Numerous research results [6], [7], [8] have
been reported with a number of receivers studied
Manuscript received October 1, 2009; revised December 23, 2009;
accepted January 21, 2010. illustrating various receiver structures. The receiver
Corresponding author: Bernard Fong, Prognostics and Health structures required for high modulation QAM schemes
Management Center, City University of Hong Kong, 83 Tat Chee Road,
Kowloon, Hong Kong, Tel: +852 3442 5936 Email: bfong@ieee.org
© 2010 ACADEMY PUBLISHER
doi:10.4304/jait.1.1.59-6560 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
(cid:708)(cid:707)- (cid:711)(cid:709)
(cid:730)(cid:731)z
(cid:708)(cid:707)- (cid:711)(cid:707)
(cid:730)(cid:731)z
(cid:735)(cid:738)(cid:742)
< (cid:713)(cid:766)(cid:768)
(cid:732)(cid:739)(cid:706)
(cid:739)-(cid:775)(cid:770)-(cid:739) (cid:739)-(cid:775)(cid:770)-(cid:739) (cid:728)(cid:769)(cid:759) (cid:744)(cid:774)(cid:760)(cid:773) (cid:737)(cid:770)(cid:759)(cid:760)
(cid:728)(cid:775)(cid:763)(cid:760)(cid:773)(cid:769)(cid:760)(cid:775)
(cid:741)(cid:742)(cid:744) (cid:741)(cid:742)(cid:744)
(cid:770)(cid:773) (cid:726)(cid:776)(cid:774)(cid:775)(cid:770)(cid:768)(cid:760)(cid:773) (cid:725)(cid:776)(cid:764)(cid:767)(cid:759)(cid:764)(cid:769)(cid:762)
(cid:739)(cid:742)(cid:743)(cid:737)
(cid:738)(cid:771)(cid:775)(cid:764)(cid:758)(cid:756)(cid:767) (cid:741)(cid:770)(cid:776)(cid:775)(cid:760)(cid:773) (cid:725)(cid:756)(cid:769)(cid:759)(cid:778)(cid:764)(cid:759)(cid:775)(cid:763) (cid:739)(cid:736)(cid:739)
(cid:736)(cid:776)(cid:767)(cid:775)(cid:764)(cid:771)(cid:767)(cid:760)(cid:779)(cid:760)(cid:773) (cid:736)(cid:756)(cid:769)(cid:756)(cid:762)(cid:760)(cid:773)
(cid:739)(cid:742)(cid:737)
< (cid:711)(cid:712) (cid:736)(cid:757)(cid:771)(cid:774)
(cid:738)(cid:771)(cid:775)(cid:764)(cid:758)(cid:756)(cid:767) (cid:738)(cid:771)(cid:775)(cid:764)(cid:758)(cid:756)(cid:767)
(cid:728)(cid:710)
(cid:736)(cid:776)(cid:767)(cid:775)(cid:764)(cid:771)(cid:767)(cid:760)(cid:779)(cid:760)(cid:773) (cid:736)(cid:776)(cid:767)(cid:775)(cid:764)(cid:771)(cid:767)(cid:760)(cid:779)(cid:760)(cid:773)
(cid:726)(cid:776)(cid:774)(cid:775)(cid:770)(cid:768)(cid:760)(cid:773) (cid:725)(cid:776)(cid:764)(cid:767)(cid:759)(cid:764)(cid:769)(cid:762)
(cid:726)(cid:760)(cid:769)(cid:775)(cid:773)(cid:756)(cid:767) (cid:738)(cid:761)(cid:761)(cid:764)(cid:758)(cid:760) (cid:741)(cid:756)(cid:759)(cid:764)(cid:770) (cid:731)(cid:776)(cid:757)
(cid:742)(cid:778)(cid:764)(cid:775)(cid:758)(cid:763)(cid:764)(cid:769)(cid:762) (cid:743)(cid:773)(cid:756)(cid:769)(cid:774)(cid:771)(cid:770)(cid:773)(cid:775) (cid:724)(cid:758)(cid:758)(cid:760)(cid:774)(cid:774)
Figure 1. A point-to-multipoint (PMP) network infrastructure
(cid:738)(cid:726)(cid:710) (cid:724)(cid:743)(cid:736) (cid:709)(cid:713) (cid:730)(cid:731)z (cid:709)(cid:713) (cid:730)(cid:731)z (cid:709)(cid:713) (cid:730)(cid:731)z (cid:736)(cid:770)(cid:757)(cid:764)(cid:767)(cid:760)
(cid:727)(cid:742)(cid:735) (cid:743)(cid:760)(cid:773)(cid:768)(cid:764)(cid:769)(cid:756)(cid:775)(cid:770)(cid:773) (cid:725)(cid:756)(cid:774)(cid:760)(cid:774)(cid:775)(cid:756)(cid:775)(cid:764)(cid:770)(cid:769)
(cid:742)(cid:778)(cid:764)(cid:775)(cid:758)(cid:763) (cid:725)(cid:741)(cid:744) (cid:742)(cid:741)(cid:744) (cid:727)(cid:760)(cid:777)(cid:764)(cid:758)(cid:760)
(cid:739)-(cid:775)(cid:770)-(cid:739)
(cid:708)(cid:712)(cid:712) (cid:736)(cid:757)(cid:771)(cid:774)
(cid:770)(cid:773)
(cid:737)(cid:760)(cid:775)(cid:778)(cid:770)(cid:773)(cid:766) (cid:724)(cid:758)(cid:758)(cid:760)(cid:774)(cid:774)
(cid:739)(cid:736)(cid:739)
(cid:711)(cid:712) (cid:736)(cid:757)(cid:771)(cid:774)
Figure 2. System block diagram
become much more complex than those required for 4 not contain redundancies such as error correction or
(QPSK) and 16-QAM schemes. Further, an adaptive synchronization characters. It is further assumed that the
equalization scheme for a 16-QAM receiver has been data traffic volume associated with such redundancies is
proposed [9]. Low modulation QAM schemes are very much less than that of the video data itself hence the
therefore very attractive in terms of costs and size of omission would not affect actual system performance.
receiving devices used by consumers. To minimize the In this introductory section, we have
effects of cell-to-cell interference, lower modulation summarized the advantages of using single-carrier
schemes are studied. modulation techniques such as QAM and stated the
The use of Orthogonal Frequency Division differences between using a high (e.g. M= 128) and a low
Multiplexing (OFDM) techniques for multiplexed QAM (e.g. QPSK, M= 4) order M-ary QAM scheme. The
for a voiceband modem had been studied extensively in remaining sections of this paper are organized as follows.
the 1980’s [10], [11]. More recently, the authors of [12] In Section II, we describe the system layout for
employed a Coded Orthogonal Frequency Division performance evaluation and the OFDM transmission
Multiplexing (COFDM) transmission scheme for an technique over fading channels is discussed with its
IEEE 802.16 based local multipoint distribution systems performance analyzed in Section III. In Section IV,
(LMDS), leading to further research opportunities for channel utilization is discussed with analysis on sending
development of modems using an OFDM transmission multimedia data over the same channel simultaneously.
scheme. The transmission channel is characterized in Finally, we conclude the paper in Section V.
terms of time and frequency fading predominantly due to
movement of receivers relative to the transmitter and
multipath propagation, respectively. Although OFDM II. SYSTEM LAYOUT
exhibits certain advantages, single carriers such as
variants of QAM offer comparable advantages such as A. Set Up
lower power efficient which is particularly suitable for
Position A network has been installed to provide
wearable health monitoring receivers; asymmetrical
broadband wireless access (BWA) for data delivery
operations is made easier due to simpler transmission
through wireless networks across premises of close
circuitry relative to that of reception. Further, high level
proximity. While different countries have different
of narrow-band noise immunity due to inherent capability
spectrum allocations with appropriate regulations, they
by use of adaptive equalization makes QAM particularly
generally operate in the range of 10 to 66 GHz, and
suitable for transmission over such systems.
mostly below 40 GHz. In places where radio link
We evaluate the system performance by
availability is greatly affected by persistent heavy
simulating its bit-error probability and coverage. In our
rainfall, such as those classified as region-P by ITU, a
experiments, we assumed that the video transmitted does
lower frequency of around 10 GHz is preferred as it is
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 61
less affected by rain attenuation. Fig. 1 shows the basic B. Modulation Scheme
operation of a typical network currently set up around the
Factors that influence our choice of modulations
world to provide broadband wireless access delivering a scheme are:
range of services such as video multicasting [13].
The system consists of three main components. It uses 1. Fading immunity
the switching and transport portions of the network for 2. Spectral efficiency
connection to the backbone network that processes the 3. Receiver complexity
incoming multimedia data for distribution over the 4. Power efficiency
wireless channel. The system consists of a DSL
terminator equipped with an OC3-ATM card used to
route IP traffic over ATM connected to the network As discussed in Section I, single-carrier modulation
backbone. The base station that controls the base station offers better overall compromise for use in many wireless
radio unit (BRU) consists of a base modem module, a systems. Our work therefore concentrates on comparing
base network module, and a base station controller. The lower order QAM schemes such as QPSK and 16-QAM.
BRU is a self-contained unit with a 45-degree antenna System immunity to fading is important to ensure
azimuth at 7-degree elevation. The access side, reliable operation under both time and frequency
separated by an outdoor wireless channel from the selective fading environments. The effects of fading are
system, evaluates the system performance by studying the discussed in Section II D. Spectral efficiency is a measure
received signal. In this network, video data is of data rate per unit bandwidth per unit area given by
distributed to make use of wireless internet access which bps/Hz/m2. Our aim is to maximize this quantity.
transmits both the video and audio (sound track) signals Receiver complexity and power efficiency are important
over wireless broadband channels. Our system is initially considerations, particularly for mobile receivers. For
intended for stationary reception due to bulkiness of the example, power efficiency places a limitation on battery
receiver. However, further research on optimizing for size for mobile receivers.
mobile receivers traveling at high speed will be In essence, higher order M-ary QAM schemes
conducted for extending services to ambulances. (e.g. M=128) provide higher spectral efficiency at the
Fig. 2 shows a schematic diagram of its expense of receiver sensitivity with an increase in
operations while keeping the basic elements of the modulation level and poor noise immunity. A tradeoff
network infrastructure unchanged. The system is between bps/Hz and transmission quality is therefore an
designed to provide a range of multimedia services for issue in comparison of QAM schemes [16]. We present
subscribers through wireless channels. It is optimized for the result from our study of this tradeoff in Section III.
reliable transmission of video with other forms of data
such as text and graphics handled at a much lower
priority.
The video data is played and distributed to the
transmitter through the switching center and up to the
wireless backbone network. The BMU handles data
traffic between subscribers and the network. In our
system, it serves as a modulator that processes the video
clip data and sends it via the radio channel to the
subscriber receivers. In this system, the BMU does not
receive anything from the subscribers as we made an
assumption that the data is sent in simplex mode. The
envelope of the received signal is given by
N
r(t)=a(t).∑s (t−kT) (1)
k
k=1 Figure 3. Coverage vs. link availability
C. Link Availability
Where s(t) is a signaling pulse and T is the symbol
period; N is the total number of paths and k=1 is the line- The effect of path loss reduces signal power over
of-sight (LOS) path. The distortion caused by multipath distance. The maximum range that indicates the
fading a(t) with linearity [14], [15] is given by: maximum distance of receivers from the basestation,
depends primarily on antenna gain and rainfall statistics.
t−kT The fade margin can be adjusted for higher link
a(t)=a (0)+a (1) (2) availability. The range covered by a 10 GHz carrier as a
k k T
function of link availability in percentage of time is
illustrated in Fig. 3. The maximum range decreases
approximately linearly with a higher availability. The
link with 99.99% availability, disabled for no more than
© 2010 ACADEMY PUBLISHER62 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
52 minutes per year, offers a range of close to 20km. dispersion as the signal propagates through the frequency
This is to ensure a bit error rate (BER) performance of 10- selective fading channel is given by:
6 or below. Result shown indicates the maximum link
range for a transmission rate of 12 Mbps with 10 GHz L
carrier with LOS and no rainfall. The range for 99.99% h(t)=∑a .e−jθ.δ(t−τ) (4)
l
availability is 18 km. l=1
D. Effects of Generalized Fading Channel
In a fixed network environment, the main sources of
signal distortion are
1) Multipath fading
2) Frequency selective fading
3) Additive noise
Multipath fading is a collective term used to describe the
constructive and destructive superposition of signal
components that have taken different paths due to such
phenomena as scattering, diffraction and reflection. It
therefore amounts to time selective fading. The signal
propagation path often has no direct LOS due to physical
obstacles between transmitter/receiver antennas. The
spectral components of the signal due to frequency
selective fading are affected by different fading
Figure 4. Average BER performance of receiver
amplitudes and phase shifts as the signal bandwidth is
larger than that of the channel’s coherence bandwidth.
We consider the effects of generalized channel fading
where δ(t-τ) is the Dirac delta function, L is the number
below.
of paths with the first LOS path being L= 0 and a, θ, and
Although in conditions where severe interference and τ represent the amplitude, phase, and magnitude of time
fading make robust carrier recovery become a necessity, delay for each path, respectively.
the use of OFDM offers good performance in fading and
time-variant transmission media [17]. Experiment shows
that low-order QAM schemes also provide adequate III. PERFORMANCE ANALYSIS
performance in such situation. A comparison is carried
The purpose of our experiments is to evaluate the E /
out between the ideal channel and our estimated channel b
N performance of QPSK and 16-QAM for our system
based on assumptions made in [18], [19]. The receiver o
under the transmission channel with characteristics
remains stationary and the bit rate is varied with a carrier
described earlier in II D. To maintain link availability, the
frequency of 10 GHz. The channel is defined with the
comparison is made at the cut-off point at which the link
properties
is no longer available where BER reaches 10-6. Gray
encoding with absolute phase coherent detection has been
2mmR2m−1
pdf(R)= .e−m (3) used to improve BER performance. From Fig. 5, QPSK
R2m shows a better E b/N
o
performance of 4.3 dB over 16-
QAM. The symbol-error rate (SER) of each symbol is
different between M = 4 and M = 16. The measured
where R is a random variable representing the mean-
system data rate is 12 Mbps.
square fading amplitude and
Thus, 16-QAM performs noticeably poorer
compared to QPSK as shown in Fig. 5. Larger M may
R2
m= |m| ≥ 0.5 offer better performance in number of bits per baud at the
var(R2) expense of increased receiver structure complexity and
decrease in BER performance. The system with M = 4
and M = 16 has been compared under different symbol
There is no fading when m tends to infinity as the pdf
lengths and its performance is shown in Fig. 6. It shows
becomes an impulse function. The received signal is
that both 4 and 16-QAM perform very similarly with a
subjected to additive white Gaussian noise (AWGN),
constant E / N value and varying symbol rate.
which is assumed to be independent of channel fading. b o
Results for BER better than 10-6 are not
Fig. 4 shows the performance of the receiver with M = 4
presented as the link is considered available and analysis
and M = 16.
is assumed to be the worst scenario. In actual fact, the
We consider a multilink channel having L
difference in performance widens between these
independent fading channels as described in [20], the
modulation schemes as BER decreases further.
equivalent low-pass impulse response of the time
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 63
subscribers, n is defined as the number of subscribers
sharing the channel (i.e. n= 5 in our simulation). Each
subscriber uses the channel to send different types of data
to the BMU. The channel bandwidth is dynamically
assigned to utilize the allocated bandwidth to each
subscriber. With a mixture of audio (voice) and video
(series of images at a fixed rate of 29.7 frames per
second) being used as test data, the redundancy
bandwidth for audio signal is assumed to be 60% of the
time as proposed by [22]. A protocol described in [23]
uses redundancy when there is no voice signal
transmitted, as detected by voice activation, to transmit
other types of signals. With a channel capacity of 50
cells per block for data with overheads neglected, we
compute the system throughput as a variation of
redundancy bandwidth.
Cell delay is the combined effect of BMU cell
Figure 5. BER comparison between QPSK and 16-QAM waiting time and the actual data transmission time, and
the system throughput is measured by the average
number of cells transmitted per block of the channel. We
assume that video transmission is constant at 29.7 fps
with no redundancy, and we further assume that the only
data sent is video composed of a mono audio track and
pictures only. The system throughput S is given by
S = S +S (5)
A V
An assumption that the system only handles data traffic
consists of audio and video without other information
such as additional synchronization or error correction
redundancies has been made so that data handled by the
system consists only of audio and video without any other
types of data.
Figure 6. BER vs. data symbol length
For the characteristics of video, its throughput S is
V
assumed to be constant which is determined by a function
Analysis shows that increasing the modulation order of the transmission rate R V is given by
from 4 to 16 offers a very slight improvement in bit error
rate. However, QPSK offers a slight advantage of R = P .P .c.f (6)
V h v
simpler receiver structure for the customer modem at the
expense of decrease in spectral efficiency by a ratio of
where P and P are the number of horizontal and vertical
15:3.5. Although it appears that QPSK offers a better h v
pixels respectively, c is the color depth (e.g. 8 bits for 28
compromise than 16-QAM when data symbol length is
or 256 colors) and f is the scan rate in number of frames
varied.
per second (typically ~30fps for video clips). In our
simulation, we tested a video sample of resolution 320 x
IV. BANDWIDTH UTILIZATION
240 with 8-bit or 256 colors at 29.7 fps. In this case, R
V
The system has been set up to measure the suitability of is 4.8 Mb/s.
various modulation techniques. With data collected from The throughput characteristics of audio S A is given
the actual system, a series of computer simulations are by
carried out to examine the system’s behavior under
different contents of audio and video data transmitted. T
The importance of bandwidth utilization in such network S A = R A.n.K. Tt (7)
impacts subsequent stages of data processing. Electronic
Patient Record (EPR) updating can be affected when the
where T = T + T
network saturates. The consequential network t i
degradation resulting from saturation can be mitigated by
The following parameters are used to compute the
automatic fuzzy ontology algorithms similar to that in
throughput as an average number of cells transmitted per
[21].
block of the channel. R is the data rate or the bandwidth
To evaluate the effects of network saturation, we A
assigned for the audio (voice) signal. K is the simulation
simulate the channel of 9.9 – 10.5 GHz when shared by 5
parameter for the audio signal that measures the
© 2010 ACADEMY PUBLISHER64 JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010
proportion of time that there is audio data to be sent (0 ≤ of significant degradation in E b/N o performance, this
K ≤ 10). T is the total transmission time that derives the paper leads to the conclusion that a 16-QAM scheme
redundancy bandwidth as in [24] such that T and T provides optimal compromise between data throughput
t i
denote the duration of sound being transmitted and the and cell coverage while provides adequate performance
duration when there is no sound (idle), respectively. for multicast distribution of video traffic. It is also
apparent that the objective of an efficient deployment
would be to maximize bandwidth utilization by
considering the proportion of time that the audio track is
silent.
REFERENCES
[1] B. Fong and G. Y. Hong, “RF net scales broadband to local
area”, EETimes, June 17, 2002.
[2] J. K. Carvers, “An analysis of pilot symbol assisted
modulation for Rayleigh Fading Channels”, Proc. IEEE
VTC, St. Louis, pp. 380-385, 1991
[3] M. S. Alouini and Goldsmith A. J., “A Unified Approach for
Calculating Error Rates of Linearly Modulated Signals over
Generalized Fading Channels”, IEEE Trans.
Communications, Vol. 47 No. 9, pp. 1324-1330, Sep 1999
Figure.7. Cell delay characteristics of transmitted data
[4] A. C. M. Fong, S. C. Hui and C. T. Lau, “An experimental
learn-on-demand system in a wireless campus
environment”, IEEE Multimedia, Vol. 11/4, pp. 50-60,
2004.
[5] I. Korn, “Digital Communications”, Van Nostrand Reinhold,
1985
[6] V. Mignone, and A. Morello, “CD3-OFDM: a novel
demodulation scheme for fixed and mobile receivers”, IEEE
Trans. Communications, Vol. 44 No. 9, pp. 1144- 1151, Sep
1996
[7] B. D. Hart and D. P. Taylor, “Extended MLSE receiver for
the frequency-flat, fast-fading channel”, IEEE Trans.
Communications, Vol. 46 No. 2, pp. 381- 389, May 1997
[8] A. Mouaki Benani and F., Gagnon, “Comparison of carrier
recovery techniques in M-QAM digital communication
systems”, Proc. Electrical and Computer Engineering Conf.,
Canada, Vol. 1, pp. 73- 77, 7- 10 Mar 2000
[9] R. A. Peloso,” Adaptive equalization for advanced
Figure 8. System throughput characteristics television”, IEEE Trans. Consumer Electronics, Vol. 38 No.
3, pp. 119- 126, Aug 1992
[10] B. Hirosaki, ”An Orthogonally multiplexed QAM system
Fig. 7 shows that each cell maintains a low latency using the discrete Fourier Transform”, IEEE Trans.
and a small cell delay. When more audio data is Communications, Vol. 29, pp. 982- 989, Jul 1981
transmitted (i.e. K increases and R also increases), cell [11] S. Hirosaki et. al., “Advanced groupband data modem
A
delay becomes more severe. Fig. 8 shows the throughput using orthogonally multiplexed QAM technique”, IEEE
characteristics of the system where its throughput Trans. Communications, Vol. 34, pp. 587-592, Jun 1986
[12] V. Tralli et. al,.”Adaptive Time and Frequency Resource
increases with data traffic load linearly until its capacity
Assignment with COFDM for LMDS Systems”, IEEE
approaches the maximum available bandwidth. It is
Trans. Communications, Vol. 49 No. 2, pp. 235-238, Feb
noted from Fig. 8 that the channel utilization is affected
2001
by T i as a large T i decreases channel utilization. So, R A [13] “Local Multipoint Distribution System”, Web ProForum
determines the bandwidth efficiency where it is a close Tutorials of The International Engineering Consortium,
approximation to the system maximum throughput. http://www.iec.org
[14] G. M. Vitetta, D. P. Taylor and U. Mengali, “Double-
filtering receivers for PSK signals transmitted over
Rayleigth frequency-flat fading channels”, IEEE Trans.
V. CONCLUSIONS
Communications, Vol. 44 No. 6, pp. 686-695, Jun 1996
We have compared two single-carrier modulation [15] G. M. Vitetta, U. Mengali and D. P. Taylor, “Double-filter
schemes for good compromise between bandwidth differential detection of PSK signals transmitted over
efficiency and ease of implementation for use with a linearly time-selective Rayleigh fading channels:, IEEE
Trans. Communications, Vol. 47 No. 2, pp. 239-247, Feb
fixed WiMAX telemedicine system at a carrier frequency
1999
of 10 GHz for distribution of multimedia data over a local
[16] S. Sampei, “Applications of Digital Wireless Technologies
area. Although lower modulation schemes offer a
to Global Wireless Communications”, Prentice-Hall NJ,
reduction in receiver structure complexity at the expense
1997
© 2010 ACADEMY PUBLISHERJOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY, VOL. 1, NO. 1, FEBRUARY 2010 65
[17] W. Y. Zou and Y. Wu, “COFDM: An Overview”, IEEE Bernard Fong is currently with the
Trans. Broadcasting, Vol. 41 No. 1, pp. 1-8, Mar 1995 Prognostics and Health Management
[18] M. S. Alouini and A. J. Goldsmith “A Unified Approach Center, City University of Hong Kong. He
for Calculating Error Rates of Linearly Modulated Signals received his BS degree from University of
over Generalized Fading Channels”, IEEE Trans. Manchester Institute of Science and
Communications, Vol. 47 No. 9, pp. 1324-1334, Sep 1999 Technology, United Kingdom, and PhD in
[19] M.A. Do and S.Y. Wu, “Hybrid diversity combining healthcare information systems from the
techniques for DS-CDMA over a multipath fading University of New South Wales, Australia.
channel”, ACM Wireless Networks 3, pp. 155–158, 1997 Prior to joining CityU PHM Centre
[20] F. Classen and H. Meyr, “Frequency synchronization (PHMC), he was a Visiting Associate Professor with the
algorithms for OFDM systems suitable for communication Hong Kong Polytechnic University. His current research
over frequency-selective channels”, Proc. VTC, pp. 1655- interests are broadly in the areas of biomedical engineering,
1659, 1994 health informatics, and prognostics for healthcare and
[21] Q. T. Tho, S. C. Hui, A. C. M. Fong and T. H. Cao, consumer electronics applications. Dr. Fong is serving as the
“Automatic fuzzy ontology generation for semantic web”, South Asia Representative of the IEEE International
IEEE Trans Knowledge and Data Engineering, Vol. 18/6, Conference on Consumer Electronics (ICCE) and General
pp. 842-856, June 2006. Chair for the ICST Connecting Health International
[22] P. T. Brady, “A Statistical Analysis of on-off patterns in 16 Workshop, co-sponsored by the ACM and IEEE-EMBS; he is
conversions”, Bell Syst Tech. J., Vol 47, pp. 71-93, Jan currently an editorial board member of the Journal of
1968 Advances in Information Technology, IEEE Consumer
[23] K. S. Meier-Hellstern, G. P. Pollini, D. J. Goodman, Electronics Newsletter, and held a number of other positions
“Network protocols for the cellular packet switch”, IEEE such as Editor-in-Chief for the International Journal of
Trans. Communications, Vol. 47, pp. 1235- 1244, Feb-Apr. Information Technology, Associate Editor for the Electronic
1994 Commerce Research & Applications Journal, and Guest
[24] P. H. Moose, “A technique for orthogonal frequency Editor for special feature topic in wireless telemedicine of the
division multiplexing frequency offset correction”, IEEE IEEE Communications Magazine. He is authoring the book
Trans. Communications, Vol. 42 No. 10, pp. 1590-1598, Telemedicine Technologies: Information Technologies in
Oct 1994 Medicine and Telehealth, to be published by John Wiley &
Sons U. K. (2010).
Guan Yue Hong is an IT consultant.
© 2010 ACADEMY PUBLISHERCall for Papers and Special Issues
Aims and Scope
JAIT is intended to reflect new directions of research and report latest advances. It is a platform for rapid dissemination of high quality research /
application / work-in-progress articles on IT solutions for managing challenges and problems within the highlighted scope. JAIT encourages a
multidisciplinary approach towards solving problems by harnessing the power of IT in the following areas:
• Healthcare and Biomedicine - advances in healthcare and biomedicine e.g. for fighting impending dangerous diseases - using IT to model
transmission patterns and effective management of patients’ records; expert systems to help diagnosis, etc.
• Environmental Management - climate change management, environmental impacts of events such as rapid urbanization and mass migration,
air and water pollution (e.g. flow patterns of water or airborne pollutants), deforestation (e.g. processing and management of satellite imagery),
depletion of natural resources, exploration of resources (e.g. using geographic information system analysis).
• Popularization of Ubiquitous Computing - foraging for computing / communication resources on the move (e.g. vehicular technology), smart
/ ‘aware’ environments, security and privacy in these contexts; human-centric computing; possible legal and social implications.
• Commercial, Industrial and Governmental Applications - how to use knowledge discovery to help improve productivity, resource
management, day-to-day operations, decision support, deployment of human expertise, etc. Best practices in e-commerce, e-commerce, e-
government, IT in construction/large project management, IT in agriculture (to improve crop yields and supply chain management), IT in
business administration and enterprise computing, etc. with potential for cross-fertilization.
• Social and Demographic Changes - provide IT solutions that can help policy makers plan and manage issues such as rapid urbanization, mass
internal migration (from rural to urban environments), graying populations, etc.
• IT in Education and Entertainment - complete end-to-end IT solutions for students of different abilities to learn better; best practices in e-
learning; personalized tutoring systems. IT solutions for storage, indexing, retrieval and distribution of multimedia data for the film and music
industry; virtual / augmented reality for entertainment purposes; restoration and management of old film/music archives.
• Law and Order - using IT to coordinate different law enforcement agencies’ efforts so as to give them an edge over criminals and terrorists;
effective and secure sharing of intelligence across national and international agencies; using IT to combat corrupt practices and commercial
crimes such as frauds, rogue/unauthorized trading activities and accounting irregularities; traffic flow management and crowd control.
The main focus of the journal is on technical aspects (e.g. data mining, parallel computing, artificial intelligence, image processing (e.g. satellite
imagery), video sequence analysis (e.g. surveillance video), predictive models, etc.), although a small element of social implications/issues could be
allowed to put the technical aspects into perspective. In particular, we encourage a multidisciplinary / convergent approach based on the following
broadly based branches of computer science for the application areas highlighted above:
Special Issue Guidelines
Special issues feature specifically aimed and targeted topics of interest contributed by authors responding to a particular Call for Papers or by
invitation, edited by guest editor(s). We encourage you to submit proposals for creating special issues in areas that are of interest to the Journal.
Preference will be given to proposals that cover some unique aspect of the technology and ones that include subjects that are timely and useful to the
readers of the Journal. A Special Issue is typically made of 10 to 15 papers, with each paper 8 to 12 pages of length.
The following information should be included as part of the proposal:
• Proposed title for the Special Issue
• Description of the topic area to be focused upon and justification
• Review process for the selection and rejection of papers.
• Name, contact, position, affiliation, and biography of the Guest Editor(s)
• List of potential reviewers
• Potential authors to the issue
• Tentative time-table for the call for papers and reviews
If a proposal is accepted, the guest editor will be responsible for:
• Preparing the “Call for Papers” to be included on the Journal’s Web site.
• Distribution of the Call for Papers broadly to various mailing lists and sites.
• Getting submissions, arranging review process, making decisions, and carrying out all correspondence with the authors. Authors should be
informed the Instructions for Authors.
• Providing us the completed and approved final versions of the papers formatted in the Journal’s style, together with all authors’ contact
information.
• Writing a one- or two-page introductory editorial to be published in the Special Issue.
Special Issue for a Conference/Workshop
A special issue for a Conference/Workshop is usually released in association with the committee members of the Conference/Workshop like general
chairs and/or program chairs who are appointed as the Guest Editors of the Special Issue. Special Issue for a Conference/Workshop is typically made of
10 to 15 papers, with each paper 8 to 12 pages of length.
Guest Editors are involved in the following steps in guest-editing a Special Issue based on a Conference/Workshop:
• Selecting a Title for the Special Issue, e.g. “Special Issue: Selected Best Papers of XYZ Conference”.
• Sending us a formal “Letter of Intent” for the Special Issue.
• Creating a “Call for Papers” for the Special Issue, posting it on the conference web site, and publicizing it to the conference attendees.
Information about the Journal and Academy Publisher can be included in the Call for Papers.
• Establishing criteria for paper selection/rejections. The papers can be nominated based on multiple criteria, e.g. rank in review process plus the
evaluation from the Session Chairs and the feedback from the Conference attendees.
• Selecting and inviting submissions, arranging review process, making decisions, and carrying out all correspondence with the authors. Authors
should be informed the Author Instructions. Usually, the Proceedings manuscripts should be expanded and enhanced.
• Providing us the completed and approved final versions of the papers formatted in the Journal’s style, together with all authors’ contact
information.
• Writing a one- or two-page introductory editorial to be published in the Special Issue.
More information is available on the web site at http://www.academypublisher.com/jait/.